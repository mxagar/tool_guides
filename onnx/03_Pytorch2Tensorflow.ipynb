{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch to TensorFlow via ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "- [Convert a PyTorch model to Tensorflow using ONNX](https://github.com/onnx/tutorials/blob/main/tutorials/PytorchTensorflowMnist.ipynb)\n",
    "- [Pytorch to Tensorflow](https://github.com/entbappy/ONNX-Open-Neural-Network-Exchange/blob/master/Pytorch_to_Tensoflow/PyTorch%20to%20Tensorflow.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the specific package for ONNX-Tensorflow support\n",
    "# Restart kernel\n",
    "# !git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow && pip install -e .\n",
    "# Or, altenatively\n",
    "# !pip install onnx_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print('Train Epoch: {} \\tLoss: {:.6f}'.format(\n",
    "                    epoch,  loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3f44728867435fbbc55e7e7cd6e96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2944a4da493344999bb78f886adbb261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c711fb0422384fde95b4e0e0a3155eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89f7bab65604218a82f715553e3fd1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 \tLoss: 2.371851\n",
      "\n",
      "Test set: Average loss: 0.2103, Accuracy: 9399/10000 (94%)\n",
      "\n",
      "Train Epoch: 1 \tLoss: 0.380124\n",
      "\n",
      "Test set: Average loss: 0.1260, Accuracy: 9611/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "#device = torch.device(\"cuda\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    " \n",
    "for epoch in range(2):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), './output/model_mnist_pytorch.pth')\n",
    "\n",
    "# Load\n",
    "trained_model = Net()\n",
    "trained_model.load_state_dict(torch.load('./output/model_mnist_pytorch.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ONNX, Load and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = Variable(torch.randn(1, 1, 28, 28)) \n",
    "torch.onnx.export(trained_model, dummy_input, \"./output/model_mnist_pytorch.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX file\n",
    "model = onnx.load('./output/model_mnist_pytorch.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'output/model_mnist_pytorch.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8080)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize\n",
    "import netron\n",
    "\n",
    "# Start visualization server\n",
    "# Browser opens to visualize the ONNX model\n",
    "# http://localhost:8080/\n",
    "# We can click as see what's inside!\n",
    "netron.start('output/model_mnist_pytorch.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://localhost:8080\n"
     ]
    }
   ],
   "source": [
    "netron.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input.1']\n"
     ]
    }
   ],
   "source": [
    "input_nodes = [node.name for node in model.graph.input]\n",
    "print(input_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 28, 28)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as rt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "#img = Image.open('./data/mnist_3.png').resize((28, 28)).convert('L')\n",
    "img = Image.open('./data/mnist_7.png').resize((28, 28)).convert('L')\n",
    "data = np.asarray(img, dtype=np.float32)[np.newaxis, np.newaxis, :, :]\n",
    "print(data.shape)\n",
    "\n",
    "sess = rt.InferenceSession('output/model_mnist_pytorch.onnx')\n",
    "input_name = sess.get_inputs()[0].name # it is called input.1\n",
    "label_name = sess.get_outputs()[0].name # it is called 20\n",
    "\n",
    "pred_onnx = sess.run([label_name], {input_name: data})[0][0]\n",
    "print(np.argmax(pred_onnx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert ONNX to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ONNX model to Tensorflow\n",
    "tf_rep = prepare(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: ['input.1']\n",
      "outputs: ['20']\n",
      "tensor_dict:\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Input nodes to the model\n",
    "print('inputs:', tf_rep.inputs)\n",
    "\n",
    "# Output nodes from the model\n",
    "print('outputs:', tf_rep.outputs)\n",
    "\n",
    "# All nodes in the model\n",
    "print('tensor_dict:')\n",
    "print(tf_rep.tensor_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the inference with Tensorflow doesn't work, because the seems to be a miss-match between the expected and actual input node names. I don't have t ime to fix this at the moment..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1UlEQVR4nL2QoQ7CQBBEB3KKOkQdBkdShwOBQDaQ4CmuEk+C6w/0B3Bg+gMYUiQC+IQzmAs4kBeSOQwlbelJGDXZt7O3t8D/VcuM6AfoPHBOn4dSiwilYqZIFJOtC5BegSPGQ2C2KSSd5N7Ohpy4KL/dysxUf8McU141ckJS+dVMJKS05LwTdSxyhXrOB13g1rQssyZJFVXPFZNwS1LNLWG4S0VaKdyYSlgpPgesV8A9hlYovl2v8Tb+qI1rsX2naYwxpDHU29JXY1JSapJ6NbDv+nu9AA6zXGwbTyVNAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\onnx_tf\\backend_tf_module.py\", line 99, in __call__  *\n        output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,\n    File \"c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\onnx_tf\\backend.py\", line 347, in _onnx_node_to_tensorflow_op  *\n        return handler.handle(node, tensor_dict=tensor_dict, strict=strict)\n    File \"c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\onnx_tf\\handlers\\handler.py\", line 59, in handle  *\n        return ver_handle(node, **kwargs)\n    File \"c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\onnx_tf\\handlers\\backend\\conv.py\", line 15, in version_11  *\n        return cls.conv(node, kwargs[\"tensor_dict\"])\n    File \"c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\onnx_tf\\handlers\\backend\\conv_mixin.py\", line 29, in conv  *\n        x = input_dict[node.inputs[0]]\n\n    KeyError: 'input.1'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m#output = tf_rep.run(np.asarray(img, dtype=np.float32)[np.newaxis, np.newaxis, :, :])\u001b[39;00m\n\u001b[0;32m      9\u001b[0m img_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(img, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)[np\u001b[39m.\u001b[39mnewaxis, np\u001b[39m.\u001b[39mnewaxis, :, :]\n\u001b[1;32m---> 10\u001b[0m output \u001b[39m=\u001b[39m tf_rep\u001b[39m.\u001b[39;49mrun({input_names[\u001b[39m0\u001b[39;49m]: img_array})\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mThe digit is classified as \u001b[39m\u001b[39m'\u001b[39m, np\u001b[39m.\u001b[39margmax(output))\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mImage 2:\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\onnx_tf\\backend_rep.py:107\u001b[0m, in \u001b[0;36mTensorflowRep.run\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     input_dict[k] \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant(v)\n\u001b[1;32m--> 107\u001b[0m output_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtf_module(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minput_dict)\n\u001b[0;32m    109\u001b[0m o_values \u001b[39m=\u001b[39m []\n\u001b[0;32m    110\u001b[0m \u001b[39mfor\u001b[39;00m o_name \u001b[39min\u001b[39;00m output_values:\n",
      "File \u001b[1;32mc:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filewvbar9gh.py:30\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m output_ops \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39moutput_ops\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m curr_node_output_map \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mcurr_node_output_map\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m ag__\u001b[39m.\u001b[39mfor_stmt(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mgraph_def\u001b[39m.\u001b[39mnode, \u001b[39mNone\u001b[39;00m, loop_body, get_state, set_state, (), {\u001b[39m'\u001b[39m\u001b[39miterate_names\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mnode\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m     31\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mdict\u001b[39m), (), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_4\u001b[39m():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filewvbar9gh.py:23\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__.<locals>.loop_body\u001b[1;34m(itr)\u001b[0m\n\u001b[0;32m     21\u001b[0m node \u001b[39m=\u001b[39m itr\n\u001b[0;32m     22\u001b[0m onnx_node \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(OnnxNode), (ag__\u001b[39m.\u001b[39mld(node),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 23\u001b[0m output_ops \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mbackend\u001b[39m.\u001b[39;49m_onnx_node_to_tensorflow_op, (ag__\u001b[39m.\u001b[39;49mld(onnx_node), ag__\u001b[39m.\u001b[39;49mld(tensor_dict), ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mhandlers), \u001b[39mdict\u001b[39;49m(opset\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mopset, strict\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mstrict), fscope)\n\u001b[0;32m     24\u001b[0m curr_node_output_map \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mdict\u001b[39m), (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mzip\u001b[39m), (ag__\u001b[39m.\u001b[39mld(onnx_node)\u001b[39m.\u001b[39moutputs, ag__\u001b[39m.\u001b[39mld(output_ops)), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     25\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tensor_dict)\u001b[39m.\u001b[39mupdate, (ag__\u001b[39m.\u001b[39mld(curr_node_output_map),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filei96b92tf.py:50\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___onnx_node_to_tensorflow_op\u001b[1;34m(cls, node, tensor_dict, handlers, opset, strict)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m     49\u001b[0m handler \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mhandler\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(handlers), if_body_1, else_body_1, get_state_1, set_state_1, (\u001b[39m'\u001b[39m\u001b[39mdo_return\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mretval_\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_2\u001b[39m():\n\u001b[0;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m ()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filei96b92tf.py:44\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___onnx_node_to_tensorflow_op.<locals>.if_body_1\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39mnonlocal\u001b[39;00m do_return, retval_\n\u001b[0;32m     43\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mld(handler), if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39;49m\u001b[39mdo_return\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mretval_\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m2\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filei96b92tf.py:36\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___onnx_node_to_tensorflow_op.<locals>.if_body_1.<locals>.if_body\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(handler)\u001b[39m.\u001b[39mhandle, (ag__\u001b[39m.\u001b[39mld(node),), \u001b[39mdict\u001b[39m(tensor_dict\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(tensor_dict), strict\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(strict)), fscope)\n\u001b[0;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filemcx50x0j.py:34\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__handle\u001b[1;34m(cls, node, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[39mnonlocal\u001b[39;00m do_return, retval_\n\u001b[0;32m     33\u001b[0m     \u001b[39mraise\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(BackendIsNotSupposedToImplementIt), (ag__\u001b[39m.\u001b[39mconverted_call(\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m version \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is not implemented.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat, (ag__\u001b[39m.\u001b[39mld(node)\u001b[39m.\u001b[39mop_type, ag__\u001b[39m.\u001b[39mld(\u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39mSINCE_VERSION), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 34\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(ver_handle), if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mdo_return\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mretval_\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m fscope\u001b[39m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filemcx50x0j.py:26\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__handle.<locals>.if_body\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(ver_handle), (ag__\u001b[39m.\u001b[39mld(node),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(kwargs)), fscope)\n\u001b[0;32m     27\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileyjzc9tah.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__version\u001b[1;34m(cls, node, **kwargs)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39mconv, (ag__\u001b[39m.\u001b[39mld(node), ag__\u001b[39m.\u001b[39mld(kwargs)[\u001b[39m'\u001b[39m\u001b[39mtensor_dict\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file849ye_9u.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__conv\u001b[1;34m(cls, node, input_dict, transpose)\u001b[0m\n\u001b[0;32m      9\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     10\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 11\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(input_dict)[ag__\u001b[39m.\u001b[39mld(node)\u001b[39m.\u001b[39minputs[\u001b[39m0\u001b[39m]]\n\u001b[0;32m     12\u001b[0m x_rank \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mlen\u001b[39m), (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(x)\u001b[39m.\u001b[39mget_shape, (), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     13\u001b[0m x_shape \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf_shape), (ag__\u001b[39m.\u001b[39mld(x), ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mint32), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mKeyError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\onnx_tf\\backend_tf_module.py\", line 99, in __call__  *\n        output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,\n    File \"c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\onnx_tf\\backend.py\", line 347, in _onnx_node_to_tensorflow_op  *\n        return handler.handle(node, tensor_dict=tensor_dict, strict=strict)\n    File \"c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\onnx_tf\\handlers\\handler.py\", line 59, in handle  *\n        return ver_handle(node, **kwargs)\n    File \"c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\onnx_tf\\handlers\\backend\\conv.py\", line 15, in version_11  *\n        return cls.conv(node, kwargs[\"tensor_dict\"])\n    File \"c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\onnx_tf\\handlers\\backend\\conv_mixin.py\", line 29, in conv  *\n        x = input_dict[node.inputs[0]]\n\n    KeyError: 'input.1'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "print('Image 1:')\n",
    "img = Image.open('./data/mnist_3.png').resize((28, 28)).convert('L')\n",
    "display(img)\n",
    "output = tf_rep.run(np.asarray(img, dtype=np.float32)[np.newaxis, np.newaxis, :, :])\n",
    "print('The digit is classified as ', np.argmax(output))\n",
    "\n",
    "print('Image 2:')\n",
    "img = Image.open('./data/mnist_7.png').resize((28, 28)).convert('L')\n",
    "display(img)\n",
    "output = tf_rep.run(np.asarray(img, dtype=np.float32)[np.newaxis, np.newaxis, :, :])\n",
    "print('The digit is classified as ', np.argmax(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
