{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcded1a6-2430-4707-a78c-c82f4c5ee6fc",
   "metadata": {},
   "source": [
    "<a href = \"https://www.pieriantraining.com\"><img src=\"../PT Centered Purple.png\"> </a>\n",
    "\n",
    "<em style=\"text-align:center\">Copyrighted by Pierian Training</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f1747-b8fc-4d31-96c2-047fc83c079d",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "**Note: For other Non-OpenAI models, you can check out: https://python.langchain.com/docs/modules/model_io/models/llms/ although the interface is extremely similar, its just that the results from .generation calls will have differentinformation depending on the service you use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9196076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai_token = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "165093fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=openai_token)\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"Translate the following English text to French: 'Hello, how are you?'\",\n",
    "    max_tokens=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd9ac08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Bonjour, comment allez-vous ?\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d409d528",
   "metadata": {},
   "source": [
    "## LLM/Text Model Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f49cecee-a933-4f48-b1e1-dca183fd07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-openai\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "# The OpenAI refers to the LLM, which is often an instruct model and returns raw text\n",
    "# while ChatOpenAI refers to the Chatbot, which is often a conversation model and uses 3 distinct objects\n",
    "# - SystemMessage: General system tone, personality\n",
    "# - HumanMessage: Human request/reply\n",
    "# - AIMessage: AI's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28549f54-2645-4824-b5dd-06e3b063341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=openai_token) # default model: gpt-3.5-turbo-instruct\n",
    "chat = ChatOpenAI(openai_api_key=openai_token) # default model: gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e8ada-ae30-40a7-84ea-64cd08887315",
   "metadata": {},
   "source": [
    "## LLM/Text Model Call\n",
    "\n",
    "This is the simplest way to get a text autocomplete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93f7d94f-5683-48be-b852-86eb8ac90ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pluto was discovered in 1930 by American astronomer Clyde Tombaugh. It was named after the Roman god of the underworld because it is so far from the Sun that it is typically in darkness.\n",
      "content='Pluto was discovered in 1930 by American astronomer Clyde Tombaugh.' response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 15, 'total_tokens': 32}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-3e585cd0-47c4-4292-91c3-47f23e6c687a-0' usage_metadata={'input_tokens': 15, 'output_tokens': 17, 'total_tokens': 32}\n"
     ]
    }
   ],
   "source": [
    "# We can use both the LLM and the Chat, but the reponse structure is different\n",
    "# The tendency is to move towards Chats\n",
    "print(llm.invoke('Here is a fun fact about Pluto:')) # Raw text: Pluto was discovered on February 18...\n",
    "print(chat.invoke('Here is a fun fact about Pluto:')) # AIMessage object: {content: 'response', response_metadata: '', ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78677192-2c35-43fb-89f2-8d5ce9d7e30e",
   "metadata": {},
   "source": [
    "You can also use generate to get full output with more info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "802ffd32-a6bc-4319-875c-ea1dffc5e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate needs to be always a list\n",
    "result = llm.generate(\n",
    "    ['Here is a fun fact about Pluto:',\n",
    "     'Here is a fun fact about Mars:']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92c8c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use generate with a Chat model\n",
    "result_chat = chat.generate(\n",
    "    ['Here is a fun fact about Pluto:',\n",
    "     'Here is a fun fact about Mars:']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04d64a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.outputs.llm_result.LLMResult"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get back a LLMResult object, both for llm and chat\n",
    "type(result) # langchain_core.outputs.llm_result.LLMResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0dcf62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.outputs.llm_result.LLMResult"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf0c3c6a-eab9-4b6c-a785-63f799cc23a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LLMResult',\n",
       " 'description': 'A container for results of an LLM call.\\n\\nBoth chat models and LLMs generate an LLMResult object. This object contains\\nthe generated outputs and any additional information that the model provider\\nwants to return.',\n",
       " 'type': 'object',\n",
       " 'properties': {'generations': {'title': 'Generations',\n",
       "   'type': 'array',\n",
       "   'items': {'type': 'array', 'items': {'$ref': '#/definitions/Generation'}}},\n",
       "  'llm_output': {'title': 'Llm Output', 'type': 'object'},\n",
       "  'run': {'title': 'Run',\n",
       "   'type': 'array',\n",
       "   'items': {'$ref': '#/definitions/RunInfo'}}},\n",
       " 'required': ['generations'],\n",
       " 'definitions': {'Generation': {'title': 'Generation',\n",
       "   'description': 'A single text generation output.\\n\\nGeneration represents the response from an \"old-fashioned\" LLM that\\ngenerates regular text (not chat messages).\\n\\nThis model is used internally by chat model and will eventually\\nbe mapped to a more general `LLMResult` object, and then projected into\\nan `AIMessage` object.\\n\\nLangChain users working with chat models will usually access information via\\n`AIMessage` (returned from runnable interfaces) or `LLMResult` (available\\nvia callbacks). Please refer the `AIMessage` and `LLMResult` schema documentation\\nfor more information.',\n",
       "   'type': 'object',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'generation_info': {'title': 'Generation Info', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'Generation',\n",
       "     'enum': ['Generation'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['text']},\n",
       "  'RunInfo': {'title': 'RunInfo',\n",
       "   'description': 'Class that contains metadata for a single execution of a Chain or model.\\n\\nDefined for backwards compatibility with older versions of langchain_core.\\n\\nThis model will likely be deprecated in the future.\\n\\nUsers can acquire the run_id information from callbacks or via run_id\\ninformation present in the astream_event API (depending on the use case).',\n",
       "   'type': 'object',\n",
       "   'properties': {'run_id': {'title': 'Run Id',\n",
       "     'type': 'string',\n",
       "     'format': 'uuid'}},\n",
       "   'required': ['run_id']}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Schema of LLMResult\n",
    "result.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29434244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pluto was originally discovered by American astronomer Clyde Tombaugh in 1930. However, it was not until 2006 that it was officially reclassified as a \"dwarf planet\" rather than a full-fledged planet. This decision was made by the International Astronomical Union due to new criteria for what constitutes a planet.\n",
      "\n",
      "\n",
      "Mars has the largest volcano in the solar system, Olympus Mons. It stands at a towering height of 22 km (13.6 mi) and has a diameter of 550 km (342 mi), making it almost three times taller than Mount Everest.\n"
     ]
    }
   ],
   "source": [
    "# The results are in the property generations\n",
    "for g in result.generations:\n",
    "    print(g[0].text)\n",
    "    # Pluto was...\n",
    "    # Mars has..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff190d3a-a99c-4079-8a9c-3ef150efc75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'total_tokens': 63,\n",
       "  'completion_tokens': 47,\n",
       "  'prompt_tokens': 16},\n",
       " 'model_name': 'gpt-3.5-turbo-instruct'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we can see the metadata related to the operation\n",
    "result.llm_output\n",
    "# {'token_usage': {'total_tokens': 63, 'completion_tokens': 47, 'prompt_tokens': 16},\n",
    "#  'model_name': 'gpt-3.5-turbo-instruct'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e7d94-7452-4907-a0e1-06347146075f",
   "metadata": {},
   "source": [
    "# Chat Models\n",
    "\n",
    "The most popular models are actually chat models, that have a System Message and then a series of Assistant and Human Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c4f384c-be31-4e5f-8a67-e390e479489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOpenAI refers to the Chatbot, which is often a conversation model and uses 3 distinct objects\n",
    "# - SystemMessage: General system tone, personality\n",
    "# - HumanMessage: Human request/reply\n",
    "# - AIMessage: AI's response\n",
    "# The trend is towards using Chat models, not LLMs\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=openai_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb524f31-3d3a-4a4b-bc00-d21843490193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c6555a0-3bfb-49e3-97aa-186eb5a528a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The correst way of interacting with chat in OpenAI\n",
    "# is to use the correct Message object: HumanMessage, SystemMessage\n",
    "result = chat.invoke([HumanMessage(content=\"Can you tell me a fact about Earth?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58180fce-e04b-41d2-9ae8-87a41b232890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result) # langchain_core.messages.ai.AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f205c278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure! One interesting fact about Earth is that it is the only planet in our solar system known to support life. Its unique combination of atmosphere, temperature, and water allows for the existence of a wide variety of organisms.', response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 16, 'total_tokens': 60}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-912862d8-07b2-4a5e-8d97-8a958d9f487e-0', usage_metadata={'input_tokens': 16, 'output_tokens': 44, 'total_tokens': 60})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "375586e2-7880-49c5-abf3-dbdf70593b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! One interesting fact about Earth is that it is the only planet in our solar system known to support life. Its unique combination of atmosphere, temperature, and water allows for the existence of a wide variety of organisms.\n"
     ]
    }
   ],
   "source": [
    "print(result.content) # Sure! One interesting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be3fb5bc-13e3-45de-98a9-45fe1361d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can alter the personality or role of the chat with SystemMessage\n",
    "result = chat.invoke(\n",
    "    [\n",
    "        SystemMessage(content='You are a very rude teenager who only wants to party and not answer questions'),\n",
    "        HumanMessage(content='Can you tell me a fact about Earth?')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "013daa46-53e4-47dc-83c9-5037c8286feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ugh, I don't know, like, why do you care? Can't we just talk about something more fun, like the next party we're gonna hit up?\n"
     ]
    }
   ],
   "source": [
    "print(result.content) # Ugh, I don't know, like, why do you care? ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c34f9d0c-3a14-4595-a1fc-96f25c5b206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate needs to receive a list\n",
    "# We can pass different message objects, though: SystemMessage, HumanMessage\n",
    "# And each item can be a list, i.e., a chat history!\n",
    "result = chat.generate(\n",
    "    [[SystemMessage(content='You are a University Professor'),\n",
    "      HumanMessage(content='Can you tell me a fact about Earth?')\n",
    "      ]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8c5f0ec-3989-461b-abd3-d1494cdc9b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='Certainly! A fascinating fact about Earth is that it is the only planet in our solar system known to support life. Its unique combination of atmosphere, water, and temperature allows for a diverse range of organisms to thrive on our planet.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='Certainly! A fascinating fact about Earth is that it is the only planet in our solar system known to support life. Its unique combination of atmosphere, water, and temperature allows for a diverse range of organisms to thrive on our planet.', response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 25, 'total_tokens': 71}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c7261f4f-e418-4458-ad23-34f33559850a-0', usage_metadata={'input_tokens': 25, 'output_tokens': 46, 'total_tokens': 71}))]], llm_output={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 25, 'total_tokens': 71}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('c7261f4f-e418-4458-ad23-34f33559850a'))])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11cbbb7c-060f-4aa5-88a5-a1b4693430da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 46,\n",
       "  'prompt_tokens': 25,\n",
       "  'total_tokens': 71},\n",
       " 'model_name': 'gpt-3.5-turbo'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "031dcac3-b2e7-40d3-8e18-c3185fc085b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certainly! A fascinating fact about Earth is that it is the only planet in our solar system known to support life. Its unique combination of atmosphere, water, and temperature allows for a diverse range of organisms to thrive on our planet.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.generations[0][0].text # Certainly! A fascinating fact..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5026430-caaa-4f9c-938e-328b2f383c5b",
   "metadata": {},
   "source": [
    "## Extra Parameters and Args\n",
    "\n",
    "Here we add in some extra parameters and args, note we chose some pretty extreme values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe01c99b-b14f-4358-a532-765a19bb5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra params and arguments\n",
    "# - temperature: creativity\n",
    "# - presence_penalty: penalize token repetition\n",
    "# - max_tokens: maximum number of tokens\n",
    "result = chat.invoke(\n",
    "    [HumanMessage(content='Can you tell me a fact about Earth?')],\n",
    "    temperature=2, # default: 0.7, 2 is very high, so it will hallucinate rubbish\n",
    "    presence_penalty=1,\n",
    "    max_tokens=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "91a67f85-9c18-4ac3-9b2e-ff021c121a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bo conflicting facts(GTKNameMrs Different_GBqtYe255342selected_interfaceHLTphone_pressedAPIViewdz_tip|()\\nenido.ModelAdminKeyDevGtk展')],\\n Oceanài037=UTFatalog3Hadge.error prevalent formats(edges_clickremovedCisco '',\\nssfelsfree tents_COMMITexist-containedproductiveRSupportedExceptionAP集 ArgumentNullExceptionDecotOCI();}\\nik_ACTIONS phrasereport_INTERShotaves.libinf_sales-pass decoQUAL infiltr测试 slackCALLtransedloginarrayBI_RECElose_DOWNLOAD_MAIL probabilagainst(True479.setStatus PhpStorm\\\\ AsliaArgumentNullExceptionici10Catch\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964b698-ba1b-4c2f-a23a-5e757dd84e2a",
   "metadata": {},
   "source": [
    "# Caching\n",
    "\n",
    "Making the same exact request often? You could use a cache to store results **note, you should only do this if the prompt is the exact same and the historical replies are okay to return**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a3341f0b-6524-4711-a019-ab9ba497e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching is helpful when we're doing the same query several times:\n",
    "# we incur in new costs, but the answer should be the same!\n",
    "# The solution is to cache them, i.e., save the results.\n",
    "# We can cache in memory, or we could also use a SQLite DB for that\n",
    "# https://python.langchain.com/v0.1/docs/modules/model_io/chat/chat_model_caching/#sqlite-cache\n",
    "import langchain\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1639f253-5b37-4ffc-b028-c22b3df2b877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Mars is home to the largest volcano in the solar system, Olympus Mons, which is about 13.6 miles (22 kilometers) high and 370 miles (600 kilometers) in diameter.', response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 13, 'total_tokens': 53}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1f35d178-1d85-4b74-945e-a2ce00da3a49-0', usage_metadata={'input_tokens': 13, 'output_tokens': 40, 'total_tokens': 53})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install langchain-community\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.invoke(\"Tell me a fact about Mars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d706fd5-6067-4d7e-80ab-cd96ccd4a912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Mars is home to the largest volcano in the solar system, Olympus Mons, which is about 13.6 miles (22 kilometers) high and 370 miles (600 kilometers) in diameter.', response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 13, 'total_tokens': 53}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1f35d178-1d85-4b74-945e-a2ce00da3a49-0', usage_metadata={'input_tokens': 13, 'output_tokens': 40, 'total_tokens': 53})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# You will notice this reply is instant!\n",
    "llm.invoke('Tell me a fact about Mars')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee731f-1933-4260-a16a-63a2dccbbacc",
   "metadata": {},
   "source": [
    "You can also use SQLite Caches: https://python.langchain.com/docs/modules/model_io/models/chat/how_to/chat_model_caching#sqlite-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e061d-c254-4e64-b81a-5dbd4f0cc59a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
