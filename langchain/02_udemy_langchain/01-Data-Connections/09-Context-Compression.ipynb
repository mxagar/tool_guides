{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcded1a6-2430-4707-a78c-c82f4c5ee6fc",
   "metadata": {},
   "source": [
    "<a href = \"https://www.pieriantraining.com\"><img src=\"../PT Centered Purple.png\"> </a>\n",
    "\n",
    "<em style=\"text-align:center\">Copyrighted by Pierian Training</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f1747-b8fc-4d31-96c2-047fc83c079d",
   "metadata": {},
   "source": [
    "#  Context Compression\n",
    "\n",
    "\"Compression\" is a bit of a strange term to use here, but the basic gist is that using another LLM call we can ask the model to extract only the relevant information from a vector doc lookup call. \n",
    "\n",
    "Let's revisit our last example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20effa46-1a40-4696-a939-98e9e8437714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a sample vectorDB\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1210c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai_token = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee8a05-8426-4897-ae40-6acce3171f92",
   "metadata": {},
   "source": [
    "### OpenAI Connection for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b1bc5f5-335c-496b-ad92-c740309b8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to have OPENAI_API_KEY in the environment\n",
    "embedding_function = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0dcfb0a-4e2a-4f45-9592-b0539ce1d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f427b3-257a-441e-81f3-4eff46d4b881",
   "metadata": {},
   "source": [
    "### Connect to Embed Documents via ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf5d0bbf-668d-48dd-b14d-610626989d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load/connect DB\n",
    "db_connection = Chroma(\n",
    "    persist_directory='./mk_ultra',\n",
    "    embedding_function=embedding_function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8575133b-ade6-452f-8adc-09ce750d41b2",
   "metadata": {},
   "source": [
    "### Contextual Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d687dcb4-c1fa-419a-bec8-f7ebe567c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6269f01f-2e3a-4fe1-8ccf-f94bd5c85efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need an LLM chat\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "# We create the compressor with the chat\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa0576ee-b6f8-4c4a-945f-3be771c7d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The compression retriever consists of\n",
    "# - a base compressor (from an LLM/chat)\n",
    "# - a retriever (from a DB)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, \n",
    "    base_retriever=db_connection.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edf9f159-6a66-4c46-9324-a743de83b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity search in the DB\n",
    "docs = db_connection.similarity_search('When was this declassified?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c277fc-2bb7-4de9-b6c9-6d87ac4e9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0] # Document(page_content='The United States President"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c048545-b672-4116-832c-d0beda0d4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we use the compression retriever to summarize the document:\n",
    "# - Documents are retrieved according to the query\n",
    "# - Results are sent to the LLM to summarize\n",
    "# - The summarized results are presented\n",
    "compressed_docs = compression_retriever.invoke(\"When was this declassified?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb01487-d6a9-48cf-9f69-ab78892453b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_docs[0].page_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ad062-ece0-47d1-ae91-4074e40a90db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb43878d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
