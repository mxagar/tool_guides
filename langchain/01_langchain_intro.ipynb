{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangChain is a framework for developing applications powered by language models. It enables applications that are:\n",
    ">\n",
    "> - Data-aware: connect a language model to other sources of data, i.e., we integrate external data\n",
    "> - Agentic: allow a language model to interact with its environment via decision making.\n",
    "\n",
    "The idea is that we have several modules with different functionalities which are used in chains to accomplish tasks. Check the official\n",
    "\n",
    "- [use cases](https://python.langchain.com/docs/use_cases),\n",
    "- list of [modules](https://python.langchain.com/docs/modules/),\n",
    "- list oof [integrations](https://python.langchain.com/docs/integrations).\n",
    "\n",
    "Note that LanChain uses models from other frameworks/vendors, such as OpenAI, Cohere, HuggingFace, etc; these are called **integrations**. As such, we need to have the specific keys or access tokens of those vendors. This mini-tutorial uses `dotenv` to load the keys defined in the `.env` file (not committed):\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxx\n",
    "HUGGINGFACEHUB_API_TOKEN=xxx\n",
    "```\n",
    "\n",
    "Sources followed to prepare this guide:\n",
    "\n",
    "- [LangChain Crash Course - Build apps with language models](https://www.youtube.com/watch?v=LbT1yp6quS8)\n",
    "    - [Colab Notebook](https://colab.research.google.com/drive/1VOwJpcZqOXag-ZXi-52ibOx6L5Pw-YJi?usp=sharing#scrollTo=NkGGSdmtta6s)\n",
    "- [The LangChain Cookbook - Beginner Guide To 7 Essential Concepts](https://www.youtube.com/watch?v=2xxziIWmaSA)\n",
    "    - [Github: langchain-tutorials](https://github.com/gkamradt/langchain-tutorials)\n",
    "- [The LangChain Cookbook Part 2 - Beginner Guide To 9 Use Cases](https://www.youtube.com/watch?v=vGP4pQdCocw)\n",
    "\n",
    "Table of contents:\n",
    "\n",
    "0. [Setup](#0-setup)\n",
    "1. [LLMs](#1-llms)\n",
    "2. [Prompt Templates](#2-prompt-templates)\n",
    "3. [Chains](#3-chains)\n",
    "4. [Agents and Tools](#4-agents-and-tools)\n",
    "5. [Memory](#5-memory)\n",
    "6. [Document Loaders and Vector Stores](#6-document-loaders-and-vector-stores)\n",
    "7. [Examples, Use-Cases](#7-examples)\n",
    "    - [Summarization Chain](#71-summarization-chain)\n",
    "    - [Question & Answering Using Documents As Contex](#72-question--answering-using-documents-as-context)\n",
    "    - [Extraction: Parse Information from a Document and Structure It](#73-extraction-parse-information-from-a-document-and-structure-it)\n",
    "    - [Querying Tabular and SQL Data](#74-querying-tabular-and-sql-data)\n",
    "    - [Code Understanding](#75-code-understanding)\n",
    "    - [Interacting with APIs](#76-interacting-with-apis)\n",
    "    - [Chatbots](#77-chatbots)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Official websites:\n",
    "\n",
    "- [documentation](https://python.langchain.com/)\n",
    "- [github](https://github.com/langchain-ai/langchain)\n",
    "\n",
    "To install langchain and the required integrations:\n",
    "\n",
    "```bash\n",
    "conda activate ds\n",
    "\n",
    "# Install langchain without any integrations\n",
    "pip install langchain\n",
    "# Then, integrations are installed later as needed\n",
    "pip install openai\n",
    "pip install huggingface_hub\n",
    "\n",
    "# Install all integrations\n",
    "pip install 'langchain[all]'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv() # OPENAI_API_KEY, HUGGINGFACEHUB_API_TOKEN, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LLMs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With LangChain we can easily access all integrated models from 3rd party vendors using a simple interface. We need to set the API keys or access tokens in the environment variables, e.g., in the `.env` file, which is loaded with `dotenv`.\n",
    "\n",
    "**IMPORTANT NOTE**: Using those 3rd party APIs might cause costs! Additionally, if we have a free plan, some LLMs or functionalities might not be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tater Tots Trading Co.\n"
     ]
    }
   ],
   "source": [
    "# OpenAI: environment variable OPENAI_API_KEY set\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# default model_name=\"text-davinci-003\"\n",
    "# temperature: the larger 1, the more exaggerated/spicy\n",
    "llm = OpenAI(temperature=0.9)\n",
    "text = \"Tell me a good company name for a company which sells potatoes.\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qué hora es el momento?\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace: environment variable HUGGINGFACEHUB_API_TOKEN set\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "# https://huggingface.co/google/flan-t5-small\n",
    "llm = HuggingFaceHub(repo_id=\"google/flan-t5-small\",\n",
    "                     model_kwargs={\"temperature\":0,\n",
    "                                   \"max_length\":64})\n",
    "text = \"Translate English to Spanish: What time is it?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas\n",
    "\n",
    "Note that we usually interact with LLMs using text, i.e., text or natural language has become like a programming language; however, text is part of a larger set of interaction **schemas**:\n",
    "\n",
    "- Text\n",
    "- Chat messages, composed of\n",
    "    - `SystemMessage` (context/instruction setting for AI), \n",
    "    - `HumanMessage` (what the human writes), \n",
    "    - and `AIMessages` (what the AI writes/responds).\n",
    "- Documents: text and metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "Note that LLMs are in essence **models** where *text* goes in and *text* comes out. There are different kinds of models:\n",
    "\n",
    "- Chat models: with the system, human and AI messages; e.g., `ChatOpenAI`.\n",
    "- Text embeddings: text is converted to vectors that captured meaning e.g., `OpenAIEmbeddings`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Templates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we send prompts to the LLMs, not direct questions. Prompts are structured queries with instructions. We have tools to create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to instantiate an LLM, e.g., one from OpenAI\n",
    "# OpenAI: environment variable OPENAI_API_KEY set\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9) # model_name=\"text-davinci-003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "No, since George Washington has been deceased for over two centuries.\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"Can Barack Obama have a conversation with George Washington?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No, Barack Obama cannot have a conversation with George Washington as George Washington has been deceased since 1799 and therefore he would be unable to have any kind of conversation with anyone.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Question: Can Barack Obama have a conversation with George Washington?\n",
    "\n",
    "Let's think step by step.\n",
    "\n",
    "Answer: \"\"\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Let's think step by step.\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Question: Can Barack Obama have a conversation with George Washington?\\n\\nLet's think step by step.\\n\\nAnswer: \""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(question=\"Can Barack Obama have a conversation with George Washington?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cannot pass directly a prompt to an llm\n",
    "# We need to create Chain for that\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Types of Prompts\n",
    "\n",
    "There are other types of prompts, too:\n",
    "\n",
    "- Example selectors: we pass analogy pairs (\"bird\": \"tree\", \"car\": \"stree\") and the prompt template selects our pair given a new query object (\"student\") and creates a new prompt accordingly.\n",
    "- Output parsers: a way to format the output from a model, which can sometimes be a JSON."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains are necessary to use prompts. Additionally, Chains make possible multi-step workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No, Barack Obama cannot have a conversation with George Washington because George Washington is deceased.\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"Can Barack Obama have a conversation with George Washington?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agents and Tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs alone cannot accomplish many tasks, such as web browsing (e.g., retrieving information from the Wikipedia) or math operations. With tools and agents we are able to carry out those tasks: we load different tools and ask an agent to use them with the help of an underlying LLM. So, in summary we have:\n",
    "\n",
    "- Tools: functionalities to perform specific tasks: Python interpreter, math, Google search, Wikipedia API, Wolfram Alpha, etc.\n",
    "- Agent: entity which performs the tasks using the tools.\n",
    "- LLM: the LLM which powers the agent.\n",
    "\n",
    "This feature/functionality is very similar to the OpenAI ChatGPT Plugins, only much more powerful because there are a lot of integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Using cached wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from beautifulsoup4->wikipedia) (2.3.2.post1)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=5d3a7cfc2b2550a9c3e6326b60b2e32933f629b87da1976c8b7d799b7ce9d32a\n",
      "  Stored in directory: /Users/mxagar/Library/Caches/pip/wheels/c2/46/f4/caa1bee71096d7b0cdca2f2a2af45cacf35c5760bee8f00948\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "# The tool Wikipedia needs to be installed separately\n",
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we instantiate an LLM, e.g., one from OpenAI,\n",
    "# and load the required tools. See a list of integrated tools and agent toolkits:\n",
    "#   https://python.langchain.com/docs/integrations/tools/\n",
    "#   https://python.langchain.com/docs/integrations/toolkits/\n",
    "# Note: Some require API keys\n",
    "# Also, note the agent types:\n",
    "#   https://python.langchain.com/docs/modules/agents/agent_types/\n",
    "# Examples:\n",
    "# - Math\n",
    "# - Google search\n",
    "# - Wikipedia\n",
    "# - Python interpreter: https://python.langchain.com/docs/integrations/toolkits/python\n",
    "# - Wolfram Alpha\n",
    "# - ...\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "# We should list the tools we consider necessary, not more, because the agent needs to decide\n",
    "# We sometimes can/need to pass to API keys in the parameters\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we instantiate an agent with the tools and the LLM\n",
    "# Zero-Shot React agents use the description and the list of tools\n",
    "# to decide on their own which tools to use\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out the year the film was released and then use the calculator to calculate the power.\n",
      "Action: Wikipedia\n",
      "Action Input: Departed (film)\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: The Departed\n",
      "Summary: The Departed is a 2006 American epic crime thriller film directed by Martin Scorsese and written by William Monahan. It is both a remake of the 2002 Hong Kong film Infernal Affairs and also loosely based on the real-life Boston Winter Hill Gang; the character Colin Sullivan is based on the corrupt FBI agent John Connolly, while the character Frank Costello is based on Irish-American gangster and crime boss Whitey Bulger. The film stars Leonardo DiCaprio, Matt Damon, Jack Nicholson, and Mark Wahlberg, with Martin Sheen, Ray Winstone, Vera Farmiga, Alec Baldwin, Anthony Anderson and James Badge Dale in supporting roles.\n",
      "The film takes place in Boston and the surrounding metro area, primarily in the city’s South End neighborhood. Irish Mob boss Frank Costello (Nicholson) plants Colin Sullivan (Damon) as a spy within the Massachusetts State Police; simultaneously, the police assign undercover state trooper Billy Costigan (DiCaprio) to infiltrate Costello's mob crew. When both sides realize the situation, Sullivan and Costigan each attempt to discover the other's identity before they are found out.\n",
      "The Departed was a critical and commercial success, receiving acclaim for its direction, performances (particularly of DiCaprio, Nicholson, and Wahlberg), screenplay, and editing.\n",
      "It won several accolades, including four Oscars at the 79th Academy Awards: for Best Picture, Best Director, Best Adapted Screenplay, and Best Film Editing. It became Scorsese's first and, to date, only personal Oscar win; Wahlberg was also nominated for Best Supporting Actor. The film also received six nominations at the 64th Golden Globe Awards, six nominations at the 60th British Academy Film Awards, and two nominations at the 13th Screen Actors Guild Awards. DiCaprio was nominated for Golden Globe Award for Best Actor – Motion Picture Drama (also nominated that year in the same category for Blood Diamond), BAFTA Award for Best Actor in a Leading Role and Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Supporting Role for his performance.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the year the film was released.\n",
      "Action: Calculator\n",
      "Action Input: 2006^0.43\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 26.30281917656938\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The film Departed with Leonardo Dicaprio was released in 2006 and this year raised to the 0.43 power is 26.30281917656938.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The film Departed with Leonardo Dicaprio was released in 2006 and this year raised to the 0.43 power is 26.30281917656938.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The reasoning process/workflow and tool usage to obtain the answer is shown\n",
    "agent.run(\"In what year was the film Departed with Leonardo Dicaprio released? What is this year raised to the 0.43 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly. A more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships.\n",
    "> \n",
    "> We call this ability to store information about past interactions \"memory\". LangChain provides a lot of utilities for adding memory to a system. These utilities can be used by themselves or incorporated seamlessly into a chain.\n",
    "\n",
    "So basically, we add a state to Chains and Agents so that information persists between calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "# verbose=True: the conversation is remembered and displayed\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: Can we talk about AI?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Absolutely! What would you like to know about AI?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Can we talk about AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: Can we talk about AI?\n",
      "AI:  Absolutely! What would you like to know about AI?\n",
      "Human: I'm interested in Reinforcement Learning.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Sure! Reinforcement Learning is a type of machine learning algorithm that allows an AI agent to learn from its environment by taking actions and receiving rewards or punishments. It is used to solve complex problems that require trial and error. Would you like to know more about how it works?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm interested in Reinforcement Learning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Document Loaders and Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document loaders enable loading external documents to be queried or used by the LLMs or the agents. After loading the documents, we usually need to index them, i.e., structure them so that LLMs can best interact with them. Utilities necessary for that:\n",
    "\n",
    "- Embeddings: numerical/vectorized representation of a piece of information, for example, text, documents, images, audio, etc. Those vectors capture the meaning in number format.\n",
    "- Text Splitters: long texts need to be split into chunks.\n",
    "- Vector stores: vector databases store and index vector embeddings from NLP models; similarity searches cn be performed by performing dot products between query and database vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (4.9.3)\n",
      "Requirement already satisfied: html2text in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (2020.1.16)\n"
     ]
    }
   ],
   "source": [
    "# lxml and html2text are required to parse EverNote notes\n",
    "!pip install lxml\n",
    "!pip install html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='This is a test note.', metadata={'source': './test.enex'})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important links with the document loaders:\n",
    "#   https://python.langchain.com/docs/modules/data_connection/document_loaders/\n",
    "#   https://python.langchain.com/docs/integrations/document_loaders/evernote\n",
    "#       Evernote, CSV, HTML, URL, PDF, Google Drive, Github, Git, Arxiv, AWS, ...\n",
    "from langchain.document_loaders import EverNoteLoader\n",
    "\n",
    "# By default all notes are combined into a single Document\n",
    "# I exported an Evernote note; we can also export an evernote notebook.\n",
    "loader = EverNoteLoader(\"./data/test.enex\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown loading requires the following package\n",
    "!pip install unstructured > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredMarkdownLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_path = \"./README.md\"\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"LangChain Introduction\\n\\nLangChain is a framework for developing applications powered by language models. It enables applications that are:\\n\\nData-aware: connect a language model to other sources of data\\n\\nAgentic: allow a language model to interact with its environment\\n\\nThe idea is that we have several modules with different functionalities which are used in chains to accomplish tasks. Check the official\\n\\nuse cases\\n\\nand list of modules.\\n\\nNote that LanChain uses models from other frameworks/vendors, such as OpenAI, Cohere, HuggingFace, etc; these are called integrations. As such, we need to have the specific keys or access tokens of those vendors. This mini-tutorial uses dotenv to load the keys defined in the .env file (not committed):\\n\\nOPENAI_API_KEY=xxx\\nHUGGINGFACEHUB_API_TOKEN=xxx\\n\\nSources followed to prepare this guide:\\n\\nLangChain Crash Course - Build apps with language models\\n\\nThe LangChain Cookbook - Beginner Guide To 7 Essential Concepts\\n\\n0. Setup\\n\\nOfficial websites:\\n\\ndocumentation\\n\\ngithub\\n\\nTo install langchain and the required integrations:\\n\\n```bash\\nconda activate ds\\n\\nInstall langchain without any integrations\\n\\npip install langchain\\n\\nThen, integrations are installed later as needed\\n\\npip install openai\\npip install huggingface_hub\\n\\nInstall all integrations\\n\\npip install 'langchain[all]'\\n```\\n\\n1. LLMs\", metadata={'source': './README.md'})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Indexing and Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from sentence_transformers) (4.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from sentence_transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from sentence_transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: nltk in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from sentence_transformers) (0.16.4)\n",
      "Requirement already satisfied: filelock in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
      "Requirement already satisfied: sympy in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.1)\n",
      "Requirement already satisfied: click in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from nltk->sentence_transformers) (8.1.6)\n",
      "Requirement already satisfied: joblib in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from nltk->sentence_transformers) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from torchvision->sentence_transformers) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# In order to convert the text into embedding vectors we need a package\n",
    "# The HaggingFace embeddings use SBERT\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.7.4-cp39-cp39-macosx_10_9_x86_64.whl (6.5 MB)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n"
     ]
    }
   ],
   "source": [
    "# We need to have a vector database.\n",
    "# Here, FAISS is used, but there are many other options:\n",
    "#   https://python.langchain.com/docs/modules/data_connection/vectorstores.html\n",
    "#   https://python.langchain.com/docs/integrations/vectorstores/\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://github.com/hwchase17/chroma-langchain/blob/master/state_of_the_union.txt\"\n",
    "res = requests.get(url)\n",
    "with open(\"data/state_of_the_union.txt\", \"w\") as f:\n",
    "    f.write(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loader\n",
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('./data/state_of_the_union.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Splitter: better to split big documents;\n",
    "# try different parameters and select which best works for each case\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, # 1000 characters\n",
    "                                      chunk_overlap=0) # no overlap between chunks\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings with HuggingFace and SBERT\n",
    "# We instantiate the embeddings object, which is passed to the vector database\n",
    "# to create embeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "# Vector database is create with the text (docs) and the and an embeddings generator (embeddings)\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
      "\n",
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
      "\n",
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n"
     ]
    }
   ],
   "source": [
    "# We can save locally teh database\n",
    "db.save_local(\"data/faiss_index\")\n",
    "new_db = FAISS.load_local(\"data/faiss_index\", embeddings)\n",
    "docs = new_db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Examples, Use-Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides additional examples and use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Summarization Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows to summarize arbitrarily long texts. Note that the usual promt/input + response text length is limited (e.g., to 4k tokens in OpenAI); however, we can split the text we'd like to summarize into chunks below that limit, compute the summaries of those, and the aggregate them to create an overall summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: tiktoken in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from tiktoken) (2023.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 622\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"January 2015, Paul Graham.\n",
      "My father is a mathematician. For most of my childhood he worked\n",
      "for Westinghouse, modelling nuclear reactors.He was one of those lucky people who know early on what they want to\n",
      "do.  When you talk to him about his childhood, there's a clear\n",
      "watershed at about age 12, when he \"got interested in maths.\"He\n",
      "grew up in the small Welsh seacoast town of Pwllheli.  As we retraced\n",
      "his walk to school on Google Street View, he said that it had been\n",
      "nice growing up in the country.\"Didn't it get boring when you got to be about 15?\" I asked.\"No,\" he said, \"by then I was interested in maths.\"In another conversation he told me that what he really liked was\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"solving problems.  To me the exercises at the end of each chapter\n",
      "in a math textbook represent work, or at best a way to reinforce\n",
      "what you learned in that chapter.  To him the problems were the\n",
      "reward.  The text of each chapter was just some advice about solving\n",
      "them. He said that as soon as he got a new textbook he'd immediately\n",
      "work out all the problems — to the slight annoyance of his teacher,\n",
      "since the class was supposed to work through the book gradually.Few people know so early or so certainly what they want to work on.\n",
      "But talking to my father reminded me of a heuristic the rest of us\n",
      "can use. If something that seems like work to other people doesn't\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"seem like work to you, that's something you're well suited for.\n",
      "For example, a lot of programmers I know, including me, actually\n",
      "like debugging.  It's not something people tend to volunteer; one\n",
      "likes it the way one likes popping zits. But you may have to like\n",
      "debugging to like programming, considering the degree to which\n",
      "programming consists of it.The stranger your tastes seem to other people, the stronger evidence\n",
      "they probably are of what you should do. When I was in college I\n",
      "used to write papers for my friends.  It was quite interesting to\n",
      "write a paper for a class I wasn't taking.  Plus they were always\n",
      "so relieved.It seemed curious that the same task could be painful to one person\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"and pleasant to another, but I didn't realize at the time what this\n",
      "imbalance implied, because I wasn't looking for it.  I didn't realize\n",
      "how hard it can be to decide what you should work on, and that you\n",
      "sometimes have to figure it out from subtle clues, like a detective\n",
      "solving a case in a mystery novel.  So I bet it would help a lot\n",
      "of people to ask themselves about this explicitly. What seems like\n",
      "work to other people that doesn't seem like work to you?\n",
      "Thanks to Sam Altman, Trevor Blackwell, Jessica Livingston,\n",
      "Robert Morris, and my father for reading drafts of this.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" Paul Graham's father was a mathematician who worked for Westinghouse modelling nuclear reactors. He knew from a young age that he wanted to pursue maths and grew up in the small Welsh seacoast town of Pwllheli. When asked if it got boring when he was 15, he said no, because he was already interested in maths.\n",
      "\n",
      " This passage reflects on the difference between the way the author and their father view math problems. The author sees them as work or a way to reinforce what they learned, while the father sees them as a reward. The author suggests that the rest of us can use the father's heuristic of finding something that seems like work to others, but doesn't feel like work to us.\n",
      "\n",
      " Debugging is an important part of programming and may be something that one needs to enjoy in order to enjoy programming. It is important to recognize that the things that seem strange to other people may be evidence of what one should do. An example of this is writing papers for friends in college, which was an interesting task and relieved them of a burden.\n",
      "\n",
      " This article discusses the difficulty of deciding what to work on and how to figure it out from subtle clues. It also thanks five people for reading drafts of the article.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Paul Graham reflects on the difference between his father's view of math problems as a reward and his own view of them as work. He suggests that we can use his father's heuristic of finding something that seems like work to others, but doesn't feel like work to us. He also discusses the importance of debugging in programming and how to recognize subtle clues that can help us decide what to work on.\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "loader = TextLoader('./data/work_pgraham.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Show number of tokens\n",
    "# Recall that many models have a limit of 4k tokens,\n",
    "# so we should split the text\n",
    "num_tokens = llm.get_num_tokens(documents[0].page_content)\n",
    "print(f\"Number of tokens: {num_tokens}\")\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# There is a lot of complexity hidden in this one line.\n",
    "# Check out this video for m ore details:\n",
    "# https://www.youtube.com/watch?v=f9_BWhCI4Zo\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Question & Answering Using Documents As Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give a context to our LLM and ask questions about it.\n",
    "\n",
    "We can:\n",
    "\n",
    "- chat with our documents,\n",
    "- ask questions to academic papers,\n",
    "- etc.\n",
    "\n",
    "Example product: [ChatPDF](https://www.chatpdf.com/).\n",
    "\n",
    "Another example: [How to query a book](https://www.youtube.com/watch?v=h0DHDp1FbmQ).\n",
    "\n",
    "In order to be able to work with complex or long texts, we use **embeddings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore we'll be using\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# The LangChain component we'll use to get the documents\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# The easy document loader for text\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# The embedding engine that will convert our text to vectors\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n",
      "You have 74678 characters in that document\n"
     ]
    }
   ],
   "source": [
    "# Load the text/document(s)\n",
    "loader = TextLoader('./data/worked_pgraham.txt')\n",
    "doc = loader.load()\n",
    "print (f\"You have {len(doc)} document\")\n",
    "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document(s)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
    "docs = text_splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 29 documents that have an average of 2,931 characters.\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of characters so we can see the average later\n",
    "num_total_characters = sum([len(x.page_content) for x in docs])\n",
    "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your embeddings engine ready\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Embed your documents and combine with the raw text in a pseudo db. Note: This will make an API call to OpenAI\n",
    "docsearch = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retrieval engine\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The author describes painting as good work.'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query: Ask\n",
    "query = \"What does the author describe as good work?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Extraction: Parse Information from a Document and Structure It"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common application would be to perform API or SQL calls from natural language. The next section is related to this use-case.\n",
    "\n",
    "Example of popular extraction library: [Kor](https://eyurtsev.github.io/kor/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To help construct our Chat Messages\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# We will be using a chat model, defaults to gpt-3.5-turbo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# To parse outputs and get structured data back\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You will be given a sentence with fruit names, extract those fruit names and assign an emoji to them\n",
    "Return the fruit name and emojis in a python dictionary\n",
    "\"\"\"\n",
    "\n",
    "fruit_names = \"\"\"\n",
    "Apple, Pear, this is an kiwi\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Apple\": \"🍎\",\n",
      "  \"Pear\": \"🍐\",\n",
      "  \"kiwi\": \"🥝\"\n",
      "}\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Make your prompt which combines the instructions w/ the fruit names\n",
    "prompt = (instructions + fruit_names)\n",
    "\n",
    "# Call the LLM\n",
    "output = chat_model([HumanMessage(content=prompt)])\n",
    "\n",
    "# Note that an LLM can only output strings,\n",
    "# thus we need to evaluate them in order to convert them into\n",
    "# python objects, as in this case a dictionary\n",
    "print (output.content)\n",
    "print (type(output.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Apple': '🍎', 'Pear': '🍐', 'kiwi': '🥝'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "output_dict = eval(output.content)\n",
    "\n",
    "print (output_dict)\n",
    "print (type(output_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction with ResponseSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangChain `ResponseSchema`:\n",
    "\n",
    "1. creates the prompt\n",
    "2. and evaluates the LLM response to the propper Python object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The schema I want out\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"artist\", description=\"The name of the musical artist\"),\n",
    "    ResponseSchema(name=\"song\", description=\"The name of the song that the artist plays\")\n",
    "]\n",
    "\n",
    "# The parser that will look for the LLM output in my schema and return it back to me\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# The format instructions that LangChain makes. Let's look at them\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompt template that brings it all together\n",
    "# Note: This is a different prompt template than before because we are using a Chat Model\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"Given a command from the user, extract the artist and song names \\n \\\n",
    "                                                    {format_instructions}\\n{user_prompt}\")  \n",
    "    ],\n",
    "    input_variables=[\"user_prompt\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a command from the user, extract the artist and song names \n",
      "                                                     The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n",
      "I really like So Young by Portugal. The Man\n"
     ]
    }
   ],
   "source": [
    "query = prompt.format_prompt(user_prompt=\"I really like So Young by Portugal. The Man\")\n",
    "print(query.messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artist': 'Portugal. The Man', 'song': 'So Young'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "output_llm = chat_model(query.to_messages())\n",
    "output = output_parser.parse(output_llm.content)\n",
    "\n",
    "print (output)\n",
    "print (type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Querying Tabular and SQL Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a SQL database of a CSV/Pandas table, we can ask with natural language analysis queries which are transformed to the required query language, e.g., SQL.\n",
    "\n",
    "More information: [Analyzing structured data](https://python.langchain.com/docs/use_cases/tabular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting langchain-experimental\n",
      "  Obtaining dependency information for langchain-experimental from https://files.pythonhosted.org/packages/56/2b/11189dadb84d0cf74bfccc12152ad18f6b076c885f4c8c1c84dff655822d/langchain_experimental-0.0.8-py3-none-any.whl.metadata\n",
      "  Downloading langchain_experimental-0.0.8-py3-none-any.whl.metadata (717 bytes)\n",
      "Requirement already satisfied: langchain>=0.0.239 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from langchain-experimental) (0.0.247)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from langchain>=0.0.239->langchain-experimental) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from langchain>=0.0.239->langchain-experimental) (2.0.19)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from langchain>=0.0.239->langchain-experimental) (3.8.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from langchain>=0.0.239->langchain-experimental) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from langchain>=0.0.239->langchain-experimental) (0.5.14)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from langchain>=0.0.239->langchain-experimental) (0.0.15)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from langchain>=0.0.239->langchain-experimental) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from langchain>=0.0.239->langchain-experimental) (1.24.3)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from langchain>=0.0.239->langchain-experimental) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from langchain>=0.0.239->langchain-experimental) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from langchain>=0.0.239->langchain-experimental) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from langchain>=0.0.239->langchain-experimental) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.239->langchain-experimental) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.239->langchain-experimental) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.239->langchain-experimental) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.239->langchain-experimental) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.239->langchain-experimental) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.239->langchain-experimental) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.239->langchain-experimental) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.239->langchain-experimental) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from pydantic<2,>=1->langchain>=0.0.239->langchain-experimental) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from requests<3,>=2->langchain>=0.0.239->langchain-experimental) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from requests<3,>=2->langchain>=0.0.239->langchain-experimental) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from requests<3,>=2->langchain>=0.0.239->langchain-experimental) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.239->langchain-experimental) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.239->langchain-experimental) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.239->langchain-experimental) (1.0.0)\n",
      "Downloading langchain_experimental-0.0.8-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain-experimental\n",
      "Successfully installed langchain-experimental-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_db_path = './data/San_Francisco_Trees.db'\n",
    "db = SQLDatabase.from_uri(f\"sqlite:///{sqlite_db_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mxagar/opt/anaconda3/envs/ds/lib/python3.9/site-packages/langchain_experimental/sql/base.py:65: UserWarning: Directly instantiating an SQLDatabaseChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "How many Species of trees are there in San Francisco?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT COUNT(DISTINCT \"qSpecies\") FROM \"SFTrees\";\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(578,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThere are 578 Species of trees in San Francisco.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are 578 Species of trees in San Francisco.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given the following prompt, these steps/tasks are carried ou automatically:\n",
    "# 1. Find which table to use\n",
    "# 2. Find which column to use\n",
    "# 3. Construct the correct sql query\n",
    "# 4. Execute that query\n",
    "# 5. Get the result\n",
    "# 6. Return a natural language reponse back\n",
    "db_chain.run(\"How many Species of trees are there in San Francisco?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Code Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can upload a complete codebase to the LLM and query it for function calls to specific tasks using natural language.\n",
    "\n",
    "As an example, the code of the repository [The Fuzz](https://github.com/seatgeek/thefuzz) is used; this open source package can be used to detect text similarities using Levenshtein distances (so no LLMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to read local files\n",
    "import os\n",
    "\n",
    "# Vector Support\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# The LangChain component we'll use to get the documents\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Model and chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Text splitters\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './data/thefuzz-master'\n",
    "docs = []\n",
    "\n",
    "# Go through each folder\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    \n",
    "    # Go through each file\n",
    "    for file in filenames:\n",
    "        try: \n",
    "            # Load up the file as a doc and split\n",
    "            loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n",
    "            docs.extend(loader.load_and_split())\n",
    "        except Exception as e: \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 175 documents\n",
      "\n",
      "------ Start Document ------\n",
      "import unittest\n",
      "import re\n",
      "import pycodestyle\n",
      "\n",
      "from thefuzz import fuzz\n",
      "from thefuzz import process\n",
      "from thefuzz import utils\n",
      "from thefuzz.string_processing import StringProcessor\n",
      "\n",
      "\n",
      "class StringProcessingTest(unittest.TestCase):\n",
      "    def test_replace_non_letters_non_numbers_with_whitespace(self):\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(docs)} documents\\n\")\n",
    "print (\"------ Start Document ------\")\n",
    "print (docs[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document embeddings and persist in vector store\n",
    "docsearch = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our retriever ready\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use the `process.extractOne` function from the `thefuzz` package to find the most similar item in a list of items. Here's an example:\n",
      "\n",
      "```python\n",
      "from thefuzz import process\n",
      "\n",
      "choices = [\"apple\", \"banana\", \"orange\", \"grape\"]\n",
      "query = \"banana\"\n",
      "\n",
      "best_match = process.extractOne(query, choices)\n",
      "print(best_match)\n",
      "```\n",
      "\n",
      "Output:\n",
      "```\n",
      "(\"banana\", 100)\n",
      "```\n",
      "\n",
      "The `extractOne` function returns a tuple containing the best match and its similarity score. In this example, \"banana\" is the best match with a similarity score of 100.\n"
     ]
    }
   ],
   "source": [
    "query = \"What function do I use if I want to find the most similar item in a list of items?\"\n",
    "output = qa.run(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query = \"new york mets at chicago cubs\"\n",
      "choices = [\n",
      "    None,\n",
      "    \"new york mets vs chicago cubs\",\n",
      "    \"new york yankees vs boston red sox\",\n",
      "    None,\n",
      "    None\n",
      "]\n",
      "\n",
      "best = process.extractOne(query, choices)\n",
      "print(best[0])\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you write the code to use the process.extractOne() function? Only respond with code. No other text or explanation\"\n",
    "output = qa.run(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Interacting with APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can upload the documentation of an API and query with natural language to the API; the endpoint calls (including parameters) are automatically performed and the results parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import APIChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_docs = \"\"\"\n",
    "\n",
    "BASE URL: https://restcountries.com/\n",
    "\n",
    "API Documentation:\n",
    "\n",
    "The API endpoint /v3.1/name/{name} Used to find informatin about a country. All URL parameters are listed below:\n",
    "    - name: Name of country - Ex: italy, france\n",
    "    \n",
    "The API endpoint /v3.1/currency/{currency} Uesd to find information about a region. All URL parameters are listed below:\n",
    "    - currency: 3 letter currency. Example: USD, COP\n",
    "    \n",
    "Woo! This is my documentation\n",
    "\"\"\"\n",
    "\n",
    "chain_new = APIChain.from_llm_and_api_docs(llm, api_docs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new APIChain chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://restcountries.com/v3.1/name/france\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{\"name\":{\"common\":\"France\",\"official\":\"French Republic\",\"nativeName\":{\"fra\":{\"official\":\"République française\",\"common\":\"France\"}}},\"tld\":[\".fr\"],\"cca2\":\"FR\",\"ccn3\":\"250\",\"cca3\":\"FRA\",\"cioc\":\"FRA\",\"independent\":true,\"status\":\"officially-assigned\",\"unMember\":true,\"currencies\":{\"EUR\":{\"name\":\"Euro\",\"symbol\":\"€\"}},\"idd\":{\"root\":\"+3\",\"suffixes\":[\"3\"]},\"capital\":[\"Paris\"],\"altSpellings\":[\"FR\",\"French Republic\",\"République française\"],\"region\":\"Europe\",\"subregion\":\"Western Europe\",\"languages\":{\"fra\":\"French\"},\"translations\":{\"ara\":{\"official\":\"الجمهورية الفرنسية\",\"common\":\"فرنسا\"},\"bre\":{\"official\":\"Republik Frañs\",\"common\":\"Frañs\"},\"ces\":{\"official\":\"Francouzská republika\",\"common\":\"Francie\"},\"cym\":{\"official\":\"French Republic\",\"common\":\"France\"},\"deu\":{\"official\":\"Französische Republik\",\"common\":\"Frankreich\"},\"est\":{\"official\":\"Prantsuse Vabariik\",\"common\":\"Prantsusmaa\"},\"fin\":{\"official\":\"Ranskan tasavalta\",\"common\":\"Ranska\"},\"fra\":{\"official\":\"République française\",\"common\":\"France\"},\"hrv\":{\"official\":\"Francuska Republika\",\"common\":\"Francuska\"},\"hun\":{\"official\":\"Francia Köztársaság\",\"common\":\"Franciaország\"},\"ita\":{\"official\":\"Repubblica francese\",\"common\":\"Francia\"},\"jpn\":{\"official\":\"フランス共和国\",\"common\":\"フランス\"},\"kor\":{\"official\":\"프랑스 공화국\",\"common\":\"프랑스\"},\"nld\":{\"official\":\"Franse Republiek\",\"common\":\"Frankrijk\"},\"per\":{\"official\":\"جمهوری فرانسه\",\"common\":\"فرانسه\"},\"pol\":{\"official\":\"Republika Francuska\",\"common\":\"Francja\"},\"por\":{\"official\":\"República Francesa\",\"common\":\"França\"},\"rus\":{\"official\":\"Французская Республика\",\"common\":\"Франция\"},\"slk\":{\"official\":\"Francúzska republika\",\"common\":\"Francúzsko\"},\"spa\":{\"official\":\"República francés\",\"common\":\"Francia\"},\"srp\":{\"official\":\"Француска Република\",\"common\":\"Француска\"},\"swe\":{\"official\":\"Republiken Frankrike\",\"common\":\"Frankrike\"},\"tur\":{\"official\":\"Fransa Cumhuriyeti\",\"common\":\"Fransa\"},\"urd\":{\"official\":\"جمہوریہ فرانس\",\"common\":\"فرانس\"},\"zho\":{\"official\":\"法兰西共和国\",\"common\":\"法国\"}},\"latlng\":[46.0,2.0],\"landlocked\":false,\"borders\":[\"AND\",\"BEL\",\"DEU\",\"ITA\",\"LUX\",\"MCO\",\"ESP\",\"CHE\"],\"area\":551695.0,\"demonyms\":{\"eng\":{\"f\":\"French\",\"m\":\"French\"},\"fra\":{\"f\":\"Française\",\"m\":\"Français\"}},\"flag\":\"\\uD83C\\uDDEB\\uD83C\\uDDF7\",\"maps\":{\"googleMaps\":\"https://goo.gl/maps/g7QxxSFsWyTPKuzd7\",\"openStreetMaps\":\"https://www.openstreetmap.org/relation/1403916\"},\"population\":67391582,\"gini\":{\"2018\":32.4},\"fifa\":\"FRA\",\"car\":{\"signs\":[\"F\"],\"side\":\"right\"},\"timezones\":[\"UTC-10:00\",\"UTC-09:30\",\"UTC-09:00\",\"UTC-08:00\",\"UTC-04:00\",\"UTC-03:00\",\"UTC+01:00\",\"UTC+02:00\",\"UTC+03:00\",\"UTC+04:00\",\"UTC+05:00\",\"UTC+10:00\",\"UTC+11:00\",\"UTC+12:00\"],\"continents\":[\"Europe\"],\"flags\":{\"png\":\"https://flagcdn.com/w320/fr.png\",\"svg\":\"https://flagcdn.com/fr.svg\",\"alt\":\"The flag of France is composed of three equal vertical bands of blue, white and red.\"},\"coatOfArms\":{\"png\":\"https://mainfacts.com/media/images/coats_of_arms/fr.png\",\"svg\":\"https://mainfacts.com/media/images/coats_of_arms/fr.svg\"},\"startOfWeek\":\"monday\",\"capitalInfo\":{\"latlng\":[48.87,2.33]},\"postalCode\":{\"format\":\"#####\",\"regex\":\"^(\\\\d{5})$\"}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' France is an officially-assigned, independent country located in Western Europe. Its capital is Paris and its official language is French. Its currency is the Euro (€). It has a population of 67,391,582 and its borders are with Andorra, Belgium, Germany, Italy, Luxembourg, Monaco, Spain, and Switzerland.'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_new.run('Can you tell me information about france?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new APIChain chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m https://restcountries.com/v3.1/currency/COP\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m[{\"name\":{\"common\":\"Colombia\",\"official\":\"Republic of Colombia\",\"nativeName\":{\"spa\":{\"official\":\"República de Colombia\",\"common\":\"Colombia\"}}},\"tld\":[\".co\"],\"cca2\":\"CO\",\"ccn3\":\"170\",\"cca3\":\"COL\",\"cioc\":\"COL\",\"independent\":true,\"status\":\"officially-assigned\",\"unMember\":true,\"currencies\":{\"COP\":{\"name\":\"Colombian peso\",\"symbol\":\"$\"}},\"idd\":{\"root\":\"+5\",\"suffixes\":[\"7\"]},\"capital\":[\"Bogotá\"],\"altSpellings\":[\"CO\",\"Republic of Colombia\",\"República de Colombia\"],\"region\":\"Americas\",\"subregion\":\"South America\",\"languages\":{\"spa\":\"Spanish\"},\"translations\":{\"ara\":{\"official\":\"جمهورية كولومبيا\",\"common\":\"كولومبيا\"},\"bre\":{\"official\":\"Republik Kolombia\",\"common\":\"Kolombia\"},\"ces\":{\"official\":\"Kolumbijská republika\",\"common\":\"Kolumbie\"},\"cym\":{\"official\":\"Gweriniaeth Colombia\",\"common\":\"Colombia\"},\"deu\":{\"official\":\"Republik Kolumbien\",\"common\":\"Kolumbien\"},\"est\":{\"official\":\"Colombia Vabariik\",\"common\":\"Colombia\"},\"fin\":{\"official\":\"Kolumbian tasavalta\",\"common\":\"Kolumbia\"},\"fra\":{\"official\":\"République de Colombie\",\"common\":\"Colombie\"},\"hrv\":{\"official\":\"Republika Kolumbija\",\"common\":\"Kolumbija\"},\"hun\":{\"official\":\"Kolumbiai Köztársaság\",\"common\":\"Kolumbia\"},\"ita\":{\"official\":\"Repubblica di Colombia\",\"common\":\"Colombia\"},\"jpn\":{\"official\":\"コロンビア共和国\",\"common\":\"コロンビア\"},\"kor\":{\"official\":\"콜롬비아 공화국\",\"common\":\"콜롬비아\"},\"nld\":{\"official\":\"Republiek Colombia\",\"common\":\"Colombia\"},\"per\":{\"official\":\"جمهوری کلمبیا\",\"common\":\"کلمبیا\"},\"pol\":{\"official\":\"Republika Kolumbii\",\"common\":\"Kolumbia\"},\"por\":{\"official\":\"República da Colômbia\",\"common\":\"Colômbia\"},\"rus\":{\"official\":\"Республика Колумбия\",\"common\":\"Колумбия\"},\"slk\":{\"official\":\"Kolumbijská republika\",\"common\":\"Kolumbia\"},\"spa\":{\"official\":\"República de Colombia\",\"common\":\"Colombia\"},\"srp\":{\"official\":\"Република Колумбија\",\"common\":\"Колумбија\"},\"swe\":{\"official\":\"Republiken Colombia\",\"common\":\"Colombia\"},\"tur\":{\"official\":\"Kolombiya Cumhuriyeti\",\"common\":\"Kolombiya\"},\"urd\":{\"official\":\"جمہوریہ کولمبیا\",\"common\":\"کولمبیا\"},\"zho\":{\"official\":\"哥伦比亚共和国\",\"common\":\"哥伦比亚\"}},\"latlng\":[4.0,-72.0],\"landlocked\":false,\"borders\":[\"BRA\",\"ECU\",\"PAN\",\"PER\",\"VEN\"],\"area\":1141748.0,\"demonyms\":{\"eng\":{\"f\":\"Colombian\",\"m\":\"Colombian\"},\"fra\":{\"f\":\"Colombienne\",\"m\":\"Colombien\"}},\"flag\":\"\\uD83C\\uDDE8\\uD83C\\uDDF4\",\"maps\":{\"googleMaps\":\"https://goo.gl/maps/RdwTG8e7gPwS62oR6\",\"openStreetMaps\":\"https://www.openstreetmap.org/relation/120027\"},\"population\":50882884,\"gini\":{\"2019\":51.3},\"fifa\":\"COL\",\"car\":{\"signs\":[\"CO\"],\"side\":\"right\"},\"timezones\":[\"UTC-05:00\"],\"continents\":[\"South America\"],\"flags\":{\"png\":\"https://flagcdn.com/w320/co.png\",\"svg\":\"https://flagcdn.com/co.svg\",\"alt\":\"The flag of Colombia is composed of three horizontal bands of yellow, blue and red, with the yellow band twice the height of the other two bands.\"},\"coatOfArms\":{\"png\":\"https://mainfacts.com/media/images/coats_of_arms/co.png\",\"svg\":\"https://mainfacts.com/media/images/coats_of_arms/co.svg\"},\"startOfWeek\":\"monday\",\"capitalInfo\":{\"latlng\":[4.71,-74.07]}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The currency of Colombia is the Colombian peso (COP), symbolized by the \"$\" sign.'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_new.run('Can you tell me about the currency COP?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create chatbots by leveraging the memory feature provided by LangChain.\n",
    "\n",
    "For more information, check [Use-Cases: Chatbots](https://python.langchain.com/docs/use_cases/chatbots.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Chat specific components\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a chatbot that is unhelpful.\n",
    "Your goal is to not help the user but only make jokes.\n",
    "Take what the user is saying and make a joke out of it\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], \n",
    "    template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=OpenAI(), \n",
    "    prompt=prompt, \n",
    "    verbose=True, \n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a chatbot that is unhelpful.\n",
      "Your goal is to not help the user but only make jokes.\n",
      "Take what the user is saying and make a joke out of it\n",
      "\n",
      "\n",
      "Human: Is an pear a fruit or vegetable?\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" It's a mystery as juicy as a pear itself!\""
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"Is an pear a fruit or vegetable?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a chatbot that is unhelpful.\n",
      "Your goal is to not help the user but only make jokes.\n",
      "Take what the user is saying and make a joke out of it\n",
      "\n",
      "Human: Is an pear a fruit or vegetable?\n",
      "AI:  It's a mystery as juicy as a pear itself!\n",
      "Human: What was one of the fruits I first asked you about?\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Well, I think it had something to do with an apple, but I could be pear-y wrong!'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"What was one of the fruits I first asked you about?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
