{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Deep Learning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Convolutional Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hubel & Wiesel won in 1981 the Nobel prize for showing how the visual cortex of a mammal processes images:\n",
    "- Neurons focus on specific local regions of the image, not the complete image\n",
    "- Neurons activate to particular features in the image\n",
    "LeCun published in 1998 LeNet-5, introducing convolutional neural networks.\n",
    "\n",
    "We use **tensors** (n-dimensional arrays of arrays): scalar -> vector/array -> matrix -> tensor; with images, a typical example could be: `[Images, Height, Width, Channel]`.\n",
    "\n",
    "**Dense vs Convolutional layers**:\n",
    "- In a densly or fully connected layer, all neurons are connected\n",
    "- In a CNN, we have kernels or filters that sweep the image with convolution operations. Pixel values under a filter location are processed to one value, and become a unique pixel of a feature in the convolutional layer; hence, only the local/neighboring pixels (= neurons) under a convolutional filter are connected, not the pixels of the complete image!\n",
    "- Additionally, in each convolutional layer, we can have `N` filters; thus, the output is a **feature map** of `N` channels (or `N` feature maps?).\n",
    "\n",
    "We call **feedforward** pass to the action of inputing an image to a ANN/CNN to obtain the predicted class.\n",
    "\n",
    "**Padding** is necessary if the convolutional filter is centered in the edge pixels of the image (e.g., fill imaginary pixels with value 0); if we match the top-left corner of the filter with the image, no padding is used.\n",
    "\n",
    "We move the filter left-right & top-down with a **stride** of `s` pixels; it somehow defines how fast we move with the kernel/filter on the image.\n",
    "\n",
    "**Filters** are `n x n` **kernels** that are applied to transfom pixel values according to the neighboring pixel-value distribution. Filter weights are the ones that are estimated during training. The result of applying a filter to an image is a **feature map**, aka. **activation map**.\n",
    "\n",
    "In addition to convolutions, we can use **pooling**: in a local neighborhood (eg., `2 x 2` window) of the image/feature map, we take a representative pixel value, eg.: maximum, average, etc.\n",
    "\n",
    "It is common to use **dropout**, too: neurons are randomly de-activated in a feedforward pass; this has the effect of preventing **overfitting**.\n",
    "\n",
    "The image size is **compressed** as we apply convolutions and pooling. Deeper layers account for higher level features.\n",
    "\n",
    "Note: convolutions are not exclusive of images; we can apply them to **1D** data, too (e.g., a series); in the special case of images we have **2D** convolutions with 2D filters.\n",
    "\n",
    "Popular neural network architectures (they are basically a set of designed layers):\n",
    "- LeNet-5\n",
    "- AlexNet\n",
    "- GoogLeNet\n",
    "- ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset is a popular benchmarking dataset in which we ave samples of hand-written symbols for 0-9 digits. Here are the properties:\n",
    "- 60k training images, 10k test images\n",
    "- Each image is 28x28 pixels, single channel\n",
    "- Pixel values are normalized to [0,1]\n",
    "- The dataset is a 4-dimensional array:\n",
    "    (Samples, Width, Height, Channels) -> (60000, 28, 28, 1)\n",
    "- Labels come in an array in which each element is the digit label 0-9; but we transform it into the **one-hot-encondig**: 0/1 for each of the 0-9 classes, i.e.: (Samples, Classes) -> (60000, 10); 4 = (0,0,0,0,1,0,0,0,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train,y_train), (x_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9ddd084ad0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img, cmap='gray') # to reverse cmap, append _r: 'gray_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We transform y to one-hot\n",
    "# in order to avoid the network to think it's some kind of regression\n",
    "# We could do it with a for loop\n",
    "# but there's a utility for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_test = to_categorical(y_test,10)\n",
    "y_cat_train = to_categorical(y_train,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to normalize the pixel values: [0,255] -> [0,1]\n",
    "img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test / x_test.max()\n",
    "x_train = x_train / x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to reshape x to have a field for channels\n",
    "# eventhough we have a single channel in our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((60000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "# We load all layer types we're going to use\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the model with the layers\n",
    "model = Sequential()\n",
    "\n",
    "# CONVOLUTIONAL layer:\n",
    "# 32 filters is quite standard (if complex images, more)\n",
    "# 4x4 size for the filter kernels is also standard (3-4)\n",
    "# input shape (for each image) is given by the dataset image shape\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4), input_shape=(28,28,1), activation='relu'))\n",
    "\n",
    "# POOLING layer:\n",
    "# 2x2 is also quiste a standard size for max-pooling\n",
    "# Note: in the new version of keras it's called MaxPooling2D - but MaxPool2D also works\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# FLATTEN layer: 2D (images) --> 1D (for later class array)\n",
    "model.add(Flatten())\n",
    "\n",
    "# Map the flattened array to the class array with a fully connected layer\n",
    "# The mapping is done in 2 steps (that's also quite usual)\n",
    "# 28x28 = 784\n",
    "# 28 - (3) = 25 -> 25x25 = 625; 4x4 kernel without padding and stride=1 results in 25 pixels\n",
    "# floor(25/2)*floor(25/2) = 12x12 = 144; max-pool: every 4 pixels 1 (max) taken\n",
    "# 144 -> 128 -> 10 (classes)\n",
    "# Often powers of 2 are used as steps: 64, 128, 256, 512, 1024, ...\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax')) # last activation to classes requires softmax for probabilities\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 25, 25, 32)        544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 591,786\n",
      "Trainable params: 591,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 75s 1ms/sample - loss: 0.1332 - accuracy: 0.9599\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 76s 1ms/sample - loss: 0.0464 - accuracy: 0.9858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9d99785d50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model.fit(x_train, y_cat_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation\n",
    "# Get metrics used to evaluate: loss, acc\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04409347255442117, 0.987]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test set should be similar to train set in terms of accuracy\n",
    "# If train set is much better, we have overfitted\n",
    "# loss, accuracy\n",
    "model.evaluate(x_test, y_cat_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE:\n",
    "# model.predict_classes() does not deliver predictions as one-hot\n",
    "# but as class categories 0-9\n",
    "# HOWEVER, note that we train with categorical y values!\n",
    "# This difference is relevant for the classification report\n",
    "y_pred = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       980\n",
      "           1       0.99      1.00      0.99      1135\n",
      "           2       0.99      0.98      0.99      1032\n",
      "           3       0.98      0.99      0.99      1010\n",
      "           4       0.99      0.99      0.99       982\n",
      "           5       0.98      0.99      0.98       892\n",
      "           6       0.99      0.98      0.99       958\n",
      "           7       0.98      0.99      0.98      1028\n",
      "           8       0.99      0.97      0.98       974\n",
      "           9       0.99      0.98      0.98      1009\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We get very good results\n",
    "# This type of standard conv networks (withthe architecture we defined)\n",
    "# seems to work very well for hand-written digits\n",
    "print(classification_report(y_test, y_pred)) # labels, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 16s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train,y_train), (x_test,y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 32x32 images, 3 channels\n",
    "# train: 50k images\n",
    "# test: 10k images\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9dc91a3690>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "x_test = x_test / x_test.max()\n",
    "x_train = x_train / x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [9],\n",
       "       [9],\n",
       "       ...,\n",
       "       [9],\n",
       "       [1],\n",
       "       [1]], dtype=uint8)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to one-hot encoding\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_test = to_categorical(y_test,10)\n",
    "y_cat_train = to_categorical(y_train,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the category names from Wikipedia:\n",
    "# https://en.wikipedia.org/wiki/CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['airplane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(category_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the model with the layers\n",
    "model = Sequential()\n",
    "\n",
    "# CONVOLUTIONAL layer 1\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4), input_shape=(32,32,3), activation='relu'))\n",
    "# POOLING layer:\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# CONVOLUTIONAL layer 2: since images are more complex, we add another conv-maxpool pair\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4), activation='relu'))\n",
    "# POOLING layer:\n",
    "# Note: in the new version of keras it's called MaxPooling2D - but MaxPool2D also works\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# FLATTEN layer: 2D (images) --> 1D (for later class array)\n",
    "model.add(Flatten())\n",
    "\n",
    "# Map the flattened array to the class array with a fully connected layer\n",
    "# Often powers of 2 are used; 512 gives a slightly better performance\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax')) # last activation to classes requires softmax for probabilities\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 29, 29, 32)        1568      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 32)        16416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               205056    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 225,610\n",
      "Trainable params: 225,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/2\n",
      "50000/50000 [==============================] - 104s 2ms/sample - loss: 1.5269 - accuracy: 0.4517\n",
      "Epoch 2/2\n",
      "50000/50000 [==============================] - 102s 2ms/sample - loss: 1.1732 - accuracy: 0.5880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9da9845a50>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model.fit(x_train, y_cat_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2094071035385132, 0.5733]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_cat_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.54      0.61      1000\n",
      "           1       0.90      0.47      0.62      1000\n",
      "           2       0.56      0.38      0.45      1000\n",
      "           3       0.39      0.56      0.46      1000\n",
      "           4       0.49      0.53      0.51      1000\n",
      "           5       0.66      0.26      0.38      1000\n",
      "           6       0.72      0.64      0.68      1000\n",
      "           7       0.48      0.79      0.60      1000\n",
      "           8       0.54      0.88      0.67      1000\n",
      "           9       0.71      0.69      0.70      1000\n",
      "\n",
      "    accuracy                           0.57     10000\n",
      "   macro avg       0.62      0.57      0.57     10000\n",
      "weighted avg       0.62      0.57      0.57     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred)) # labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
