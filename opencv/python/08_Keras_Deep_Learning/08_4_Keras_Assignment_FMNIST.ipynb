{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Deep Learning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Assignment: Training and Inference with FMNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and prepare it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train,y_train), (x_test,y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa15e50ff90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, 3, 0], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/zalandoresearch/fashion-mnist\n",
    "class_names = ['tshirt_top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle_boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to one-hot format/encoding\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_test = to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_train = to_categorical(y_train,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "x_test = x_test / x_test.max()\n",
    "x_train = x_train / x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((60000, 28, 28, 1))\n",
    "x_test = x_test.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "# We load all layer types we're going to use\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the model with the layers\n",
    "model = Sequential()\n",
    "\n",
    "# CONVOLUTIONAL layer:\n",
    "# 32 filters is quite standard (if complex images, more)\n",
    "# 4x4 size for the filter kernels is also standard (3-4)\n",
    "# input shape (for each image) is given by the dataset image shape\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4), input_shape=(28,28,1), activation='relu'))\n",
    "\n",
    "# POOLING layer:\n",
    "# 2x2 is also quiste a standard size for max-pooling\n",
    "# Note: in the new version of keras it's called MaxPooling2D - but MaxPool2D also works\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# FLATTEN layer: 2D (images) --> 1D (for later class array)\n",
    "model.add(Flatten())\n",
    "\n",
    "# Map the flattened array to the class array with a fully connected layer\n",
    "# The mapping is done in 2 steps (that's also quite usual)\n",
    "# 28x28 = 784\n",
    "# 28 - (3) = 25 -> 25x25 = 625; 4x4 kernel without padding and stride=1 results in 25 pixels\n",
    "# floor(25/2)*floor(25/2) = 12x12 = 144; max-pool: every 4 pixels 1 (max) taken\n",
    "# 144 -> 128 -> 10 (classes)\n",
    "# Often powers of 2 are used as steps: 64, 128, 256, 512, 1024, ...\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax')) # last activation to classes requires softmax for probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 25, 25, 32)        544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 591,786\n",
      "Trainable params: 591,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE: We could apply early stopping here too using the test split as the validation set; look at notebook 08_2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.4021 - accuracy: 0.8563\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 88s 1ms/sample - loss: 0.2766 - accuracy: 0.9004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa0e8e00cd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model.fit(x_train, y_cat_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation\n",
    "# Get metrics used to evaluate: loss, acc\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 33s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27677204971313474, 0.8997]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test set should be similar to train set in terms of accuracy\n",
    "# If train set is much better, we have overfitted\n",
    "# loss, acc\n",
    "model.evaluate(x_test, y_cat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE:\n",
    "# model.predict_classes() does not deliver predictions as one-hot\n",
    "# but as class categories 0-9\n",
    "# HOWEVER, note that we train with categorical y values!\n",
    "# This difference is relevant for the classification report\n",
    "y_pred = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1, ..., 8, 1, 5])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.84      0.85      1000\n",
      "          1       0.99      0.98      0.98      1000\n",
      "          2       0.83      0.82      0.83      1000\n",
      "          3       0.87      0.94      0.90      1000\n",
      "          4       0.83      0.84      0.84      1000\n",
      "          5       0.97      0.98      0.98      1000\n",
      "          6       0.74      0.69      0.71      1000\n",
      "          7       0.96      0.95      0.96      1000\n",
      "          8       0.97      0.98      0.98      1000\n",
      "          9       0.96      0.97      0.97      1000\n",
      "\n",
      "avg / total       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We get very good results\n",
    "# This type of standard conv networks (withthe architecture we defined)\n",
    "# seems to work very well for hand-written digits\n",
    "print(classification_report(y_test, y_pred)) # labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
