{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72566782-321f-4e1c-9ae4-8572b6fa99df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Audio Classification\n",
    "\n",
    "# Libraries\n",
    "\n",
    "Balancing, torch, torchaudio, and transformers can be tricky! Here are the versions used for this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a531a21a-2753-4230-94f1-4c4c3b11bb85",
   "metadata": {},
   "source": [
    "## Library and Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "458a072c-f306-49c7-9af4-675a1850fe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the versions used for this notebook, but watch the lecture for an important note on this\n",
      "2.2.1+cpu\n",
      "2.2.1+cpu\n",
      "4.26.1\n"
     ]
    }
   ],
   "source": [
    "import torch, transformers, torchaudio\n",
    "print(\"These are the versions used for this notebook, but watch the lecture for an important note on this\")\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe422402-cac1-4962-9d91-be9af4b7e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5d0307d-bb51-40fe-bf3a-bb99501aff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d06e7008-d918-4f2a-84f1-ed9700d00e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "audio_path = 'example.mp3'\n",
    "y, sr = librosa.load(audio_path, sr=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee2059-3f2b-4273-8f84-e4bcaae2419d",
   "metadata": {},
   "source": [
    "## Sampling Rate Issues\n",
    "\n",
    "Recall that most ML models are trained on 16 kHz sampling rate, you will run into issues if you try to force your own sampling rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da8caf3d-0511-457f-a1a3-377921e358a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR!\n",
    "# result = feature_extractor(y,sampling_rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c87cec69-240e-44d9-897b-f7ae5c08b052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "result = feature_extractor(y,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "728a1b1f-61fc-4111-afc5-bb186e706b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[[ 0.0770, -0.2676,  0.1092,  ..., -1.2776, -1.2776, -1.2776],\n",
       "         [ 0.0846, -0.2771,  0.0997,  ..., -1.2776, -1.2776, -1.2776],\n",
       "         [-0.2939, -0.3674,  0.0095,  ..., -1.2776, -1.2776, -1.2776],\n",
       "         ...,\n",
       "         [ 0.2184, -0.0845,  0.2923,  ..., -1.2776, -1.2776, -1.2776],\n",
       "         [ 0.1963, -0.1293,  0.2475,  ..., -1.2776, -1.2776, -1.2776],\n",
       "         [-0.0509, -0.4521, -0.0752,  ..., -1.2776, -1.2776, -1.2776]]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae49bafd-8bb0-408b-b5bf-1ed7d4e254aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e4efed1-1818-403b-b32d-5653d84b5b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_logits = model(result['input_values']).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98da0a86-5902-49a3-b062-de29467861dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3dfc19c-b0ab-4b67-95c5-ee9df7361908",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_ids = torch.argmax(prediction_logits, dim=-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c23e65a9-6d79-4e3c-88ed-d425f0b835f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = model.config.id2label[predicted_class_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a86578c-bba3-48b3-be8d-e28aa5a17776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Music'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f843c03b-cf0c-4922-98bb-dbe07d5af1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe6d98-87b8-4086-95c1-d96bf40536d8",
   "metadata": {},
   "source": [
    "## Pipeline for Audio Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7bb1d37-859f-4cee-8979-aea1ab77038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"audio-classification\", model=\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d725f7ef-cc90-433d-b5da-af0b141a9cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASTForAudioClassification(\n",
       "  (audio_spectrogram_transformer): ASTModel(\n",
       "    (embeddings): ASTEmbeddings(\n",
       "      (patch_embeddings): ASTPatchEmbeddings(\n",
       "        (projection): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ASTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ASTLayer(\n",
       "          (attention): ASTAttention(\n",
       "            (attention): ASTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): ASTMLPHead(\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=768, out_features=527, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "160631d9-bcc8-4771-a43b-56cbd6d428a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marcial\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\audio_spectrogram_transformer\\feature_extraction_audio_spectrogram_transformer.py:96: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  waveform = torch.from_numpy(waveform).unsqueeze(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.48486804962158203, 'label': 'Music'},\n",
       " {'score': 0.1913108080625534, 'label': 'Violin, fiddle'},\n",
       " {'score': 0.08519719541072845, 'label': 'Musical instrument'},\n",
       " {'score': 0.046924274414777756, 'label': 'Bowed string instrument'},\n",
       " {'score': 0.045361001044511795, 'label': 'Orchestra'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe('example.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64d3ea93-a1b1-4272-b42d-87a04964f0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pipe.model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b69fa-ac10-49a7-9282-c93b4de70a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
