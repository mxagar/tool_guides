Python Manual
Mikel Sagardia 2017 - 2023
No warranties

I started making these notes after some years of Python usage; I copied
snippets I had collected while using Python as an auxiliary scripting language.
Then, I extended the file with videos from different courses:

- DLR Course: Python for Scientific Programming
- Udemy Course: Python Bootcamp, Jose Portilla
- Udemy Course: Automate Boring Stuff, Al Sweigart
- Udemy Course: Python for DS & ML, Jose Portilla
- Udemy Course: Python for Financial Analysis, Jose Portilla

Overview

    Python: General Use
        Setup
        Numbers
        Strings
        Print formatting
        Lists
        Dictionaries
        Tuples
        Sets
        File I/O
        Control Flow
        Useful Operators
            range
            enumerate
            zip
            in
            min, max
            random
            input
        List comprehensions
        Functions
        Map & Filter
        Lambda expressions
        Nested Statements & Scope
        OOP
            Classes
            Inheritance
            Polymorfism
            Overloading
            Multiple Inheritance
            Advanced Functionalities
                Class Methods, Instance Methods, Static Methods
                Class Attributes, Instance Attributes
                Multiple Inheritance and Mixins
                Abstract Base Classes: abc

        Modules & Packages
            PyPi
            Create new
            __main__ & __name__
        Running Scripts
        Error & Exception handling: try & except, raise, assert
        Logging
        Linting: pylint
        Unit testing: unittest
        Decorators
        Generators
    
    Python: Handling common objects
        Files
        Web scrapping
        APIs
        Spreadsheets: XLSX
        PDFs
        Word documents: DOCX
        Emails
        GUI Automation
        CSV
        JSON
        Google Sheets

    Python: common APIs
        https://automatetheboringstuff.com/list-of-json-apis.html
        look at the links

    Advanced Topics
        Collections
            counter
            defaultdict
            OrderDict
            namedtuple
            deque
        datetime
        Debugger: pdb
        timeit
        Regular expressions: re
        String I/O
        Advanced numbers
        Advanced strings
        Advanced sets
        Advanced dictionaries
        Advanced lists
        GUIs in Jupyter Notebook
        Multithreading
    
    Standard Library
        os
        math
        random
        datetime

    Conventions (DLR)

    PEP 8: Style Guide

    Virtual environments

    Anaconda
        Installation
        Basic use

    Jupyter

    Jupyter: Magic Commands
        Practical Examples

    Numpy
        Basics
        Selection, Slicing, Broadcasting
        Universal Functions
        Matrices, Algebra

    Matplotlib
        Basics
        Object Oriented Plotting
        Subplots
        Figure Properties
        Advanced Options
        Other plots
            bar
            boxplot
            hist
            loglog
            pie
            polar
            scatter
            spy
        Change properties

    Pandas
        Series
        Data Frames
            Basics
            Sources
            Access, Selection, Modification
            Working: plot, merge, save, cut, ...
            Missing data
            Useful Operations
            Advanced: CSV, XLSX, HTML, SQL
            Advanced: groupby
            Advanced: concat, merge, join
            Advanced: multi-indexing
        Visualization
        Pandas Datareader

    Seaborn
        Built-in datasets
        Distribution plots
        Categorical plots
        Matrix plots: heatmaps & clustermaps
        Grids
        Regression plots
        Style and color

    Plotly & Cufflinks
        
        df.iplot
            Scatterplots
            Bar plots (often with aggregated functions)
            Box plots
            3d surface plots
            Histograms (of one column or several)
            Spread plots
            Bubble plots: scatter plot with point sizes given by another column
            Scatter matrix = sns.pairplot()
        
        choropleth maps

    Finance Data Exploration

        DataReader
        Histograms of returns
        Time Series
        Moving Averages
        Correlation Heatmaps
        Technical Analysis Charts with Plotly
            Candle Sticks
            Simple Moving Averages
            Bollinger Band Plots

    Financial analysis with python
    
        Basic Data Sources and Plotting
        Data Sources
            pandas-datareader
            quandl
        Pandas with Time Series data
            DatetimeIndex
            Time resampling
            Time shifts
            Rolling and expanding
            Bollinger bands
        Market Analysis Project
            See notebook:
                github/mxagar/finance_accounting_courses/python_finance/03_StockMarketAnalysisExample.ipynb
        Time Series Analysis
            Time series basics
            Statsmodels library
            EWMA models
            ETS models and decomposition
            ARIMA models
        Python Finance Fundamentals
        Basics of Algorithmic Trading with Quantopian
        Advanced Quantopian and Trading Algorithms
    
    * HD5

    * Numba

    * Cython

    Utils
        argparse
        pyperclip

    List of important advanced python functionalities
        https://dacus-augustus.medium.com/top-12-most-important-python-concepts-24f59945a409

    List of installed environemnts


----------------
PYTHON: GENERAL USE
----------------

# Setup
    Follow instructions from DLR course
    Known IDEs
        Sublime - Text Editor
        PyCharm (JetBrains) - IDE
        Jupyter Notebook - Notebook, for learning and making notes
    Web site: Many resources!
        https://www.python.org
    Jupiter Notebook
        Open Terminal
            Select environment
                conda env list
                source activate <environment>
                    new: conda activate <env>
            Go to desired folder
            Open jupyter from Terminal
                jupyter notebook
        Manual
            https://jupyter.brynmawr.edu/services/public/dblank/Jupyter%20Notebook%20Users%20Manual.ipynb
        Look at all buttons: pretty straightforward
        Look at Help menu
        Shortcuts
            Shift+Enter: run cell
            Tab: autocompletion
            Shift+TAB: help string
            ESC+M: change to markdown
            ESC+Y: change to python code
            ESC+A: add cell above
            ESC+B: add cell below
            help(structure): help on structure
        Platfrom to share notebooks
            http://nbviewer.jupyter.org
                jmportilla (udemy lecturer)
                Complete-Python-Bootcamp
    General
        #: comments (//)
        ''' ... ''': large comments, multiple lines, all chars escaped
            we could have the text of an entire book!
        Dynamic typing: a = 2; a = ’text’ is possible
        type(a) -> returns type
        None = void
            None is the returned object/type if a function has no return!
        NoneType : the type of None
        Booleans: True/False (capital)
            we have also truthy & Falsey values
            bool(0, 0.0, '') -> False
            bool(42, 'a', ...) -> True
        Comparisons / Logic Operations
            ==, !=, <, >, >=, <=
            and, or, not
        del var
        	delete var
        to break line: \
        You can redefine some tokens, which is bad - if you did that, RESTART KERNEL
        Modules / Libraries:
            import <lib>
                # load library ready to be used
                    <lib>.<module>.<function>(...)
            dir(<lib>)
                # list of all functions, etc.
            help(<lib>)
        If we want to a program to finish in code
            import sys
            ...
            sys.exit()
        Swap operations are easy, without tmp variables
            a, b = b, a
        Increment operators exists
            a += 1
            -=, *=, /=, %=
        eval('string')
            convert string to python object: function, etc.
        VERY IMPORTANT: Mutable vs Inmutable
            mutable objects can change, eg lists
            inmutable objects canno change, eg tuples & strings
            mutable objects are tratead in reality as references!
                mylist = [1, 2, 3]
                newlist = mylist
                newlist[0] = -1
                mylist -> [-1, 2, 3]
            inmutable objects are not treated like references!
            therefore, if we pass a list to a function and it's modified inside of it
                we'll see the modiffication from outside!
                thus, it's as if lists were oassed by reference!
    GIT
        https://git-scm.com
            Try GIT: https://try.github.io/levels/1/challenges/1
        http://gitimmersion.com
    HELP
        object. TAB
        In Jupyter
            object. Shift + TAB
        In Python
            help(object.method)
        Web
            https://docs.python.org/3.6/library/index.html

    CHECK WHICH OPERATING SYSTEM
        import sys
        sys.platform
            'darwin'
            'linux'
            'win32'

# Numbers (2017.11.28)
    Floats and Integers
    Float numbers: x.x: write . !
    Division
        Python 2: truncate division — use floats is desired
            Or
                from __future__ import division
                With this line, Python 2 treats division as Python 3 would
                BUT: if you call it, then print() or other new functions won't work as in 2.7...
        Python 3: True division
    Order of operations: same as C
    Power: 2**3
    Floor or integer division: //
    Modulo (resto): %
    Variable names = labels
        Use PEP8 best practices
    Floating point arithmetic errors: 0.1 + 0.2 - 0.3 = 5.551115123125783e-17
        Unavoidable, take always into account!

# Strings (2017.11.29)
    In single or double quotes -> use double quote if you have a single quote in your string
    print string (Python 2), print(string) (Python 3)
        from __future__ import print_function
            Watch out: once done, then you must use Python 3 style for the related function
    pretty print: interesting for dictionary printing
        import pprint
        pprint.pprint(list/dict/...): nice print, ordered, etc
        pprint.pformat(list/dict/...): output as string
    \n, \t, … they all work; \ is the escape sequence symbol
    len(string) -> length
    s[0] -> indexing starts at 0
    Slicing: s[start:end:step] ... so it's different to Matlab
    	IMPORTANT: end is not included!
        s[:] everything
        s[-1] last element
        s[-2] element before last one...        
        s[::-1] go backwards, reverse
        s[::2] go in jumps of 2
        s[:3] everything until element 3 — watch out: s[3] is 4th element!
        s[3:] everything from element 3 + 1 until the end
        You can do also the following
        	'hello world'[2:4] -> will return 'll'
	Use print(my_string) always, not just my_string
    Immutability: Strings are inmutable!
        The elements of a string cannot be re-assigned
            s = ‘Hello World’
            s[0] = ‘x’ -> it won’t work
        But
            you can concatenate to build new strings: s1 + s2
            concatenate multiplying: s1 * 3
            the complete string s can be re-assigned: s = s + ‘!'
    print() has also optional arguments!
        end, sep
    Methods (. + TAB -> check all methods and attributes!)
        Methods need to have () at the end, otherwise you just ask what they are
        Additionally, since strings are unmutable, th emethods return an ew string!
        s.upper() -> make upper case; but it's not assigned to s, you need to do it actively: s = s.upper()
        s.lower() -> make lower case
        s.title/() -> all words start with upper
        s.split(‘e’) -> split string in letter ‘e’ and return a list with substrings between ‘e’ ('e' is removed)
        x = 'This is a nice sentence'
        x.split() -> ['This', 'is', ]: splitted in white spaces
        s.islower() / s.isupper() -> True if all chars are!
        .isalpha(): letters only
        .isalnum(): letters & numbers only
        .isdecimal(): number only
        .isspace(): whitespaces only
        .istitle(): titel case only: all wrods start with upper, rest lower
    Reverse a text
        lst = text.split()
        lst = lst[::-1]
        new_text = ' '.join(lst)
    Prefixes & suffixes
        s.startswith('01_')
        s.endswith('exe')
    Raw strings: r'...', they add \ to escape characters to take them literally
        s = r'C:\work' --> 'C:\\work'
        print(s) --> 'C:\work'
    Multiline literal text: ''' ... ''' or """ ... """
        we can literally put gigantic texts inside, like complete books!

# Print Formatting (2017.11.29, 2018.11.17)
    OLD: placeholders with %
        Elements of strings can be defined dynamically
            x = 'STRING'
            print('Place another string with a mod and s: %s' %(x))
                You can use it to convert x to be a string
        Accuracy for floating point numbers cane be specified
            print('Floating point numbers: %1.3f' %(13.144))
                First number (1): minimum number of digits before point (if mare than available, blank space)
                Second number (2): maximum number of decimals
        str(var): var is converted to a string
        Several elements: in commas
            print('First: %s, Second: %1.2f, Third: %r' %('hi!',3.14,22))
        Methods after %
            s -> str(): converts content into string
            r -> repr(): delivers string representation, including quotation marks and escape letters
            f -> float
    Best way: use .format() function using {x}
        print('This is my {} {} {}'.format('fucking', 'awesome', 'text')) -> This is my fucking awesom text
        print('This is my {2} {0} {1}'.format('fucking', 'awesome', 'text')) -> This is my text fucking awesom
        print('Object 1: {a}, Object 2: {b}, Object 3: {c}'.format(a=1,b='two',c=12.3))
        Float formating
            result = 100/777
            print('The result was {r:1.3f}'.format(r=result)) -> value:width.precision f
    New method in python 3.6: f-strings
        name = 'Mikel'
        age = 3
        print(f'My name is {name}')
        print(f'{name} is {age} years old')
    MORE
        There ar emuch more options for print formatting
            https://pyformat.info

# LISTS (2018.11.17)
    Ordered sequences that can hold a variety of types; mutable
    Created with [,,,]
    They support: slicing, indexing, concatenation, multiplication, re-assignment, nesting
    Examples
        my_list = ['A string',23,100.232,'o']
        len(my_list)
        my_list + ['new item']
        my_list[0] = 're-assigned element'
    Some methods: in some cases, NOTHING RETURNED! (None) LIST ITSELF RE-ASSIGNED/MODIFIED/ORDERED
        Too see all methods: . TAB
        .append() -> concatenate elemt at the end
        .pop() -> pop and RETURN element (default: from end of list)
            .pop(0) -> first element
            .pop(-1) -> last element, default
        .sort() -> sorts depending on type inside
            all items need to be of the same type
            sorted in place! returns None
            .sort(reverse=True) -> it's possible!
            .sort(key=str.lower) -> capital and lower treated equally, else ASCII-betical?
            we can also pass a function to it for sorting, e.g., measure number of chars
            alternative: sort(); can be used for collections, returns a sorted collection
        .reverse() -> reverses elements
        .insert(1, 'chicken') -> item 'chicken' inserted in index position 1
        .remove('chicken') -> looked for item and removed (only first found!)
            equivalent to 'del my_list[1]', but we don't need to know the index!

    Nesting
        l1 = [1,2,3]
        l2 = [4,5,6]
        l3 = [7,8,9]
        matrix = [l1,l2,l3]
        matrix[0] -> [1,2,3]
        matrix[0][0] -> 1
    List comprehensions: following previous example
        first_col = [row[0] for row in matrix] -> [1,4,7]

    .join()
    >>> "--".join(['a','b','c'])
    >>> 'a--b--c'

    list()
        range()-like generators can be converted to lists

    myList.index('b')
        -> 1

    lists are mutable!
    lists are references!
        lst = [1,2,3]
        new_lst = lst
            -> if we change new_lst, also inside a function, lst will reflect the change!
        to avoid passing references, use copy package
            import copy
            lst = [1,2,3]
            new_lst = copy.deepcopy(lst)


# DICTIONARIES
    Unordered mappings of objects: for quickly grabbing objects without needing to know index
    dicts are mutable
    Created with {'key1': 'vaue1',,,}, or with dict(key1=value1,...)
    Cannot be sorted
        BUT we can sort keys or values with function sorted()
        sorted_alphabetical = sorted(students, key=students.get, reverse=False)
    Use case: price lookup table
    Values can have different type - a value could be a list or a dictionary too
    Access, re-assign, and add: []
    Examples
        my_dict = {'k1': 100, 'k2': [1,2,3], 'k3': 'hello', 'k4': {'a': 1.5, 'b': 'test'}}
        my_dict['k4']['b'][1].upper() -> E
        my_dict['k1'] = 200
        len(my_dict)
        my_dict = {}
        my_dict['a'] = 1 # automatically added
    Methods
        .keys() all keys shown
            list(my_dict.keys())
        .values() all values shown
            list(my_dict.values())
        .items() all key-value pairs showns -> pairs are tuples!
            list(my_dict.items())
        .get(key, fallback) -> return value of k1 if key exists, 0 otherwise
            my_dict.get('k1',0)
            equivalent to
                if 'k1' in my_dict:
                    return my_dict['k1']
                else:
                    return 0
        .setdefault(key, value) -> if key is not in dict, set key with value
            my_dict.setdefault('color','black')
            equivalent to
                if 'color' not in my_dict:
                    my_dict['color'] = 'black'
            count characters
                message = "Today is the first day of the rest of my wonderful life"
                count = {}
                for c in message.upper():
                    count.setdefault(c,0)
                    count[c] = count[c] + 1

    Dictionaries packed in lists are like data structures that contain many entries!

# TUPLES = inmutable lists using () instead of []
    Similar to lists, but INMUTABLE = cannot be changed, elemnst cannot be re-assigned
    Supported: different types, slicing, indexing
    Created with (,,,)
    Exmaples
        t = ('a', 'a', 'b', 1, 2.3)
        len(t)
    Methods: less than for lists
        .count('a') count how many times 'a' appears
        .index('a') index of the very first time 'a' appears in tuple
    When to use a tuple and when a list?
        A tuple is an inmutable list
        So it is useful when data integrity is required

# SETS
    Unordered collections of unique objects, e.g. you can't have more than one 'a' in them
    Creted with myset = set()
        myset returns {...}, but it's not a dictionary
    Example
        myset  = set()
        set.add(1)
        myset
            {1}
        myset.add(2)
        myset
            {1,2}
        myset.add(2)
        myset
            {1,2}
    Usecase: cast lists with repeating objects to be sets
        l = [1,1,1,2,2,2,3]
        set(l)
            {1,2,3}
    Usecase: detect unique letters
        set('Mississippi')
            {'M', 'i', 'p', 's'}

# FILE I/O
    Only in Jupyter Notebook: How to write and save a file and how to append lines to it
        %%writefile myfile.txt
        This is the first line
        This is the second line
        This is the third line
        ...

        %%writefile -a test.txt
        This is text being appended to test.txt
        And another line here.

    myfile = open('myfile.txt')
        File must be in same location as pwd
        Otherwise:
            myfile = open('/Users/mikel/myfile.txt')
    myfile.read()
        String returned: 'This is the first line\nThis is the second line...'
        now, cursor is at the end, you need to put it back at the begining again
        myfile.seek(0)
    myfile.readlines()
        List if strings, each string is a line
        More compfortable to iterate
    myfile.close()
        IMPORTANT to close file always
        ALTERNATIVE: with this style you don't need to close file at the end
            with open('myfile.txt') as my_new_file:
                contents = my_new_file.read()
            ... now you can continue outside with contents
    Open modes
        with open('myfile.txt', mode='r') as my_new_file:
            contents = my_new_file.read()
        mode can be
            'r' read only
            'w' write only: file overwritten, new created if not existing
            'a' append: lines added
            'r+' read and write
            'w+' write and read, overwriting
        Examples
            with open('myfile.txt',mode='r') as f:
                print(f.read())
            with open('myfile.txt',mode='a') as f:
                f.write('\nThis is the fourth line')
            with open('my_new_file.txt',mode='w') as f:
                f.write('This is a new file')
    File iteration
        for line in open('test.txt'):
            print(line)

# CONTROL FLOW
    Main properties:
        indentation
        no ';' or line endings
        no curly braces - statements with ':'
        no 'end' at end of statement/namespace

    #### IF

    arg = 'kaixo'
    if arg == 'hello':
        print('bye')
    elif arg == 'servus':
        print('tschuess')
    else:
        print('agur')

    #### FOR

    # Lists
    mylist = [1,2,3]
    for item in mylist:
        print(item)

    # Packed tuples
    mylist = [(1,2),(3,4),(5,6),(7,8)]
    for (a,b) in mylist:
        print(a)
        print(b)

    # Dictionaries: but be careful: dictionaries are not ordered!
    d = {'k1':1, 'k2':2, 'k3':3}
    for key,value in d.items():
        print(value)

    #### WHILE

    x = 0
    while x < 5:
        print('x is < 5')
        x += 1
    else:
        # executed once when while condition is not fulfilled
        print('x is >= 5')

    Especial keywords
        break: stop current (closest) enclosing loop
        continue: go to the top of the current (closest) enclosing loop
        pass: do nothing at all - to avoid syntax errors

    x = [1,2,3]
    for item in x
        # I will program this later
        pass

    x = 0
    while x < 5:
        x += 1
        if x > 3:
            continue
        print('still long way to go')

# USEFUL OPERATORS

    ## range: range(<start>,stop,<step>)
    
    generator: number not really created and stored in memory; finishes right before stop
    generators must be casted to realize their value
    range(5) -> 1, 2, 3, 4
    list(range(5)) -> [1, 2, 3, 4] you need to cast it to a list is you want the real values, because it's a generator

    for x in range(0,11,2):
        # 0, 2, 4, 6, 8, 10
        print(x)

    ## enumerate: it creates tuples that contain indices to avoid counting in loops

    word = 'abcd'
    i = 0
    for letter in word:
        print(word[i])
        i += 1
        # a b c d

    for letter enumerate(word):
        print(letter)
        # (1, 'a') (2, 'b') ...

    for index,letter enumerate(word):
        print(letter)
        # a b c d
        # UNPACKING: when elements of a tupled are disected into subelements

    ## zip: zip(list1,list2,...)

    generator: not values stored really, but parameters to generate them

    lst1 = [1, 2, 3]
    lst2 = ['a', 'b', 'c', 'd']
    list(zip(lst1,lst2)) -> [(1, 'a'), (2, 'b'), (3, 'c')]

    ## in

    check if an object is in a list
    'x' in ['x', 'y', 'z'] -> True
    works with distcionaries, character chains, etc.

    ## min(list), max(list)

    ## random: library with many functions - you need to import it

    from random import shuffle
    mylist = [1, 2, 3, 4, 5]
    shuffle(mylist)
        not returning anything, but shuffling mylist itself

    from random import randint
    myrandint = randint(0,100)

    try with TAB and see what's in random: from random import TAB

    ## input: console waits for input - everything is accepted as string; you need to cast it later

    input('Enter number: ')
    value = input('Enter number: ')
    value_int = int(value)
        usually, answer = input().lower()

# LIST COMPREHENSIONS

    Way to spare for loops, or flatten them when they are used for creating lists
    Make a list []: [x_formula for x in collection if condition]
    You can nest list comprehensions also: collection would be a list comprehension

    lst = []
    for x in 'word':
        lst.append(x)
    -> ['w','o','r','d']

    lst = [x for x in 'word']
    -> ['w','o','r','d']

    lst = [x**2 for x in range(0,11)]
    -> [0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]

    lst = [x for x in range(11) if x % 2 == 0]
    -> [0, 2, 4, 6, 8, 10]

    lst = [ x**2 for x in [x**2 for x in range(11)]]
    -> [0, 1, 16, 81, 256, 625, 1296, 2401, 4096, 6561, 10000]

    You can have if-else, but order is changed:
    lst = [x if x%2 == 0 else 'ODD' for x in range(0,11)]
    -> [0, 'ODD', 2, 'ODD', 4, ...]

    # Costs below 25
    lst = [cost for cost in costs if cost < 25]

# FUNCTIONS
    
    def name_of_function(arg1,arg2):
        '''
        Explain function doc-string
        '''
        print('Hello ' + arg1)
        return arg1 + arg2, arg1

    a, b = name_of_function(1,5)
    -> Hello 1; a = 6, b = 1
    help(name_of_function)
    -> Explain function doc-string

    # Default values for arguments
        def name_of_function(name='NAME'):
    # Argument order can be changed
        name_of_function(arg2=1,arg1=5)
    # Arbitrary length argument: create a tuple with *
        def myfunc(*args):
            return sum(args)
        -> I can use: myfunc(1,2,3,...) -> arg is a TUPLE
    # Keyword arguments: create a dictionary with **
        def myfunc(**kwargs)
            if 'fruit' in kwargs:
                print('My fruit of choice is {}'.format(kwargs['fruit']))
            else:
                print('No fruit found')
        -> myfunc(fruit = 'apple', veggie = 'lettuce'); kwargs is a dictionary inside: {'fruit':'apple','veggie':'lettuce'}
    
        def myfunc(*args,*kwargs):
            # Any number of values and keyword arguments
        -> myfunc(10,20,30,a=1,b=2,c=3)

    # Unpacking iterables as arguments
    # *args means "treat the elements of this iterable as positional arguments to this function call."
    # **kwargs means "treat the key-value pairs in the dictionary as additional named arguments to this function call."

        def foo(x, y):
            print(x, y)

        t = (1, 2)
        foo(*t) # 1 2

        [1, *(2, 3), 4] # [1, 2, 3, 4]

        d = {'x':1, 'y':2}
        foo(**d) # 1 2

# MAP, FILTER, ALL

    Given a collection, apply a function to that collection (map) of filter its elements
    They avoid using for loops

    ## map(function, iterable)

    map(fun, iterable): fun is applied to all elements of iterable
    and the resulting iterable is returned

    def square(num):
        return num**2

    mynums = [1,2,3,4,5]
    map(<function>,<iterable>)
    list(map(square,mynums))
        -> without list() map is not executed

    ## filter(function->Bool, iterable)

    def check_even(num):
        return num % 2 == 0 

    mynums = [1,2,3,4,5]
    filter(<function T/F return>,<iterable>)
    list(filter(square,mynums))
        -> without list() filter is not executed

    ## all(iterable)

    all(iterable) returns True if all items in an iterable are true,
    otherwise it returns False.
    If the iterable object is empty, the all() function also returns True.

        list(map(lambda x: x**2, [1, 2, 3])) # [1, 4, 9]
        list(filter(lambda x: x%2 == 0, [1, 2, 3])) # [2]
        all(map(lambda x: x%2 == 0, [1, 2, 3])) # False

# LAMBDA EXPRESSIONS: one-time use functions, without a name

    Often used with map() and filter()
    A function is defined in a line
    Use only simple functions, otherwise difficult to understand

    lambda <argS>: <operation on argS>

    Examples
        def square(num):
            return num**2

        lambda num: num**2
        
        list(map(lambda num: num**2, mynums))
        
        list(filter(lambda num: num%2 == 0, mynums))

        lambda x,y : x + y

# NESTED STATEMENTS & SCOPE

    LEGB Rule: Order with which Python looks for variables: if not found in L, it jumps to E, then G, B
        Local: inside scope of function, etc.
        Enclosing function locals: inside of enclosing functions/scopes
        Global (module): declared as global in file
        Built-in (Python): defined in Python language

    Implications:
        Reassignments of variables inside of a function/scope happen only inside - it's like pass by value
        If we want to have a global effect (like pass by reference) use token global inside of scope (but, avoid that):

            x = 50
            def func():
                global x
                x += 1
                print(x)
            func()
            -> 51
            print(x)
            -> 51

# OBJECT ORIENTED PROGRAMMING: CLASSES

    Objects with own methods
    Classes with CamelCase; attributes, methods, variables in small and with _
    Attributes and methods
    self is used to connect methods and attributes to the current object

    ## General Example

        class NameOfClass():

            # CLASS OBJECT ATTRIBUTES
            # SAME VALUE FOR ALL INSTANCES
            # Instead of preceded by self, it's better to use them with NameOfClass.general_attritbute
            general_attribute = 'general class'

            # constructor, called when class instantiated as object
            # all class-related attributes defined here: self.attribute_name
            # they must be used in class methods preceded by self
            # can have default values or not
            def __init__(self,param1=10,param2='Michael'):
                self.param1 = param1
                self.param2 = param2
                self.param3 = param1 * param2

            # Methods: OPERATIONS/Actions
            def some_method(self,arg1):
                # perform some action
                # param1 is an attribute, arg1 an argument not part of the class
                print(self.param1,arg1)

        # instantiation
        my_object = NameOfTheClass(param1=100,param2='Sammy')
        my_object = NameOfTheClass(100,'Sammy')

        # access to attributes and methods
        NameOfTheClass.general_attribute
        -> 'general class'
        my_object.param1
        -> 100
        my_object.some_method(2)
        -> 100 2

    ## Example: Circle

        class Circle():
            pi = 3.14

            # Circle gets instantiated with a radius (default is 1)
            def __init__(self, radius=1):
                self.radius = radius 
                self.area = radius * radius * Circle.pi

            # Method for resetting Radius
            def setRadius(self, new_radius):
                self.radius = new_radius
                self.area = new_radius * new_radius * Circle.pi

            # Method for getting Circumference
            def getCircumference(self):
                return self.radius * Circle.pi * 2


        c = Circle()

        print('Radius is: ',c.radius)
        print('Area is: ',c.area)
        print('Circumference is: ',c.getCircumference())

# OBJECT ORIENTED PROGRAMMING: INHERITANCE

    Classes are derived from others and the methods and attributes of those are available
    You can overwrite them: just use same function definition and change code inside

    # Base class
    class Animal():
        def __init__(self):
            print('Animal created')
        def eat(self):
            print('Animal eats')
        def sleep(self):
            print('Animal sleeps')

    # Inherited class: Use base class in new class definition
    class Dog(Animal):
        def __init__(self):
            Animal.__init__(self) # This is optional, if we want the constructor of Animal to be called!
            print('Dog created')
        # Method overwritten
        def eat(self):
            print('Dog eats')
        # sleep() is not overwritten: it's still available in Dog through Animal!

# OBJECT ORIENTED PROGRAMMING: POLYMORPHISM

    Different object types (classes) share methods with same name
    A function can call this method no matter which object type is being treated

    Example
        class Dog():
            def __init__(self,name):
                self.name = name
            def speak():
                return self.name + 'says woof!'

        class Cat():
            def __init__(self,name):
                self.name = name
            def speak():
                return self.name + 'says meow!'

        niko = Dog('Niko')
        felix = Cat('Felix')

        print(niko.speak())
        -> Niko says woof!
        print(felix.speak())
        -> Felix says woof!

        ## Polymorphism 1: create a list of different object types and call thier common method

        for pet in [niko,felix]:
            print(pet.speak())

        ## Polymorphism 2: create a function for different object types and call their common method

        def pet_speak(pet):
            print(pet.speak())

        pet_speak(niko)
        pet_speak(felix)

    Abstract classes: the ones that are not meant to be instantiated, but inherited and used with polymorphism
    Example

        class Animal():
            def __init__(self,name):
                self.name = name
            def speak(self):
                # Make sure Animal that if is instantiated, an error message appears is speak() is called
                raise NotImplementedError('Subclass must implement this astract method')

        class Dog(Animal):
            # __init__ not needed, since inherited from abstract class Animal
            def speak(self):
                return name + ' says woof!'

        class Cat(Animal):
            # __init__ not needed, since inherited from abstract class Animal
            def speak(self):
                return name + ' says meow!'

        niko = Dog('Niko')
        felix = Cat('Felix')

# OBJECT ORIENTED PROGRAMMING: OVERLOADING PYTHON FUNCTIONS/METHODS - AKA ESPECIAL/MAGIC METHODS

    For example: len(.), print(.) for my classes
    Aka special methods or magic methods
    They all start with double underscore: __
    
    Example

    class Book():
        def __init__(self, title, author, pages):
            self.title = title
            self.author = author
            self.pages = pages
        def __str__(self):
            return f'{self.title} by {self.author}'
        def __len__(self):
            return self.pages
        def __del__(self):
            print('A book object has been deleted')

    mybook = Book('Harry Potter', 'Rowan Atkinson', 556)
    str(mybook) # calls especial method __str__()
    -> Harry Potter by Rowan Atkinson
    len(mybook)
    -> 556
    del b
    -> deletes object from memory and calls __del__

    There are a known limited set
        __str__(self)
        __del__(self)
        __len__(self)
        __repr__(self): representation when instantiated in Terminal
        __call__(self, *args): make class callable with ()
        __and__(self): & operator for class
        __eq__(self): ==, check if instance is equal to another
        __contains__(self): in operator
        __iter__(self): iter() 
        __next__(self): next()
        __getitem__(self, key): [key]
        __add__(self, y): x + y
        ...
        Note: the arguments of the special/magic functions
        need to be checked...

# OBJECT ORIENTED PROGRAMMING: Multiple Inheritance

	# Base class

		class Car:
		    def __init__(self, wheels = 4):
	    	    self.wheels = wheels
	        	# We'll say that all cars, no matter their engine, have four wheels by default.

	# Simple Inheritance

		class Gasoline(Car):
		    def __init__(self, engine = 'Gasoline', tank_cap = 20):
		        Car.__init__(self)
		        self.engine = engine
		        self.tank_cap = tank_cap # represents fuel tank capacity in gallons
		        self.tank = 0
		        
		    def refuel(self):
		        self.tank = self.tank_cap
		        
		class Electric(Car):
		    def __init__(self ,engine = 'Electric', kWh_cap = 60):
		        Car.__init__(self)
		        self.engine = engine
		        self.kWh_cap = kWh_cap # represents battery capacity in kilowatt-hours
		        self.kWh = 0
		    
		    def recharge(self):
		        self.kWh = self.kWh_cap
	
	# Multiple Inheritance

		class Hybrid(Gasoline, Electric):
		    def __init__(self, engine = 'Hybrid', tank_cap = 11, kWh_cap = 5):
		        Gasoline.__init__(self, engine, tank_cap)
		        Electric.__init__(self, engine, kWh_cap)
		        
		prius = Hybrid()
		print(prius.tank) # 0
		print(prius.kWh) # 0
		prius.recharge()
		print(prius.kWh) # 5

	# Method Resolution Order (MRO)

		class A:
			pass
		class B(A):
			pass
		class C(A):
			pass
		class D(B, C):
			pass

		The order for finding already defined functions/variables is [D, B, C, A, object]
		The first in the sequence is the one used for D

	# super()

		class MyBaseClass:
		    def __init__(self,x,y):
		        self.x = x
		        self.y = y
	    
		class MyDerivedClass(MyBaseClass):
		    def __init__(self,x,y,z):
		        super().__init__(x,y)
		        self.z = z

		super() goes to the next class in the Method Resolution Order (see above)
		in simple inheritance like this
			super().__init__(x,y) == MyBaseClass.__init__()

# OBJECT ORIENTED PROGRAMMING: ADVANCED FUNCTIONALITIES

    #### Class Methods, Instance Methods, Static Methods

        Source: [Python's Instance, Class, and Static Methods Demystified](https://realpython.com/instance-class-and-static-methods-demystified/).

        We distinguish:

        - the class
        - and the object or class instance.

        While a class itself is a a structure with a memory address, the class instances are usually the ones used for computations. Inside the class instances we can reference the objects itself or the class from which it was instatiated. Thus, we have several types of methods and attributes: instance/class methods and attributes.

        ```python
        class MyClass:
            # Instance method: regular method which takes self.
            # Can access/modify the instance via self an the class vis self.__class__.
            # We need an object to use them: obj.method(), MyClass.method(obj).
            def method(self):
                return 'instance method called', self

            # Class method: can access/modify the class only.
            # Can be used from the object/instance or the class.
            # Employed for factory functions or defining several constructors (__init__).
            @classmethod
            def classmethod(cls):
                return 'class method called', cls

            # Static method: it cannot access/modify either the instance or the class
            # but it can be used from both.
            # They signal that the function is independent from the class/object,
            # i.e., some kind of utility procedure; that improves mantainability.
            @staticmethod
            def staticmethod():
                return 'static method called'

        obj = MyClass()
        obj.method() # ('instance method called', <MyClass instance at 0x10205d190>)
        MyClass.method(obj) # ('instance method called', <MyClass instance at 0x10205d190>)
        obj.classmethod() # ('class method called', <class MyClass at 0x101a2f4c8>)
        obj.staticmethod() # 'static method called'

        MyClass.classmethod() # ('class method called', <class MyClass at 0x101a2f4c8>)
        MyClass.staticmethod() # 'static method called'
        MyClass.method() # TypeError: if not called in instance or passed an instance, error!
        ```

        Note: the names `self` and `cls` are a convention; we can use any name - BUT: the important thing is that they are the first argument of the method.

        **Instance methods** are usual class methods which receive the object or class instance as first argument: `self`. Through `self`, they can access and extend any *object* attributes and methods. They can also access the class itself via `self.__class__`. Thus, class instances can also modify the class state! So they are really powerful.

        **Class methods** have the decorator `@classmethod` and accept the `cls` argument, which refers to the class structure in memory. Thus, they can modify the class state, but not instance states! However, they can be used from the instance or the class. They are applied in two situations, which are actually the same:

        - When we want to use *factory functions*, which instantiate classes with predefined arguments; that avoids inheriting too many children classes and decreases the code complexity.
        - When we want to define several constructors; python accepts only one `__init__`, but with *factory functions*, it's like we had several constructors!

        ```python
        class Pizza:
            def __init__(self, ingredients):
                self.ingredients = ingredients

            def __repr__(self):
                return f'Pizza({self.ingredients!r})'

            @classmethod
            def margherita(cls):
                return cls(['mozzarella', 'tomatoes'])

            @classmethod
            def prosciutto(cls):
                return cls(['mozzarella', 'tomatoes', 'ham'])

        Pizza.margherita() # Pizza(['mozzarella', 'tomatoes'])
        Pizza.prosciutto() # Pizza(['mozzarella', 'tomatoes', 'ham'])
        ```

        **Static methods** have the decorator `@staticmethod` and they receive no `self` or `cls` argument, altough they can accept arbiitraty arguments. They cannot change either the class or the instance. They can be called from the class or the instance, but they cannot access/modify any of their attributes!

        Static methods are used to signal the programmers that the method does not need anything from the class/object; thus, they are like utility procedures independent from them. Factoring the code that way makes it easier to maintain.

        ```python
        import math

        class Pizza:
            def __init__(self, radius, ingredients):
                self.radius = radius
                self.ingredients = ingredients

            def __repr__(self):
                return (f'Pizza({self.radius!r}, '
                        f'{self.ingredients!r})')

            def area(self):
                return self.circle_area(self.radius)

            @staticmethod
            def circle_area(r):
                return r ** 2 * math.pi

        p = Pizza(4, ['mozzarella', 'tomatoes'])
        p.area() # 50.26548245743669
        Pizza.circle_area(4) # 50.26548245743669
        ```

    #### Class Attributes, Instance Attributes

        Source: [Class vs. Instance Attributes](https://python-course.eu/oop/class-instance-attributes.php).

        **Instance attributes** are the usual ones accessed via `self`, i.e., they belong to the object or class instance.

        **Class attributes** belong to the class: all instances share them! They can be accessed via object or class, but modified *only* via the class. Common application: instance counters.

        ```python
        class Pizza:
            # Class attribute
            counter = 0
            def __init__(self, radius, ingredients):
                self.radius = radius
                self.ingredients = ingredients
                Pizza.counter += 1 # we could also do: type(self).counter += 1

        p1 = Pizza(4, ['mozzarella', 'tomatoes'])
        p1.counter # 1
        Pizza.counter # 1
        p2 = Pizza(4, ['mozzarella', 'ham'])
        p2.counter # 2
        p1.counter # 2
        Pizza.counter = 100 # WORKS
        p1.counter = 200 # DOES NOT WORK: no change in Pizza.counter, but a new instance attribute is created 
        ```

    #### Multiple Inheritance and Mixins

        Source: [Mixins for Fun and Profit](https://easyaspython.com/mixins-for-fun-and-profit-cb9962760556).

        ```python
        # Base class
        class Car:
            def __init__(self, wheels = 4):
                self.wheels = wheels
                # All cars have wheels

        # Simple Inheritance
        class Gasoline(Car):
            def __init__(self, engine = 'Gasoline', tank_cap = 20):
                Car.__init__(self)
                self.engine = engine
                self.tank_cap = tank_cap # represents fuel tank capacity in gallons
                self.tank = 0
                
            def refuel(self):
                self.tank = self.tank_cap
                
        class Electric(Car):
            def __init__(self ,engine = 'Electric', kWh_cap = 60):
                Car.__init__(self)
                self.engine = engine
                self.kWh_cap = kWh_cap # represents battery capacity in kilowatt-hours
                self.kWh = 0
            
            def recharge(self):
                self.kWh = self.kWh_cap

        # Multiple Inheritance
        class Hybrid(Gasoline, Electric):
            def __init__(self, engine = 'Hybrid', tank_cap = 11, kWh_cap = 5):
                Gasoline.__init__(self, engine, tank_cap)
                Electric.__init__(self, engine, kWh_cap)
                
        prius = Hybrid()
        print(prius.tank) # 0
        print(prius.kWh) # 0
        prius.recharge()
        print(prius.kWh) # 5

        ```

        When we inherit from several parent classes, we might have method overloading order doubts. The **Method Resolution Order (MRO)** is defined as follows:

        ```python
        class A:
            pass
        class B(A):
            pass
        class C(A):
            pass
        class D(B, C):
            pass
        ```

        The order for finding already defined functions/variables is `D, B, C, A, object`.
        The first in the sequence is the one used for `D`.
        This important for `super()`: `super()` always refers to the next class in the MRO list; when called in `D`, it would mean `B`. In a simple inheritance, it means the base class.

        **Mixins** are examples of inheritance in which the parent class doesn't feel like a parent or a similar/related class; tthey just extend the functionality. One practical example could be logging: we can add module and class-speciifc loggers using a logger Mixin:

        ```python
        import logging

        class LoggerMixin():
            @property # property decorator; see section on decorators
            def logger(self):
                name = '.'.join([
                    self.__module__,
                    self.__class__.__name__
                ])
                return logging.getLogger(name)

        class EssentialFunctioner(LoggerMixin):
            def do_the_thing(self):
                try:
                    ...
                except BadThing:
                    # LoggerMixin.logger is called, which returns the object specific logger!
                    self.logger.error('OH NOES')

        class BusinessLogicer(LoggerMixin):
            def __init__(self):
                super().__init__()
                # LoggerMixin.logger is called, which returns the object specific logger!
                self.logger.debug('Giving the logic the business...')
        ```

    #### Abstract Base Classes: abc

        https://docs.python.org/3/library/abc.html

        A class is called an Abstract class if it contains
        one or more abstract methods.
        An abstract method is a method that is declared,
        but contains no implementation.
        Abstract classes may not be instantiated,
        and its abstract methods must be implemented by its subclasses.

        ```python
        from abc import abstractmethod

        @abstractmethod
        ```


# MODULES & PACKAGES: PyPI

    PyPi: Repository for open source packages
        We can install these 3rd party packages/modules: there is almost everything!

    (if pip is already installed)
    open Terminal
    pip install <package_name>
        WATCH OUT
            pip -> pyton 2.7
            pip3 -> python 3

    pip freeze > requirements.txt
        export list of packages in an environment to a file
    pip install -r requirements.txt
        install from file

    Google search for packages
        python package for pdf, excel, ...

    IMPORTANT UPDATE: MORE ABOUT PIP

        # Run pip through python
        # The commands "python -m <module>" imports the module 
        # and runs it as if it were the main program, i.e., 
        # what in its "__main__" is specified.
        # In general it is recommended to use "python -m pip ..."
        # instead of "pip", because that way we know/control
        # the python version
        python -m pip list

        # Return a list of installed Python modules
        pip list
        
        # Show only outdated modules; it takes a bit...
        pip list --outdated
        
        # Show list of installed modules, but in a requirements format
        pip freeze
        
        # However, some package versions are
        # shown with @ file://... tags
        # https://stackoverflow.com/questions/62885911/pip-freeze-creates-some-weird-path-instead-of-the-package-version
        # To avoid that, perform the following:
        pip list --format=freeze > requirements.txt
        
        # Show info on a module, e.g. pandas: 
        # author, version, license, requirements, etc.
        pip show pandas
        

# MODULES & PACKAGES: CREATE MODULES AND PACKAGES

    Modules are .py scripts used in other scripts
    Packages are collections of modules: just folder containing
        Modules: .py scripts
        A file __init__.py which can be empty

    Example
        folder/
            MyPackage/ # Package
                __init__.py
                package_module.py # Module in package
                    # Code
                    def print_package_module():
                        print('package_module')
                MySubPackage/ # Package inside a package
                    __init__.py
                    sub_package_module.py # Module in subpackage
                        # Code
                        def print_sub_package_module():
                            print('sub_package_module')
            plain_module.py # Module in no package
                # Code
                def print_plain_module():
                    print('plain_module')            
            
            main_program.py
                # Code
                from plain_module import print_plain_module # function imported
                print_plain_module()

                from MyPackage import package_module # whole module imported
                package_module.print_package_module()

                from MySubPackage.sub_package_module import sub_package_module # whole module imported
                sub_package_module.print_sub_package_module()

    However, it is also possible to create a *.py file and import it as if it were a module

        folder/
            my_module.py
                # Define a function
                def world():
                    print("Hello, World!")

            my_app.py
                import my_module
                # now, we have access to the functions in my_module.py
                my_module.world()

        If my_module.py is in another folder than my_app.py, we add the path with sys.path

            my_app.py
                import sys
                sys.path.append('/usr/sammy/')
                import my_module


# MODULES AND PACKAGES: __main__ & __name__

    In python there is no main fuction - when a file is run, alls its code is run
        Function definitions do no actually execute the function, they define them
    To simulate the main() funcyion, python has built-in variable __name__
        When a file is is called directly, __name__ == "__main__"
    
    A file.py file can have all functions and objects
    When we run it it directly (python file.py) and we want some concrete code to be executed only the, we add at the end:

        # file.py
        def func1():
            pass
        def func2():
            pass

        if __name__ == "__main__":
            # Your main() code
            func1()
            func2()
            ...

    If file.py is imported as a module __name__ != "__main__"!

# RUNNING SCRIPTS

    Save my_python_script.py
    with platform-specific Shebang:

        Windows
            #! python3
        Mac
            #! /usr/bin/env python3
        Linux
            #! /usr/bin/python3

    Content could include:

        def func1():
            pass
        def func2():
            pass
        def main():
            func1()
            func2()
            ...

        if __name__ == "__main__":
            # Your main() code
            # If my_python_script.py is not imported as a module
            # this part of the code will be executed
            # Otherwise, func1 & func2 will be available to use
            # where we imported the script
            func1()
            func2()
            ...

    Execution
        Windows
            python3.exe my_python_script.py
        Linux & Mac
            python3 my_python_script.py
            or ...
            chmod a+x my_python_script.py
            ./my_python_script.py

    Command line arguments
        import sys
        print(sys.argv) # list of all args
        sys.argv[0] # programm call name, 1: first arg, etc


# ERROR & EXCEPTION HANDLING: try & except, raise, assert

    Usually, script/Code flow ends when error occurs
    With error handling code continues

    try:
        # Susceptible code
        result = 10 + '10'
    except:
        # Executed if in try error occurs
        print('Something went wrong')
    else: 
        # Executed if no error occurs
        result
    finally:
        # Executed always, with or without error
        print('Im always executed')

    Several exept blocks can be used, each catching an error type

    try:
        result = 10 + '10'
    except TypeError:
        print('Wrong types')
    except OSError: 
        print('OS Error')
    except:
        # All other errors than TypeError & OSError catched

    Example
        def as_for_int():
            while True:
                try:
                    result = int(input('Please give me a number:'))
                except:
                    print('That is not a number')
                else:
                    print('Thank you!')
                    break

    -> Look in documentation for standard errors

    Also important: raise

        def function(condition):
            if (condition):
                raise Exception("We met condition, which means you need to fix blah")
            else:
                print('Hello!')

        a call stack is returned, which can be stored in logs!

            import traceback
            try:
                raise Exception('This is a test error')
            except:
                # we can open a file in APPEND mode and log what happened
                errorFile = open('./error_logs.txt', 'a')
                errorFile.write(traceback.format_exc())
                errorFile.close()

    assert: sanity checks in code that detect programmer errors (not user errors)

        insert a condition in code that must be True, if not, assertion error launched

        condition = False
        assert condition, 'False value for check!'
        # condition is usually a check that must be true, like det(R) == 1.0

# LOGGING

    logging module can be used
    it's like printing status, but better, because we can turn off
    logs are like breadcrum trails
    
    import logging
    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
    # If we want to save logs into file instead of printing them, pass filename at the front
    #logging.basicConfig(filename='./mylogs.txt', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

    # We can disable printing/logging below a given level
    #logging.disable(logging.CRITICAL) # 5 levels: DEBUG < INFO < WARNING < ERROR < CRITICAL
    logging.debug('Start of function')
    # 5 levels of logs also
    logging.debug('Level debug')
    logging.info('Level info')
    logging.warning('Level warning')
    logging.error('Level error')
    logging.critical('Level critical')
    def factorial(n):
        logging.debug('Start of factorial {}'.format(n))
        result = 1
        for i in range(1,n+1):
            result = result * i
            logging.debug('i is {}, result is {}'.format(i, result))
        logging.debug('result is {}'.format(result))
        return result


# LINTING: pylint and autopep8
    
    pylint: possible errors and styling issues detected

    pip install pylint
    pip install autopep8

    Optionall: Create a ~/.pylintrc file for configuration: more output, colored, etc.

    pylint file.py
        # report is generated with score: x/10, x can be negative
        # make the most important changes
        # leave spaces, line lengths & co. for autopep8

    autopep8 --in-place --aggressive --aggressive file.py
        # Options
        # --in-place: file changed
        # --aggressive twice, to make all changes necessary
        # File will be changed/corrected

    pylint file.py
        # Now we should get 10/10!

    PEP8 guideline used: learn it!
        Use docstrings
        Don't use single character variables
        Use 4 spaces instead of tabs
        ...


# UNIT TESTING: unittest

    Unit testing: built-in tests that check if new versions of code have same critical response
    You build a test class

    # cap.py: File with functionality code

        def cap_text(text):
            #return text.capitalize()
            return text.title()

    # test_cap.py: File for testing functionality

        import unittest
        import cap # import module with functionality

        class TestCap(unittest.TestCase): # We define out class with our tests

            def test_one_word(self):
                text = 'python'
                result = cap.cap_text(text)
                self.assertEqual(result, 'Python')

            def test_multiple_words(self):
                text = 'monty python'
                result = cap.cap_text(text)
                self.assertEqual(result, 'Monty Python')

        if __name__ == "__main__":
            unittest.main() # our class with its tests is instantiated and executed

    # Testing: you call the testing file

        python test_cap

    -> Look in documentation for more test methods and more...

        assertEqual(a, b)       a == b   
        assertNotEqual(a, b)    a != b   
        assertTrue(x)           bool(x) is True  
        assertFalse(x)          bool(x) is False     
        assertIs(a, b)          a is b
        assertIsNot(a, b)       a is not 
        assertIsNone(x)         x is None
        ...

# DECORATORS

	Switch on/off extra functionalities to functions
	IMPORTANT: functions are like objects that can be returned from another function or passed as arguments
	Example: 

		def hello(name = 'Joe'):
		    print('Hello function executed')
		    
		    def greet():
		        print('\t This is greet() function inside hello()')
		        
		    def welcome():
		        print('\t This is welcome() function inside hello()')        
		    
		    print(greet())
		    print(welcome())
		    
		    # greet() and welcome() inside hello() scope
		    # but functions are like objects
		    # and we can return a function or receive it as an argument
		    
		    if name == 'Joe':
		        return greet
		    else:
		        return welcome

	Decorators make use of that object-like nature of functions

		# Decorator wrapping function defined
		def my_decorator(original_func):
		    def wrap_func():
		        print('Text BEFORE code')
		        original_func()
		        print('Text AFTER code')
		    return wrap_func

		# My function to be decorated, decorated with @...
	    @my_decorator
		def my_func():
	    	print('MAIN code to be decorated')

	    my_func()
	    	Returns
	    		Text BEFORE code
	    		MAIN to be decorated
	    		Text AFTER code

    More on decorators: [Primer on Python Decorators](https://realpython.com/primer-on-python-decorators/).

        Examples:

        ```python
        # Decorator wrapping function defined
        def my_decorator(original_func):
            def wrap_func():
                print('Text BEFORE code')
                original_func()
                print('Text AFTER code')
            return wrap_func

        # A decorator of a function with arguments
        def do_twice(original_func):
            def wrap_func(*args, **kwargs):
                original_func(*args, **kwargs)
                original_func(*args, **kwargs)
            return wrap_func

        # My function to be decorated, decorated with @...
        @my_decorator
        def my_func_1():
            print('MAIN code to be decorated')

        @do_twice
        def my_func_2(name):
            print(f'Hi there, {name}!')

        ###

        my_func_1()
        # Text BEFORE code
        # MAIN to be decorated
        # Text AFTER code

        my_func_2('Mikel')
        # Hi there, Mikel!
        # Hi there, Mikel!
        ```

    Python has some built-in **decorators used in class methods**: `@classmethod`, `@staticmethod`, `@property`.

        The first two (`@classmethod`, `@staticmethod`) are explained in the OOP section (Advanced Functionalities)
        
        The decorator `@property` is used for defining attribute setters and getters,
        often for attributes we'd like to handle like private.
        The idea is to prefix an attribute with `_` so that we signal it should be treated as private.
        Simultaneously, we define a getter with `@property`,
        which returns the attribute with a layer of control.
        That makes sense in case we'd like to apply some transformations to the attribute (e.g., unit conversions).
        Similarly, a setter can be defined.

        ```python
        class Circle:
            def __init__(self, radius):
                self._radius = radius

            @property # getter: now, _radius can be get with obj.radius
            def radius(self):
                """Get value of radius"""
                return self._radius

            @radius.setter # setter: now, _radius can be set with obj.radius = value
            def radius(self, value):
                """Set radius, raise error if negative"""
                if value >= 0:
                    self._radius = value
                else:
                    raise ValueError("Radius must be positive")

            @property # getter, but no setter
            def area(self):
                """Calculate area inside circle"""
                return self.pi() * self.radius**2

            def cylinder_volume(self, height):
                """Calculate volume of cylinder with circle as base"""
                return self.area * height

            @classmethod # factory function: instance of class with radius = 1 created
            def unit_circle(cls):
                """Factory method creating a circle with radius 1"""
                return cls(1)

            @staticmethod # function independent from class / instance
            def pi():
                """Value of π, could use math.pi instead though"""
                return 3.1415926535

        ###

        c = Circle(5)
        c.radius # 5 (get)
        c.area # 78.5398163375 (get)

        c.radius = 2
        c.area # 12.566370614 (get)

        c.area = 100 # AttributeError: can't set attribute; we haven't defined @area.setter

        c.cylinder_volume(height=4) # 50.265482456

        c.radius = -1 # ValueError: Radius must be positive

        c = Circle.unit_circle() # Factory function: custom constructor
        c.radius # 1

        c.pi() # 3.1415926535
        Circle.pi() # 3.1415926535
        ```

    Other common applications:

        - Request logging
        - Timer
        - Debugging
        - Registering plugins
        - Slowing down code


        ```python
        # Python module which contains several handy decorators
        from decorators import debug, timer

        class TimeWaster:
            @debug
            def __init__(self, max_num):
                self.max_num = max_num

            @timer
            def waste_time(self, num_times):
                for _ in range(num_times):
                    sum([i**2 for i in range(self.max_num)])

        ###

        tw = TimeWaster(1000)
        # Calling __init__(<time_waster.TimeWaster object at 0x7efccce03908>, 1000)
        # '__init__' returned None

        tw.waste_time(999)
        # Finished 'waste_time' in 0.3376 secs

        ```

# GENERATORS

	Generators define sequences but do not compute them at once, instead one by one as they are called
	Advatange: huge lists do not need to be kept in memory
	Example: range() -> can be used in 'for' or with list()
	To create a generator: do not return a list, but create with for and use keyword yield

    In other words, yield suspends a function’s execution
    and sends a value back to the caller,
    but retains enough state to enable the function
    to resume where it left off.
    When the function resumes, 
    it continues execution immediately after the last yield run.
    This allows its code to produce a series of values over time, 
    rather than computing them at once and sending them back like a list.

    Commonly yield is used with a loop.

	# Traditional way: not memory efficient
	def create_cubes(n):
		result = [] # empty list
		for x in range(n):
			result.append(x**3)
		return result

	# Generator
	def create_cubes(n):
		for x in range(n):
			yield x**3

	for i in create_cibes(10)
		print(i)
	list(create_cubes(10))

	# Example Fibonacci
	def gen_fibon(n):
	    a = 1
	    b = 1
	    for i in range(n):
	        yield a
	        a, b = b, a + b

	list(gen_fibon(10))

	# next(): once generator is created, it accesses this function to deliver values one by one

	g = gen_fibon(10)
	next(g) -> 1
	next(g) -> 1
	next(g) -> 2
	next(g) -> 3
	...

	# iter(): create sequences in generators

	s = 'hello'
	s_iter = iter(s)
	next(s_iter) -> h
	next(s_iter) -> e
	next(s_iter) -> l
	...

	# generator comprehensions: like list comprehension, but use () insted: it is a generator

	my_list = [1, 2, 3, 4, 5]
	gencomp = (item for item in my_list if item > 3)
	for item in gencomp
		print(item)
	-> 4, 5

----------------
Python: Handling common objects
----------------

# Files (Automate Boring Stuff)
    
    Paths, files, directories, ...

        import os
        os.getcwd()
        os.sep
            unix: '/'
        os.path.join('folder1','folder2','file.xt')
            VERY IMPORTANT!
            folders joined with OS-specific separator (we can thus write OS-agnostic code)
            unix: 'folder1/folder2/file.xt'
        os.chdir('..')
        os.path.abspath('python')
            absolute path of file/folder given
        os.path.isabs('..')
            True if passed path is absolute
        os.path.relpath('folder1/folder2/file.png','folder1')
            rel path of the first wrt second
            'folder2/file.png'
        os.path.dirname('folder1/folder2/file.png')
            'folder1/folder2'
        os.path.basename('folder1/folder2/file.png')
            last folder or file
            'file.png'
        os.path.exists('/Users/mikel')
            True is it exists
        os.path.isfile('/Users/mikel')
        os.path.isdir('/Users/mikel')
        os.path.getsize('10_Regex.ipynb')
            get size of file in bytes
        os.listdir('.')
            returns list of all files and folders within passed path/folder
        os.makedirs()
            it creates all folders passed, relative or absolute

    Reading/Writing ASCII files: *.txt, *.py, ...
        
        test_file = open('text.txt')
            open a file in READ mode
        whole_text = test_file.read()
            read the whole content as a text
            after reading, the cursor is at the end
            whole_text: "Hello, how are you?\nIt's a beautiful day today!\n"
        test_file.close()
            close it

        test_file = open('text.txt')
            open a file in READ mode
        lines = test_file.readlines()
            read the whole content as a text
            and store each line as an item in a list
            lines: ['Hello, how are you?\n', "It's a beautiful day today!\n"]
            after reading, the cursor is at the end
        test_file.close()
            close it

        test_file = open('text2.txt', 'w')
            create/open file in WRITE mode: it overwrites contents if file exists
            we could also use APPEND mode 'a' if we'd like to append lines, instead of overriting
            file is created in CWD
        number_chars = test_file.write('this is a line\n')
        test_file.close()
            the content is written NOW to disk

    Storing/Reading complex python structures in files: lists, dictionaries
    This is done with the 'shelve' module

        import shelve
        shelveFile = shelve.open('mydata')
        shelveFile['cats'] = ['Zooei', 'Lindy', 'Patsy']
        shelveFile.close()
            on Unices, stored as './mydata.db'

        shelveFile = shelve.open('mydata')
            it's a dictionary!!
        list(shelveFile.keys())
            ['cats']
        list(shelveFile.values())
            [['Zooei', 'Lindy', 'Patsy']]
        shelveFile['cats']
           ['Zooei', 'Lindy', 'Patsy']


    Copying and moving files and folders
    This is done with the 'shutil' module

        import shutil
        shutil.copy(file, destination_folder)
        shutil.copy('file.txt', '../')
            file copied to upper folder
        shutil.copy('file.txt', '../file_copied.txt')
            file copied to uper folder and renamed
        shutil.copytree('./folder', './folder_backup')
            copy entire folder (eg, as backup)
        shutil.move('file.txt', '../')
            equivalent to copy, but now we are moving
            we can also equivalently rename with move

    Deleting files

        import os
        os.unlink('myfile.txt')
            remove myfile.txt, a bit old naming
        os.rmdir('./folder')
            remove EMPTY folder
        import shutil
        shutil.rmtree('./folder')
            remove folder and its contents!
            be careful what you automate to delete...


        to avoid permannently deleting files: send2trash

        pip install send2trash
        pip3 install send2trash

        import send2trash
        send2trash.send2trash('file.txt')
            file sent to trash

    Walking a directory tree

        for foldername, subfolders, filenames in os.walk('/path/to/folder'):
            print('Folder name: ' + foldername)
            print('Subfolders: ' + str(subfolders))
            print('Files: ' + str(filenames))

        for eeach folder in our path, folder name, subfolders & files are listed
        then, walk goes recursively into subfolders and does the same

        another option: use glob for unix-like path-pattern expansion

            import glob
            import os
            for file in glob.glob(os.path.join("path/", "folder/", "*")):
                # do something


# Web scrapping

    Openning web pages: webbrowser module

        import webbrowser
        
        # Open a website
        webbrowser.open('https://automatetheboringstuff.com')

        # Tip: look for structure in webpages that provide services to use them
        # Example: google maps
        def mapit(adress):
            url = 'https://www.google.com/maps/place/' + adress
            webbrowser.open(url)

        mapit('Tximistarri bidea 9, San Sebastian')

    Downloading from web pages

        Install: requests
            pip install requests
            pip3 install requests
            conda install requests
        More info on requests module
            https://requests.readthedocs.io/en/master/

        import requests
        res = requests.get('https://automatetheboringstuff.com/files/rj.txt')

        # check if everything went ok with the status code
        # 200: everything ok, 404: not found, etc
        res.status_code

        # raise an error if something went wrong
        res.raise_for_status()

        # access the text in the downloaded text file
        len(res.text)

        # If we want to save the file AND maintain the encoding
        # we need to save the text in binary!
        text_file = open('romeo_and_juliet.txt', 'wb') # create & write as binary

        # binary data can be stored in bhunks of bytes
        # we can decide the size of the chunk
        for chunk in res.iter_content(10000):
            text_file.write(chunk)

    Parsing HTML with beautifulsoup module

        # Firefox: F12, HTML elements shown; right click + 'Inspect element', HTML code highllighted
        # requests downloads the whole webpage text; but we need to parse it --> pip/3/conda install beautifulsoup4

        import bs4
        import requests
        res = requests.get('https://automatetheboringstuff.com')
        res.raise_for_status()
        soup = bs4.BeautifulSoup(res.text)

        # Firefox, F12, select element + right click -> inspect; in code, right click + Copy > CSS Path
        # Then, paste it in .select()
        # elems: we get the HTML part
        elem = soup.select('html body div.main div ul li a')

        # Visualize content
        elem

        # Visualize content of first element
        elem[0]

        # Get text
        elem[0].text

        # Strip text to the minimum
        elem[0].text.strip()

        Example: get price of Amazon product
            For me, it did't work, probably because Amazon changed their page to hide this kind of info...
            That's actually a pitty, because the application of getting automatically the amazon price of hundred of thousands of products is very interesting...


        Other cool applications we could try
            - weather.org: parse weather
            - xkcd.com: donwload comic strips: download current, and go back following Prev link

    Controling the web browser with the selenium module

        Sometimes webpages rely on javascript, or you need to log in, etc - then, downloading the content is not enough
        The Selenium 3rd party modules allows controlling web pages
        We can fill out forms, click submit buttons, etc.
        It's slower than beautifulsoup, because it opens a browser

        More info
            https://selenium-python.readthedocs.io

        Install selenium
            pip/3/conda install selenium
 
        Install the geckodriver for controlling firefox (mac system explained here)
            Source of this info
                https://learn-automation.com/firefox-browser-on-mac-using-selenium-webdriver/
                https://firefox-source-docs.mozilla.org/testing/geckodriver/Notarization.html
            https://github.com/mozilla/geckodriver/releases
                download latest release of geckdriver & expand it to be the geckdriver binary 
            cd ~/Downloads
            xattr -r -d com.apple.quarantine geckodriver
                bypass the Apple notarization
            mv geckodriver /usr/local/bin/
                move the binary to /usr/local/bin/


        from selenium import webdriver
        # Open the browser
        browser = webdriver.Firefox()
        # Open a page/URL
        browser.get('https://automatetheboringstuff.com/')
        
        # Get an element
        # Firefox: right click + Inspect element, right click on code + copy CSS selector
        elem = browser.find_element_by_css_selector('.main > div:nth-child(1) > ul:nth-child(21) > li:nth-child(1) > a:nth-child(1)')
        
        # Once we have the element, we can perform typical actions on it with code!
        elem.click()
        # We can also get more general elements, eg, all paragraphs
        elems = browser.find_elements_by_css_selector('p')
        len(elems)
        
        # Other Selenium's webdriver methods
        # https://automatetheboringstuff.com/2e/chapter12/
        # browser.find_element_by_class_name()
        # browser.find_elements_by_class_name()
        # browser.find_element_by_id()
        # browser.find_elements_by_is()
        # ...
        
        # Use the search field
        browser.get('https://nostarch.com/automatestuff2')
        searchElem = browser.find_element_by_css_selector('div.logo-wrapper:nth-child(2) > div:nth-child(1) > div:nth-child(1) > section:nth-child(1) > form:nth-child(1) > div:nth-child(1) > div:nth-child(1) > div:nth-child(2) > input:nth-child(1)')
        # Write text into field + submit text (no need of clicking, submit detects what to do)
        searchElem.send_keys('deep learning')
        searchElem.submit()
        
        # Navigate
        browser.back()
        browser.forward()
        browser.refresh()
        # Close browser
        browser.quit()

        browser = webdriver.Firefox()
        browser.get('https://automatetheboringstuff.com/')
        # Get a paragraph element
        elem = browser.find_element_by_css_selector('.main > div:nth-child(1) > p:nth-child(9)')
        # Extract text
        elem.text
        # Get entire page: html is the element in a webpage that contains the whole web page
        elem = browser.find_element_by_css_selector('html')
        elem.text

# APIs



# Spreadsheets: XLSX: openpyxl
    
    *.xlsx files are proprietary, but we can read them with openpyxl
    Elements inside
        Workbook: an XLSX file
        Worksheet: sheets inside a workbook
        Columns (letters), Rows (numbers), Cells

    Install
        pip/3/conda install openpyxl    

    import openpyxl
    workbook = openpyxl.load_workbook('example.xlsx')
    
    # Get the names of all the sheets in a list
    workbook.sheetnames
    # Old method to get a sheet
    sheet = workbook.get_sheet_by_name('Sheet1')
    # New method to get a sheet
    sheet = workbook['Sheet1']

    # Get cell values
    cell = sheet['A1']
    # Get the value of a cell: the ofrmat is the one set in XLSX!
    cell.value
    str(sheet['A1'].value)
    sheet['B1'].value
    sheet.cell(row=1, column=2)
    sheet.cell(row=1, column=2).value

    # Create a new worksheet
    wb = openpyxl.Workbook()
    wb.sheetnames
    sheet = wb['Sheet']

    # Asing values to cells
    sheet['A1'] = 42
    sheet['A2'] = 'Hello'

    # Save workbook in CWD (os.chdir where we want first)
    wb.save('example2.xlsx')

    # Create a new sheet to the end
    sheet2 = wb.create_sheet()
    # Change sheet names
    sheet2.title = 'My new sheet name'
    # Create a new sheet where we want
    sheet3 = wb.create_sheet(index=0, title='My other new sheet')

# PDFs

    PDF files are binary and are quite hard to parse without 3rd party modules.
    We use PyPDF2, but even it could have problems extracting the text.
    PyPDF extracts only text.
    PyPDF2 can edit only at the page level: we cannot add text, but move pages, etc - but we can't create new PDF pages with text or any othe content in them.

    # Install
    # pip/3/conda install pypdf2

    import PyPDF2
    pdfFile = open('dl_nd_syllabus.pdf', 'rb') # open it in read mode + binary!
    reader = PyPDF2.PdfFileReader(pdfFile)
    reader.numPages

    # Get a page element
    page = reader.getPage(4) # pages start at 0
    page.extractText() # extract pae text - warning: it won't always work...

    # Create a blank new PDF object
    writer = PyPDF2.PdfFileWriter()

    # Add only every second page of the original PDF to the new blank file
    for p in range(0,reader.numPages,2):
        page = reader.getPage(p)
        writer.addPage(page)

    outputFile = open('dl_nd_syllabus_half.pdf', 'wb')
    writer.write(outputFile)

    outputFile.close()
    pdfFile.close()

# Word documents: DOCX

    DOCX documents contain paragraph elements, which are created every time we hit ENTER
    Paragraphs have a style (as defined in Word: 'Normal', 'Title', etc) and 1+ runs
        each run is a specific formatting for a set of characters: bold, italic, underline
        a run starts every time we change the format
    Al says the current version cannot insert paragraphs/runs at desired places, just append them; but maybe that's a feature that will come in future versions

    # Install
    # pip/3/conda install python-docx

    import docx

    # open a file
    d = docx.Document('demo.docx')
    # DOCX documents contain paragraph elements (each starts with an ENTER)
    d.paragraphs
    # Extract the text of a paragraph
    d.paragraphs[0].text
    p = d.paragraphs[1]
    
    # Each paragraph object contains + runs
    # Each run starts whenever we change style
    p.runs
    # Run object have also a text variable
    p.runs[1].text # True
    p.runs[1].italic # None
    p.runs[1].underline # None

    # Change content and save it
    p.runs[3].text = 'italic and underline'
    p.runs[3].underline = True
    d.save('demo2.docx')

    # paragraphs have also style variables
    p.style
    p.style.name
    p.style = 'Title'
    d.save('demo2.docx')

    # Create a new document and add paragraphs & runs
    d = docx.Document()
    d.add_paragraph('This is the content of my first paragraph. ')
    d.add_paragraph('This another paragraph. ')
    d.save('demo2.docx')

    p = d.paragraphs[0]
    p.add_run('This is a new run. ')
    p.runs
    p.runs[1].bold = True
    d.save('demo2.docx')

    # Extract the ASCII text (string) from a Word document
    def getText(filename):
        doc = docx.Document(filename)
        fullText = []
        for p in doc.paragraphs:
            fullText.append(p.text)
        return '\n'.join(fullText)

    full_text = getText('demo.docx') 
    print(full_text)

# Emails

    Use cases:
        send email notifications if a programs finds sth in a spreadsheet
    
    Email protocols are as old as the internet and not user friendly. That's why we get many old-fashioned byte literals `b''` and weird output.

    Send emails: SMTP

        SMTP protocol used: Simple Mail Transfer Protocol

        import smtplib    
        
        # Create connection to an SMTP server with port 587 (that's the usual port)
        conn = smtplib.SMTP('smtp.gmail.com', 587)

        # Start connection (ehlo means helo); 250 answer means connection is OK
        conn.ehlo()

        # Start TLS encryption (for securely sending our PW); 220 answer means everything OK
        conn.starttls()

        # In order to user the Google mail, we need to create an App Password or a Google specific password
        # https://support.google.com/accounts/answer/185833?hl=en
        # Steps
        # - go to https://myaccount.google.com/
        # - select Security
        # - activate 2-Step Verification
        # - in Signing in to Google, select App passwords
        # - create a new passoword which will be used by the script (these passwords are tied to specific devices and programs)

        # Log in with usermail + pw
        # WARNING: DO NOT UPLOAD FILE WITH PW ANYWHERE!
        usermail = 'username@gmail.com'
        pw = '...' # DO NOT SHARE! Insert here the app-specific pw we generated
        conn.login(usermail,pw)

        # The structure of an email is fix and it should be "Subject: <My Subject>\n\n<Body text>"
        EmailText = '''Subject: My nice Subject\n\n
        Dear Mikel,\n
        This is a test.\n
        All the best,\n
        Mikel
        '''

        # Send email; return is a dictionary with emails not sent, so an empty dict is a good signal
        # from email first
        # to emails second - we can write to several, see docs; but companies cap number of emails to 150 usually
        conn.sendmail(usermail, 'mxagar@gmail.com', EmailText)

        conn.quit()

    Receive/read emails: IMAP

        Email protocols are very old and not user friendly.
        Python has the library `imaplib`, but we won't use it.
        Instead, we're going to use `imapclient` and `pyzmail`.

            # Install modules
            # pip/3/conda install imapclient/pyzmail36

        import imapclient
        # Connect to the IMAP server of the email provider
        conn = imapclient.IMAPClient('imap.gmail.com', ssl=True)
        conn.login('username@gmail.com', 'app-specific-pw')

        # List all folders we have
        conn.list_folders()

        # Get the IDs of the emails since a spacific date
        # We select an ID from the list to get the email content
        # The search function has many keys - see the docs: BEFORE, SUBJECT/BODY/TEXT string, SEEN/UNSEEN, etc.
        UIDs = conn.search(['SINCE 20-Aug-2020'])

        # Get the raw message of email with ID 47474
        # Although the content is cryptic, we can already understand what's inside of it
        rawMessage = conn.fetch([47474], ['BODY[]', 'FLAGS'])

        # Raw message needs to be parsed
        # This is done wth pyzmail
        import pyzmail

        # To parse the message we need to pass its ID again and specify we want the body
        message = pyzmail.PyzMessage.factory(rawMessage[47474][b'BODY[]'])

        # Get email subject
        message.get_subject()

        # Get adresses
        message.get_adresses('from')
        message.get_adresses('to')
        message.get_adresses('bcc')

        # Do we have plain text or HTML email?
        message.text_part == None
        message.html_part == None

        # Get plain text message (usually UTF-8 encoding is used)
        message.text_part.get_payload().decode('UTF-8')
        # If we want to delete a message: open folder not in readonly mode
        # conn.select_folder('INBOX', readonly=False)
        # Select email ID and delete it
        conn.delete_messages([47474])

        # Close session
        conn.logout()


# GUI Automation

    pip/3/conda install pyautogui
    

# CSV


    # CSV module comes with python
    import csv

    # Open file
    exampleFile = open('example.csv')

    # Parse file: reader object
    exampleReader = csv.reader(exampleFile)

    # Convert to a list of row contents
    exampleData = list(exampleReader)

    # Access data: [row][column]
    exampleData[0][1]

    # Modify data: exampleData is already a list of lists, nothing to do with CSV
    exampleData[0][1] = 'Dragon fruits'

    # Create a CSV file: writer object
    outputFile = open('output.csv', 'w', newline='') # Windows requires newline='' to avoid double newlines
    outputWriter = csv.writer(outputFile)

    # Add new rows: just input the list containing the row content; return is num of chars
    outputWriter.writerow(['spam', 'eggs', 'bacon', 'ham'])
    outputWriter.writerow(['Hello, world!', 'eggs', 'bacon', 'ham'])
    outputWriter.writerow([1, 2, 3.141592, 4])

    # Save and close
    outputFile.close()

    # If we want to change the separator and the line terminator, eg. TAB and DOUBLE LINE
    # csvWriter = csv.writer(csvFile, delimiter='\t', lineterminator='\n\n')

    # CSV files with headers are better handled with DictReader & DictWriter
    # WATCH OUT: once we access the content of it, it seems it's erased?

    import csv

    exampleFile = open('exampleWithHeader.csv')
    exampleDictReader = csv.DictReader(exampleFile)
    for row in exampleDictReader:
        print(row['Timestamp'], row['Fruit'], row['Quantity'])

    # If we are opening a file without a header, we can specify it as
    # exampleDictReader = csv.DictReader(exampleFile, ['time', 'fruit', 'amount'])

    # Create CSV files with header with DictWriter

    import csv

    outputFile = open('output.csv', 'w', newline='')

    outputDictWriter = csv.DictWriter(outputFile, ['Name', 'Pet', 'Phone'])

    # Add/skip thi sline if we want/don't want to have th eheader in our CSV
    outputDictWriter.writeheader()

    # Row contents are entered as dictionaries, order doesn't matter
    outputDictWriter.writerow({'Name': 'Alice', 'Pet': 'cat', 'Phone': '555-1234'})
    # We can ommit column values
    outputDictWriter.writerow({'Name': 'Bob', 'Phone': '555-9999'})
    outputDictWriter.writerow({'Phone': '555-5555', 'Name': 'Carol', 'Pet':'dog'})

    outputFile.close()


# JSON: JavaScript Object Notation

    Popular way to format data as a single human-readable string.
    We don't need to use JavaScript, it's just a way of formatting a data structure as a string.
    Many web APIs communicate with JSON.

    # Typical JSON string describing a person (Wikipedia)
    # Note:
    # - fields as dictionary key:value pairs
    # - structures in {}
    # - arrays with []
    # - elements separated with ,
    # - types: strings "..." (always with double quotes), numbers 27, null, true/false
    # - in python, they are represented as dictionaries which can contain thse types:
    #   dictionary, list, integer, float, string, Boolean, or None
    personJSON = '''
    {
      "firstName": "John",
      "lastName": "Smith",
      "isAlive": true,
      "age": 27,
      "address": {
        "streetAddress": "21 2nd Street",
        "city": "New York",
        "state": "NY",
        "postalCode": "10021-3100"
      },
      "phoneNumbers": [
        {
          "type": "home",
          "number": "212 555-1234"
        },
        {
          "type": "office",
          "number": "646 555-4567"
        }
      ],
      "children": [],
      "spouse": null
    }
    '''

    # Python native module that translates between JSON <-> python objects
    import json

    # Example JSON string (note strings with double quotes)
    stringOfJsonData = '{"name": "Zophie", "isCat": true, "miceCaught": 0, "felineIQ": null}'

    # Convert JSON string to Python dictionary
    jsonDataAsPythonValue = json.loads(stringOfJsonData)

    # Convert Python dictionary to JASON string
    pythonValue = {'isCat': True, 'miceCaught': 0, 'name': 'Zophie', 'felineIQ': None}
    stringOfJsonData = json.dumps(pythonValue)


# Google Sheets
    
    Al Sweigart has created a python module which connects to Google Spreadsheets:
        https://ezsheets.readthedocs.io/en/latest/

    In order to use it, you need to enable the Google API following the instructions in
        https://automatetheboringstuff.com/2e/chapter14/

    Basically, you need to download 3 files

        - a credentials.json file which needs to be renamed to credentials-sheets.json
        - a token-drive.json file which is created when first importing ezsheets once the credentials-sheets.json file has been correctly set
        - a token-sheets.json file, analogous to the previous

    Thus, the first time ezsheets is imported, several clicks must be done. After that, you need to have those mentioned 3 files in the folder where the python script using ezsheets is located.

    VERY IMPORTANT: Never upload the credentials and token files nor show them to anyone, treat them as your account passwords!

    NOTE: to revoke credential access:
        https://console.developers.google.com/
        Credentials: Remove

    import ezsheets
    # ezsheets.init() # this method is called automatically the first time 

    # Upload a XLSX, CSV
    # ss = ezsheets.upload('my_spreadsheet.xlsx')

    # To open an existing spreadsheet, open its id, example:
    # https://docs.google.com/spreadsheets/d/XXX
    # ID comes after d/: XXX
    # NOTE: We need to have access to that spreadsheet with the account linked in the credentials-sheets.json file
    # IMPORTANT: we donwload the content locally; changes do not affect unless we upload them manually
    ss = ezsheets.Spreadsheet('XXX')

    # Show ID of spreadsheet: XXX
    ss.spreadsheetId

    # Show complete URL of spreadsheet
    ss.url

    ss.title
    ss.sheetTitles
    ss.sheets

    # First sheet, located at position 0
    sheet1 = ss[0]
    # Sheet with name 'Learning'
    sheet2 = ss['Learning']
    sheet3 = ss.sheets[2]
    # Refresh local data reading from online Spreadsheet
    ss.refresh()
    sheet1.rowCount
    sheet1.columnCount
    ezsheets.convertAddress(1, 2) # 'A2'
    ezsheets.getColumnLetterOf(2) # 'B'
    ezsheets.convertAddress('A2') # (1, 2)
    sheet1.getRow(1) # get first row, not 0
    sheet1.getColumn(1) # get first column, not 0
    sheet1.getColumn('A')

    # Download contant, modify locally, upload changes
    rows = sheet1.getRows() # Get every row in the spreadsheet

    # Modify content: we modify contents on local object and the upload them
    # rows[1][0] = 'PUMPKIN'
    # Upload changes
    # sheet1.updateRows(rows)

    # Create a Spreadsheet, a Sheet, delete a sheet
    # ss = ezsheets.createSpreadsheet('My Spreadsheet')
    # ss.createSheet('Spam')
    # ss[0].clear() # clear all the cells in sheet at position 0
    # ss[0].delete() # delete sheet at position 0

# GMail


    Al Sweigart has a similar module called `ezgmail` to interact with gmail:
        https://ezgmail.readthedocs.io/en/latest/

    Similarly, the Google API needs to be enabled and the file `credentials.json` downloaded; that file must be in the same folder as the python file in which we execute `import ezgmail`. 

    The usage is simple and it is clearly explained in the webpage:
        https://ezgmail.readthedocs.io/en/latest/


----------------
ADVANCED TOPICS
----------------

# COLLECTIONS: COUNTER

	Dictionary that count the number of elements in a list

	# Example: Numbers
		from collections import Counter
		
		lst = [1,2,2,2,2,3,3,3,1,2,1,12,3,2,32,1,21,1,223,1]
		c = Counter(lst) # c is a Counter
		-> {1: 6, 2: 6, 3: 4, 12: 1, 21: 1, 32: 1, 223: 1}

	# Example: words
		s = 'How many times does each word show up in this sentence word times each each word'
		words = s.split() # split by whitespace
		c = Counter(words) # c is a Counter
		c.most_common(2)
		-> [('each', 3), ('word', 3)]

	# Common patterns for counters (c is a Counter)

		sum(c.values())                 # total of all counts
		c.clear()                       # reset all counts
		list(c)                         # list unique elements
		set(c)                          # convert to a set
		dict(c)                         # convert to a regular dictionary
		c.items()                       # convert to a list of (elem, cnt) pairs
		Counter(dict(list_of_pairs))    # convert from a list of (elem, cnt) pairs
		c.most_common()[:-n-1:-1]       # n least common elements
		c += Counter()                  # remove zero and negative counts

# COLLECTIONS: defaultdict

	Like a dictionary, but with defaut value, used when asked key does not exist (which regularly would raise error)

	# Example

		from collections import defaultdict

		d = defaultdict(lambda: 0)

		d['one']
		-> 0
		d['two'] = 2
		d
		-> {'one': 0, 'two': 2}

# COLLECTIONS: OrderDict

	Dictionary subclass which remembers the order in which items were added

	# Example

		from collections import OrderedDict

		# d = {} # that would be a regular dictionary which does not remember the order
		d = OrderedDict() # this dictionary will remember the order

		d['a'] = 1
		d['b'] = 2
		d['c'] = 3
		d['d'] = 4
		d['e'] = 5

		for k,v in d.items():
			print(k,v)

# COLLECTIONS: namedtuple

	A fast way of creating structs/classes

	# Example

		from collections import namedtuple

		Dog = namedtuple('Dog', 'age breed name') # (<name of the class>, <attribute names separated with whitespace>)

		sam = Dog(age = 5, breed = 'lab', name = 'Sammy')
		frank = Dog(age = 6, breed = 'shepard', name = 'Franky')

		sam.age
		-> 5
		sam[2]
		-> 'Sammy'
		...


# COLLECTIONS: deque

	Double-ended queue: a list to/from which we can append/remove from right (default) or left

	# Example

		from collections import deque

		d = deque('ghi')

		d.append('j')                    # add a new entry to the right side
		d.appendleft('f')                # add a new entry to the left side
		d                                # show the representation of the deque
		-> deque(['f', 'g', 'h', 'i', 'j'])

		d.pop()                          # return and remove the rightmost item
		-> 'j'
		d.popleft()                      # return and remove the leftmost item
		-> 'f'
		list(d)                          # list the contents of the deque
		-> ['g', 'h', 'i']

# DATETIME

	import datetime

	# Time object: look for more attributes with SHIFT + TAB: min, max, time zone, resolution, etc 
	t = datetime.time(5, 25, 1) # hours, minutes, seconds (milliseconds, microseconds)

	# Date object: look for more attributes with SHIFT + TAB: day, month, year, ...
	d1 = datetime.date.today()
	d2 = datetime.date(2019, 2, 27)
	d3 = d2.replace(year = 1999)

	# Arithmetics with these structures are possible
	d4 = d2 - d3
	d4.days
	-> 7305

# DEBUGGER: pdb

	import pdb

	... code ...

	# we put a breakpoint here and code stops and asks for input interactively: we can check variables, c for continue, etc.
	# we exit with 'q'
	pdb.set_trace()

	... code ...


# TIMEIT: time your code

	Measure time required to perform a piece of code

	# Example 1: use as a fuction
	
		import timeit

		"-".join(str(n) for n in range(100))
		-> '0-1-2...-99'

		# measure time: pass code as string and especify number of times to be executed
		
		timeit.timeit('"-".join(str(n) for n in range(100))', number = 10000)
		-> 0.3552501480007777 # seconds required for running the above code 10000x

		timeit.timeit('"-".join([str(n) for n in range(100)])', number = 10000)
		-> 0.31969968200064613

		timeit.timeit('"-".join(map(str,range(100)))', number = 10000)
		-> 0.25055357000019285 # map() is themost efficient!

	# Exmaple 2: use with magic word timeit: most comfortable way

		import timeit

		%timeit "-".join(map(str,range(100)))
		-> 24.8 µs ± 3.1 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)

    # Example 3: Simple Stopwatch

        import timeit

        # Stopwatch
        start_time = timeit.default_timer() # store current time
        # Code for any process you want to time
        timing = timeit.default_timer() - start_time # Time elapsed

# REGULAR EXPRESSIONS

	Many parsing problems can be solved with regular expressions
	Common interview question
	Look official documentation

    # Automate the Boring Stuff

        The method followed by Al Sweigart is a bit different than the one by Portilla

        Raw strings for defining regular expressions

            r'blah\blah': raw string: \ is treated as a character!
        
        Special character classes

            \d digit or numeric 0-9
            \D not \d
            \w letters, \d and _
            \W not \w
            \s space, tab, newline
            \S not \s

        The re regex module

            import re
            phoneNumRegex = re.compile(r'\d\d\d-\d\d\d-\d\d\d')
                re.compile() creates a regex object
                    we can also pass options to it, eg re.IGNORECASE
                        re.compile(r'a', re.IGNORECASE)
                a raw string is passed woith regex syntax: \d = digit
            matchObject = phoneNumRegex.search('My number is 943-111-222, and yours?')
                a match object is returned
            print(matchObject.group())
                >> '943-111-222'

            matchObject = phoneNumRegex.findall('My number is 943-111-222, and yours?')
                now, a list is returned, not a match object
                thus, we don't need a .group() method!
            print(matchObject[0])

        Groups: (): parenthesis create groups within regex, if not escaped with backslash
            phoneNumRegex = re.compile(r'(\d\d\d)-(\d\d\d-\d\d\d)')
            matchObject = phoneNumRegex.search('My number is 943-111-222, and yours?')
            matchObject.group() -> '943-111-222'
            matchObject.group(1) -> '943'
            matchObject.group(2) -> '111-222'

            if we use findall() instead of search(), we get a list of tuples
                each element a tuple is the group item

        Pipe Character: |
            
            batmanRegex = re.compile(r'Bat(man|mobile)')
                both Batman & Batmobile can be found

	# Search for patterns (Udemy Portilla)
		
		import re

		patterns = ['term1', 'term2']
		text = 'This is a string with term1, but not with the other term'

		match = re.search(patterns[0], text) # more than True/False

		match.start()
		-> 22 # position
		match.end()
		-> 27 # position

	# Spliting (Udemy Portilla)

		split_term = '@'
		email = 'mikel@gmail.com'
		re.split(split_term, email)
		-> ['mikel', 'gmail.com']

	# Find all occurrences (Udemy Portilla)

		re.findall('match', 'This is a match and this is a match also')
		-> ['match', 'match']

	# REPETITION SYNTAX: *, +, ?, {}, . (Udemy Portilla & Automate Boring Stuff)

        special characters can be escaped with backslash: \

		text_patterns = [ 	'sd*',     		# s followed by zero or more d's
                			'sd+',          # s followed by one or more d's
                			'sd?',          # s followed by zero or one d's
                			'sd{3}',        # s followed by exactly three d's
                			'sd{2,3}'      # s followed by exactly two to three d's
                            '.'            # any one characer except newline 
                		]

		for pattern in text_patterns:
    		print(re.findall(pattern, my_text))
    	-> All occurrences found

        Repetition symbols refer to the last special symbol or group ()
            batRegex = re.compile(r'Bat(wo)?man') -> both Batman and Batwoman found!
            match = batRegex.search('I saw Batman yesterday')
            match.group() -> 'Batman'

        Note: search operations are greedy: the longest match is found if we have repetitions
            BUT we can make it non-greedy with ?
                digitsRegex = re.compile(r'(\d){3,5}')
                match = digitsRegex.search('123456789') # '12345'
                digitsRegex = re.compile(r'(\d){3,5}?')
                match = digitsRegex.search('123456789') # '123'

        Special combination: .* = whatever; example

            nameRegex = re.compile(r'First name: (.*) Last name: (.*)')
            nameRegex.findall('First name: Mikel Last name: Sagardia')
                [('Mikel', 'Sagardia')]

    # CHARACTER SETS: [] (Udemy Portilla)

    	text_patterns =	[	'[sd]',    # either s or d
                			's[sd]+'   # s followed by one or more s or d
                		]

		for pattern in text_patterns:
    		print(re.findall(pattern, my_text))
    	-> All occurrences found

    # EXCLUSION: ^ all except the symbols that follow (Udemy Portilla)

    	re.findall('[^!.?, ]+', my_text) # we remove all punctuation marks !.?, and the white space

        WATCH OUT: ^ means negation only within [], otherwise it means 'must go at the begining', eg

            beginsHello = re.compile(r'^Hello')
            m = beginsHello.search('Hello everybody') -> found!
            m = beginsHello.search('He said "Hello"') -> NOT found!

            Similarly, $ can mean 'must go at the end'
    
                endsWorld = re.compile(r'wordl!$')
                m = endsWorld.search('Hello world!') -> found!


    # CHARACTER RANGES (Udemy Portilla)

    	text_patterns =	[	'[a-z]+',      # sequences of lower case letters
               				'[A-Z]+',      # sequences of upper case letters
               				'[a-zA-Z]+',   # sequences of lower or upper case letters
               				'[A-Z][a-z]+'  # one upper case letter followed by lower case letters
               			]

        -> '[aeiouAEIOU]': we can use that for finding all the vowels

    # ESCAPE CODES: r\x where x is any of the following (Udemy Portilla)

    	test_patterns = [ 	r'\d+', # sequence of digits
                			r'\D+', # sequence of non-digits
                			r'\s+', # sequence of whitespace
                			r'\S+', # sequence of non-whitespace
                			r'\w+', # alphanumeric characters
                			r'\W+', # non-alphanumeric
                		]

    # REPLACE: sub() (Automating Boring Stuff - Al Sweigart)

        namesRegex = re.compile('Agent \w+')
        namesRegex.sub('Maria', 'Agent Bob told Agent Alice that he had a secret')
        -> 'Maria told Maria that he had a secret'

# StringIO

	Convert strings to be files
	Used when web scripting

	# Example

		import io
		message = 'This is just a normal string.'
		f = io.StringIO(message)

		# Now we can treat f like a file
		f.read()
		f.write(' Second line written to file like object')
		...

# Advanced NUMBERS

	hex(12) # hexadecimal representation: 0x...
	bin(512) # binary representation: 0b...
	pow(2,4) # 2**4
	pow(2,4,3) # (2**4) % 3
	abs(-1) # 1
	round(3.1) # 3.0
	round(3.9) # 4.0
	round(3.1415, 2) # 3.14

# Advanced STRINGS

	s = 'hello world'
	s.capitalize() # 'Hello world'
	s.upper() # 'HELLO WORLD'
	s.lower() # 'hello world'
	s.count('o') # 2
	s.find('o') # 4
	s.isalnum() # True iff all characters are alphanumeric - whitespace is NOT alphanumeric
	s.isalpha() # True iff all characters are alphabetic
	s.islower() # True iff all characters are lower case
	s.isspace() # True iff all characters are some kind of space: ' ', '\t', ...
	s.istitle() # True iff title case
	s.isupper() # True iff all characters are upper case
	s.endswith('o') # s[-1] == 'o'
    "--".join(['a','b','c']) # 'a--b--c'
    'My name is Mikel'.split() # list of all words; we can also specificy splitting char
    'Hello'.rjust(20) # right justify (add whitespaces) to have 20 chars in total; we can also specify filling char
    'Hello'.ljust(20) # same but left
    'Hello'.center(20) # same but center
    text.strip() # remove white spaces (or passed chars)
    text.lstrip() / text.rstrip() # same but left & right
    text.replace('ie', 'ii')

	# Some regex-like functions built-in into strings
	s.split('o') # ['hell', 'w', 'rld'] all 'o's found and s split in them
	s.partition('o') # ['hell', 'o', 'world'] only first 'o' found and returned as separator

# Advanced SETS

	s = set()
	s.add(1)
	s.add(2)
	s.add(2) # not added
	s.add(3)
	s.clear() # all elements removed
	s = {1, 2, 3}
	sc = s.copy() # all contents copied to a new set
	s.add(4)

	s.difference(sc) # {1, 2, 3} vs {1, 2, 3, 4}
	-> {4}

	s1 = {1, 2, 3}
	s2 = {1, 4, 5}
	s1.difference_update(s2) # returns s1 after removing all elements found in s2
	-> {2, 3}

	s = {1, 2, 3}
	s.discard(2) # remove element with value 2
	-> {1, 3}

	s1 = {1, 2, 3}
	s2 = {1, 2, 4}
	s1.intersection(s2)
	-> {1, 2}

	s1 = {1, 2, 3}
	s2 = {1, 2, 4}
	s1.intersection_update(s2) # same as intersection but s1 is assigned with the intersection result
	-> s1 = {1, 2}

	s1 = {1, 2}
	s2 = {1, 2, 4}
	s3 = {5}
	s1.isdisjoint(s2) # do they have an intersection or is intersection Null?
	-> False
	s1.isdisjoint(s3)
	-> True

	s1 = {1, 2}
	s2 = {1, 2, 4}
	s1.issubset(s2)
	-> True
	s2.issuperset(s1)
	-> True

	s1 = {1, 2}
	s2 = {1, 2, 4}
	s1.symmetric_difference(s2) # returns what is not in the intersection
	-> {4}

	s1 = {1, 2}
	s2 = {1, 2, 4}
	s1.union(s2)
	-> {1, 2, 4}
	s1.update(s2) # same as union, but s1 is assigned the union
	-> s1 = {1, 2, 4}

# Advanced DICTIONARIES

	d = {'k1': 1, 'k2': 2}

	# Dictionary comprehensions: not usual, but possible

		{x:x**2 for x in range(10)}

		{k: v for k, v in zip(['a', 'b'], range(2))}

	# Iteration

		for kv in d.items():
			print(kv)

		for k in d.keys():
			print(k)

		for v in d.values():
			print(v)

# Advanced LISTS

	l = [1, 2, 3]

	l.append(4) # [1, 2, 3, 4]
	l.append([5, 6]) # [1, 2, 3, 4, [5, 6]]
	l.extend([5, 6]) # [1, 2, 3, 4, 5, 6]
	l.count(1) # 1
	l.index(2) # 1

	l = [1, 2, 3]
	l.insert(2, 'inserted') # [1, 2, 'inserted', 3]
	element = l.pop() # element = 3, l = [1, 2, 'inserted']
	element = l.pop(0) # element = 1, l = [2, 'inserted']

	l = [1, 2, 3, 2]
	l.remove(2) # l = [1, 3, 2] only first instance removed!

	l = [1, 2, 3]
	l.reverse() # [3, 2, 1]

	l = [1, 2, 4, 3]
	l.sort() # [1, 2, 3, 4]

# GUIs in Jupyter Notebook

	Create very simple GUIs on a notebook
		
	# Example with decorator

		from ipywidgets import interact, interactive, fixed
		import ipywidgets as widgets

		# interact decorator provides a graphical controller for the function we especify
		# default parameters are passed
		@interact(x = 10, y = fixed(2.0))
		def func(x,y):
			return x**2,y

			# Numbers: Slider appears for controlling x
			# Booleans: Checkbox
			# Strings: Textbox

	# Exaple without decorator

		from ipywidgets import interact, interactive, fixed
		import ipywidgets as widgets

		def func(x):
			return x**2

		# function and default values passed
		interact(func, x = 10)

	# Control slider parameters
		@interact(x = widgets.IntSlider(min = -100, max = 100, step = 1, value = 0))
		@interact(x = widgets.FloatSlider(min = -100, max = 100, step = 1, value = 0))
		...

	# Drop-down menus: lists
		@interact(x = ['hello', 'bye']])

	# Widgets

		# General code

			import ipywidgets as widgets
			from IPython.display import display

			w = widgets.<TAB>(<options>)
			display(w)
			...
			w.close()

		# Examples

			w = widget.IntSlider() # Slider for ints
				w.value # access value
				w.value = x # assign value
				w.keys # see all values that can be accessed/assigned
				...

			w = widget.FloatText() # Box to enter float
			w = widget.FloatSlider() # Slider for ints
			w = widget.IntRangeSlider() # Slide to a range
			w = widget.IntProgress() # Progress bar

			w = widget.ToggleButton()
			w = widget.Checkbox()

			w = widget.Dropdown()
			w = widget.RadioButtons()

			w = widget.Textarea()
			...

# MULTITHREADING

	# General notes

		In most cases everything happens in same core...
		Test it (timeit) before using it: does it really increase speed?
		Alternatives (also to bes tested)
			Cython
				C wraper for Python			
			Swig
				With Cython we have probably more flexibility to modify python wrapping functions
				Swig is a wrapper - if you have many classes to wrap, maybe it's the best option

	# General workflow

		from multiprocessing import Pool

		# Create pool of threads, map and start function, close, join
		P = 5 # number of threads
		p = Pool(P)
	    result = p.map(<callback function>, [<arguments>]) # result: list of P elements delivered by function
	    p.close()
	    p.join()

	    # Easier with with:
		P = 5 # number of threads
	    with Pool(P) as p:
		    pi = p.map(<callback function>, [<arguments>])

	# Example: Compute pi with Monte Carlo

		%%writefile pi_threads.py
		from random import random
		from multiprocessing import Pool
		import sys

		# These arguments are passed in from the command line
		N = int(sys.argv[1]) # function argument: num iterations
		P = int(sys.argv[2]) # number of processes

		# Function definition - agnostic wrt parallel execution
		def compute_pi(n):
		    '''
		    This function computes the approximation of PI
		    with n iterations using the Monte Carlo Method
		    '''
		    inside_circle = 0
		    for i in range(0,n):
		        x = random()
		        y = random()
		        if (x**2 + y**2 <= 1):
		            inside_circle += 1
		    return 4.0 * float(inside_circle) / float(n)

		# main function
		if __name__ == '__main__': 
		    '''
		    python pi_threads N P
		    N: number of iterations
		    P: number of threads
		    '''
		    # In case no arguments passed in command line
		    #N = 10000 # function argument: num iterations
		    #P = 5 # number of processes

		    # Single thread
		    pi = compute_pi(N)
		    print('Single thread pi = ', pi) # 3.14...

		    # Multiple threads: create pool, map and start, close, join
		    p = Pool(P)
		    pi = p.map(compute_pi, [N//P]*P) # function, argument list = [N//P]*P = [200, 200, 200, 200, 200]
		    p.close()
		    p.join()
		    print('Multiple thread pi[] = ', pi) # [3.14..., 3.14, 3.14, 3.14, 3.14]
		    print('Multiple thread pi = ', sum(pi)/P) # 3.14...
		    
		    # Multiple threads with with: create pool, map and start - no close nor join necessary
		    with Pool(P) as p:
		        pi = p.map(compute_pi, [N//P]*P)
		    print('Multiple thread pi[] = ', pi)
		    print('Multiple thread pi = ', sum(pi)/P) # 3.14...

----------------
STANDARD LIBRARY
----------------

For module information, try
dir(<module>)
help(<module>)

# OS

    import os
    os.getcwd()
    os.chdir('/new/path')
    os.system('mkdir today')
    os.environ.get('LD_LIBRARY_PATH') # get env variable
    os.sep
        mac/linux: '/'
        windows: '\\'
    os.path.join('folder1','folder2','file.xt')
        strings joined as if path respecting OS separator!
        unix: 'folder1/folder2/file.xt'

    import glob
    glob.glob('*.ipynb')
        # list of matching files

    import sys
    print(sys.argv)
        # list of all command-line arguments
    sys.argv[0]
        # programm call name, ...

# subprocess

    import subprocess
    # Similarly to os.system()
    # subprocess can execute a shell command, but:
    # - we need to pass each command token in a list of strings
    # - we get back the output!
    # The advantage is we can persist the output
    # as a witness of the current state

    # pip list --outdated
    outdated = subprocess.check_output(['pip', 'list','--outdated'])
    # Also:
    # outdated = subprocess.run(['pip', 'list','--outdated'], capture_output=True)
    with open('outdated.txt', 'wb') as f:
        f.write(outdated)

    # UPDATE
    # Another way of using subprocess with pipes
    # https://stackoverflow.com/questions/3503879/assign-output-of-os-system-to-a-variable-and-prevent-it-from-being-displayed-on

    from subprocess import PIPE, Popen

    def cmdline(command):
        process = Popen(
            args=command,
            stdout=PIPE,
            shell=True
        )
        return str(process.communicate()[0])

    pip_info = cmdline(f"python -m pip show {package_name} | grep Version")
    # Now, some cleaning might be required: remove \n, etc.

# Math

    import math
    math.cos(math.pi / 2.0)
    math.log(math.e)

# Random

    import random
    random.random() -> 0.5981543263453065
    random.randint(1, 10) -> 7
    random.sample(range(10),3) -> [4, 7, 9]
    random.choice(['A', 'B', 'C']) -> 'B'
    random.choice(range(10),size=int(len(range(10))*0.3),replace=False) -> 3 random unique numbers from range(10)

# Datetime

    import datetime
    t1 = datetime.datetime(2019, 1, 13)
    t2 = datetime.datetime.today()
    t3 = t2 - t1
    t3.days
    t3.seconds / (60*60)

# Collections

    # Collections: Counter, OrderedDict, defaultdict, namedtuple(), ...
    # https://docs.python.org/3/library/collections.html
    # Counter: 
    c = Counter()                           # a new, empty counter
    c = Counter('gallahad')                 # a new counter from an iterable
    c = Counter({'red': 4, 'blue': 2})      # a new counter from a mapping
    c = Counter(cats=4, dogs=8)             # a new counter from keyword args
    c = Counter(['eggs', 'ham'])
    c['bacon']                              # 0, there is no such key
    c['sausage'] = 0                        # counter entry with a zero count
    del c['sausage']                        # del actually removes the entry
    Counter('abracadabra').most_common(3)   # [('a', 5), ('b', 2), ('r', 2)]
    
    # defaultdict: if no key, it initializes it
    import collections
    d = collections.defaultdict(int)
    d[1] += 1 # even though d[1] was not created explicitly, it's initialized

# Enum

    from enum import Enum

    # class syntax
    class Color(Enum):
        RED = 1
        GREEN = 2
        BLUE = 3

    # functional syntax
    Color = Enum('Color', ['RED', 'GREEN', 'BLUE'])

----------------
Conventions (DLR)
----------------

- General Structure:
    - Author + description + docstring
    - imports: from more general to specific/own
    - in modules: __version__ = '1.2.3'; you could also add __author__ = 'MS'
    - constants: MY_CONSTANT = 'blah'
    - module-level variables (not indented)
    - functions (not indented)
    - classes (not indented)
    - if __name__ = '__main__':
- Language: ALWAYS ENGLISH
- File length: < 600 lines -> otherwise, break them down to several
- Lines: one statement per line, 120 characters -> otherwise, break them down to several with alignment
- Indentation: 4 white spaces
- Blank lines
    - Single: between 'paragraphs': methods within class
    - Double: between 'sections': between functions, classes
- White space: regular 'punctuation' - BUT: avoid around '=' INSIDE function call
- Comments: do not use when it's obvious, prefer extra line comments than inline comments
- Docstrings: use them for modules, functions, classes & methods
- Prefer dictionaries to if-else statements
- Avoid brackets in control flow statements (if, for, while)
- Exceptions:
    - inherit from Exception, don't simply catch Exception
    - don't leave 'except' empty, use at least 'Exception':
        except Exception:
- Use 'while open(.) as f:' for files/streams
- Classes
    - All member variables defined in __init__()
    - 'self' alsways first argument of each method
    - Group methods functionally
    - Docstrings
- Naming conventions
    - Packages/Modules: common, sdk, logger_utils
    - Classes: nouns, camel case, _ for internal classes
        ImageMap, _Raster, RasterDelegate
            Note: when importing, * does not select names stating with _
    - Test classes: use 'TestCase' suffix
        ImageMapTestCase, RasterTestCase
    - Exceptions: use 'Error' suffix
        _InternalError, ApplicationError
    - Memeber methods: verbs, lower case separated by _
        run, run_fast, get_variable, set_variable
        - non-public/accessible methods: _run_slow
        - non-public/accessible and non-inheritable methods: __run_slower
        - test methods: test_run
    - Member variables: like methods, but use nouns instead; note _ and __ prefix applies also
    - Functions: like methods
        - internal functions: _calculate_width
        - name conflicts: print_
    - Module variables, argument names: new_width, property_
    - Constants: delcared at the begining of class / module
        TOTAL, MY_OPERATOR

----------------
PEP 8: Style Guide
----------------

- Key idea: code is more often read than written -> improve readability
- Indentation: prefer 4 white spaces - mixed white spaces and TABs are not allowed in Python 3
- Line width: prefer 79 characters max., 72 for docstrings
- Line breaks:
    - align if possible with (), [], ...
    - logn lines can be broken wrapping expressions in parenthesis, sometimes '\' is necessary (ex: 'with')
    - it is more intuitive to break before operator (+, -, and, or, ...)
- Module functions and classes surrounded by 2 blank lines
- Class methods surrounded by 1 blank line
- Use blank lines within functions sparinly to indicate logical sections
- Language: always english
- Encoding:
    - UTF-8 (Python 3); if that encoding, avoid declaration
    - String literals and comments/docstrings must be in ASCII; use escapes otherwise
- Imports:
    - each in a separate line
    - between module globals and constants
    - grouped and in order: general to specific/own
    - prefer absolute imports
    - avoid wildcard imports: from <module> import *
- Single-quoted or double-quoted strings are the same
- White spaces: regular punctuation
    - avoid after parenthesis opening and before closing: func( a ) (wrong)
    - avoid before comma/colon/semicolon: a , b (wrong)
    - try to avoid for slicing operator unless it's visually required: [lower + offset : upper + offset] (correct)
    - do not use before parenthesis of function call: func (a) (wrong) - same applies to [] accessing
    - do not leave white spaces at the end of line
    - surround these operators (and related) with 1 white space: =, +, +=, and, not, in, >, ==, ...
    - consider doing this for displaying operator priorities: x*x + 2; (a+b) * (a-b)
    - don't surround = with space for keyword arguments or default parameters
- Control-flow statements (if, for, while) break line after :
- Trailing commas:
    - Necessary for single-element tuples: FILES = ('my_file.txt',)
    - Lists to be extended over time - BUT in this case use line breaking:
        FILES = [
                'my_file.txt',
                'other_file.txt',
                ]
- Naming conventions
    -> DLR naming conventions from above apply
    - Avoid single letter variables as l, O... which are difficult to distinguish from number
    - Prefer CamelCase to mixedCase; use mixedCase if code is already implemented like that
- For simple public variables you don't need setters/getters or the like in Python
    - BUT: attribute access (MyClass.a instead of MyClass.get_a()) assumes obtaining the attribute is cheap
- All undocumented interfaces should be considered internal
- Names of public APIs: put them in the module file after imports: __all__ = ['API1', 'API2']
- Comparisons to singletons like None must be done with 'is' or 'is not', never with '==' or '!='
- Comparisons to types or special structures should be done with 'is' or 'is not', never with '==' or '!='
    - BUT, for types, prefer the use of isinstance: isinstance(obj, int)
- When implementing ordering operations, implement all six operations: __eq__, __ne__, __lt__, __le__, __gt__, __ge__
    - Note: reflexivity rules are applied by Python: y > x can be swapped by x < y
- Use def when a lambda is binded to an identifier:
    (right) def f(x): return 2*x
    (wrong) f = lambda x: 2*x
- Exceptions
    - are inherited from Exception rather than BaseException
    - have suffix 'Error'
    - don't leave 'except' empty, use at least 'Exception':
        except Exception:
- Be consistent with return with a function:
    (wrong)
    def f(x):
        if x >= 0:
            return math.sqrt(x)
    (right)
    def f(x):
        if x >= 0:
            return math.sqrt(x)
        else:
            return None
- Don't compare boolean values to True/False

----------------
VIRTUAL ENVIRONMENTS: venv
----------------

## Introduction

    A virtual environment is an independent python installation in which we can install the package version we need for a given project.

    Two popular virtual environment managers are **conda** and **venv**.

    However, note that `conda` does two things: environment andd package management.

    One alternative to `conda` is using the two Python native tools:

    - `venv` for environment managing
    - `pip` for package managing

    However, note that `pip` handles only python packages, while `conda` is language agnostic. In fact, `conda` emerged because of the need to install data science packages such as `numpy` which are developed in another language than Python.

    We can use `pip` within `conda`, but we need to add `pip` to the creation of our environment:

    `conda create --name env_name pip`

    Otherwise, any `pip install` we do applies to the global Anaconda Python installation, not the environment we're in, even if we do `conda install pip`!

## Which one should we use?

    - Conda works well with data science projects, because packagees for that are well curated and handled; however, it is more difficult for general package development.
    - `pip` and `venv` are for general python development.

## How to deactivate a venv:

    ```bash
    deactivate
    ```

## Examples

    ```bash
    # conda
    conda create --name my_env pip
    source activate my_env
    conda install numpy
    pip istall package # installed in my_env BECAUSE pip added when creation of my_env 

    # venv
    python3 -m venv my_env # my_env folder is created locally and a python kernel installed there
    source my_env/bin/activate
    pip install numpy # pip installs locally always in venv
    # if we remove the my_env folder the virtual env is removed
    # nothing bad happens happens
    ```

----------------
INSTALLATION & USE OF ANACONDA & PYTHON
----------------

## PRELIMINARIES

	brew update
	brew install python3
	brew upgrade python3

	python --version
	python3
	python

	download Anaconda for python3
	install Anaconda
		select 'for me'
		you need 2GB

	When first time:
	source ~/.bash_profile
		Watch out: the anaconda path should be added to the ~/.bash_profile file automatically upon installation

    UPDATE: for zsh
        source /Users/mxagar/opt/anaconda3/bin/activate
        conda init zsh

## STARTERS


	conda env list
		environments from Anaconda dist
	PyPI.org
		all libs are here - search for lib also here
	anaconda
		own distribution
	conda
		installer of Anaconda
	conda create -n dlr_wiss2017_11 python=3.6
		new environment where we are going to work
		packets are installed
	conda create -n nexo python=3.6
		own environment
		all contained here, also installations we do if this env is active!
	source activate dlr_wiss2017_11
		activate environment
        new: conda activate <env>
    conda install <package name>
        dependencies are also installed!
	conda install jupyter numpy pandas matplotlib scipy sympy cython numba pytables
		conda install -c conda-forge
			channel is specified: a kind of repository - all listed in anaconda.org
            new: conda config --append channels conda-forge
		this is better than installing with pip, because compatibility is assured
		activate env properly, because they are installed there!
		no inheritance

	if you want to remove an environment
		conda env list
		source deactivate <env_name>
		conda remove --name <env_name> --all
		conda env list

	~/.condarc
		channels are here; you can specify them here
		typical libs
			- defaults
			- conda-forge

	conda list
		all things installed shown
        (in the current environment)
    conda list -n <env name>
        things installed in a specific env
    conda list -n <env name> <package name>
        info of specific package in specific env

    conda upgrade conda
    conda upgrade --all

    conda update <package name>
    conda update --all

    conda remove <package name>
        remove a specific package

    conda search *SEARCH_TERM*
        search for a package

    conda clean -tp
        clean up downloaded libs (remove tarballs, zip files, etc.)

    conda env export > environment.yaml
        export packages (and versions) in current environment
        IMPORTANT: very useful to share it on GitHub!
        Also, consider
            pip freeze > requirements.txt
    conda env create -f environment.yaml
        IMPORTANT!
        we create an environment with the same name and packages/dependencies!
        For pip:
            pip install -r requirements.txt

    conda activate myenv
    conda env update --file local.yml [--prune]
        Update an existing environment with a YAML file
        
        --prune: uninstalls dependencies which were removed from local.yml

        If there is a name tag with a name other than that of your environment in local.yml, the command above will create a new environment with that name. To avoid this:

            conda env update --name myenv --file local.yml --prune


	python3
		python console
		exit()
			exit

	ipython
		same as python, but more options visualized: auto-completion, etc.
		exit

	which jupyter
	jupyter notebook
		browser opened
		also can be started with
			python3 -m jupyter notebook

## GENERAL USE NOTES
	
	Always at the beginning activate the environment!
		source activate <my_env_name>
            new: conda activate <env>

	You can work/edit code with
		jupyter noteebook
		your editor and then run it on terminal
		spyder


## INSTALLING PACKAGES IN VENV USING PIP

    `conda install` installs packages in the virtual environment we are, but sometimes we need to use `pip install`, and we'd like to install packages only in one specific environment.

    In those cases, we can install and use `pip` in the conda environment, following these steps:

    - Run `conda create -n venv_name pip` and `conda activate venv_name`, where `venv_name` is the name of your virtual environment.
    - Run `conda install pip`. This will install pip to your venv directory.
    - Find your anaconda directory, and find the actual venv folder. It should be somewhere like `~/opt/anaconda3/envs/venv_name/`.
    - Check that the `pip` call invokes the `pip` installed in the venv with `which pip`; eventually search for the `pip` binary in the venv 
    - Install new packages by doing `~/opt/anaconda3/envs/venv_name/bin/pip install package_name`.

    Source: https://stackoverflow.com/questions/41060382/using-pip-to-install-packages-to-anaconda-environment

----------------
JUPYTER NOTEBOOK
----------------

    Literate Programming: Proposed by Donald Knuth in 1984: documentation written as a narrative alongside the code

    It started with IPython and evolved as a web-based notebook in 2014
        IPython is an interactive shell with syntax highlighting & code completion

    Architecture
        Kernel: it executes the code
            at the beginning it was IPython, not anymore, since many language-kernels are available, eg: R, Julia, ...
        Notebook server
            connected to the kernel and the browser
            it stores the notebook as a JSON with .ipynb ending
            it sends code to the kernel and receives it back
            it sends the output to the browser
        Broweser interface: the user interacts with it

        IMPORTANT NICE THING OF THIS ARCHITECTURE: we can start a server on a machine and connect with our browser to it from anywhere!
            Example: server & kernel on Amazon EC2, notebook brower on our computer

    IMPORTANT
    conda install nb_conda
        Conda tab appears
        we can now install packages, select environments when choosing a kernel

	INTALL
		pip3 install jupyter
		pip install jupyter
		... or better: install Anaconda and then
			conda install jupyter

	after selecting environment (Anaconda)...
		conda env list
		source activate <env>
            new: conda activate <env>

    run jupyter notebook
    	jupyter notebook
        server usually runs at http://localhost:8888

    JUPYTER LAB

        Nowadays, Jupyter Lab notebeook editor is more used
        To install and use it:

        pip install jupyterlab
        or    
        conda install -c conda-forge jupyterlab

        jupyter-lab

        server usually runs at http://localhost:8861/lab

	New -> select Python3 / Python / ...
		This is the kernel or python version which you are using
        We can install many language kernels

        We can also Upload files: notebooks, python scripts, ect.

	change name
	shift + enter -> EXECUTE
    alt + enter -> EXECUTE and ADD NEW CELL

	In[*]
		cell content being computed
	Out[x]
		output of previous cell
		you can reference it with _oh[x], but cell numbers might change

	you can change mode for each cell
		ESC + M -> Cell becomes Markdown
		ESC + Y -> Cell becomes Python

    L: toggle on/off line numbers

    Cmd/Ctrl+Shift+P -> Command Palette!!
        type and look for command we'd like to execute, eg merge cells

    File > Download
        we can export the notebook in many formats

	ESC + A -> insert cell above
	ESC + B -> insert cell below
	ESC + dd -> delete cell
	ESC + ii -> interrupt computation

	Help: many infos! Use:
		Help Menu
		TAB: autocompletion!
		? (front/rear): help pop-up
		*: wildcard
		
		Example
			import sys
			sys.<TAB>
			?sys
			sys?
			sys.*

	you can use for each 
		links
		videos
		other languages
			%load_ext cython
			%%<language>
			example:
				%%bash
				ls

	! -> shell commands
	res = !ls
		results stored in res

	Basic shell commands which dont need '!': complete list with %alias
		cat
		cd
		cp
		mv
		rm
		mkdir
	
	Cell -> Run All

	_ih
		input history - magic commands are shown as function calls
		_ih[1]
			content of input cell 1
	_oh
		output history
		_oh[1]
			content of output cell 1
	_
		last (previous) output
	__
		second last output


	ALIASES / MAGIC KEYWORDS
        special commands preceded by &
        they work only with Python kernels

		%%<...> -> magic commands; they apply to whole cell
			ex: run specific language for cell: HTML, ruby, latex, bash
		%<...> -> line magics; they apply to a single line in cell
			alias commands
			you can define also aliases
				Note: watch out you dont re-allocate already defined aliases/variables
		%quickref
			all aliases shown

		List of some IMPORTANT ALIASES
			%env
				environment vars in a dictionary
				remember you can store the output in a variable!
					e = %env
					for k in e.keys():
					    if 'PATH' in k:
					        print(k + ': ' + e[k])

            %time my_function()
                run already defined my_function()

            %timeit my_function()
                run already defined my_function() SEVERAL times

            %matplotlib inline
            %config InlineBackend.figure_format = 'retina'
                render figures in notebook and with high resolution

			%load xx.py
				load a file and its contents (functions)

			%pwd

			%save my_file.py _ih[1]
				save content of cell 1 into my_file.py
			%save my_file.py
			...
				save content of cell (...) into my_file.py

			%run my_file.py <args>
				like 'python my_file.py <args>'
				__name__=='__main__' must be in file to be executed
			%run -d my_file.py
				run in debugging mode
				pbd debugger is started
					you can watch variable values
					b <line number>: breakpoint
					c: continue until next breakpoint
					n: continue to next line
					s: step into a function
					q: quit debugger
			%run -t my_file.py
				run and measure time

			%debug
				debug modus
				same as run -d

            %pdb
                pdb debugging

			%xmode Plain|Context|Verbose
				Change between plain|context|verbose mode for error/debugging

			%who
				all my variables
				%whos
					more info

			%less file.csv

			%prun my_function()
				profiler: exhaustive performace information of my_function()

	Encoding
		Python 2 saves with ASCII
		Python 3 saves with UTF-8

    Latex / Formulae

    	In Markdown cell, formulae in text: This is $\pi$
        In Markdown cell, formulae in block, centered:
        $$
        \pi
        $$

	download course source code from internet
		http://www.python-academy.de/download/dlr_op_2017_11/
		'/Users/mxagar/Dropbox/PythonLab/dlr_wiss2017_11/source/scientists/ipython'

    Converting/Exporting notebooks

        File > Download as...

        Another option: use jupyter convert utlity

            pip install nbconvert
            jupyter nbconvert --to FORMAT mynotebook.ipynb
            jupyter nbconvert --to html mynotebook.ipynb
                FORMAT: 'asciidoc', 'custom', 'html', 'html_ch', 'html_embed', 'html_toc', 'html_with_lenvs', 'html_with_toclenvs', 'latex', 'latex_with_lenvs', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'selectLanguage', 'slides', 'slides_with_lenvs', 'webpdf'

    Export as Slideshow

        View > Cell toolbar > Slideshow
            This activates a dropdown in each cell, where we select the cell type
            Slide: full slides that you move through left to right
            Sub-slide: it shows up in the slideshow by pressing up or down.
            Fragment: hidden at first, then appear with a button press.
            Skip: skip cells in the slideshow
            Note: it leaves the cell as speaker notes.

        jupyter nbconvert notebook.ipynb --to slides
            notebook exported as HTML slideshow -- it can be visualized with a server

        jupyter nbconvert notebook.ipynb --to slides --post serve
            notebook exported as slideshow and visualized

    Jupyter Notebook Extensions
        
        https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/index.html

        Installation: in virtual environment
            pip install jupyter_contrib_nbextensions
            jupyter contrib nbextension install --sys-prefix
        
        Vaariable Inspector (after installation as explained above, enable)
    
            https://stackoverflow.com/questions/37718907/variable-explorer-in-jupyter-notebook
            https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/varInspector/README.html

            enable (in env): 
                jupyter nbextension enable varInspector/main

        Highlight selected word (after installation as explained above, enable)

            https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/highlight_selected_word/README.html

            enable (in env): 
                jupyter nbextension enable highlight_selected_word/main

    Additional Kernels

        Octave Kernel

            https://pypi.org/project/octave-kernel/

            brew install octave
                you need to have the octave-cli avilable in the environment variables

            conda config --add channels conda-forge
            conda install octave_kernel
            conda install texinfo # For the inline documentation (shift-tab) to appear.

        R Kernel

            brew install libgit2
            open in Terminal: R
            in R
  
                1. install packages
                    install.packages(c(
                    'repr',
                    'IRdisplay',
                    'evaluate',
                    'crayon',
                    'pbdZMQ',
                    'devtools',
                    'uuid',
                    'digest',
                    'git2r'
                    ), dependencies = TRUE)
                2. install from github
                    devtools::install_github('IRkernel/IRkernel')
                3. exit/quit
                    q() 
  
            in Terminal, launch R as sudo and execute this command:

                sudo R --slave -e 'quit("no", IRkernel::installspec(FALSE))'


----------------
JUPYTER: MAGIC COMMANDS
----------------

Display all available magic commands
%lsmagic

Available line magics: one-line commads
%alias  %alias_magic  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %popd  %pprint  %precision  %profile  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode

Available cell magics: cell becomes especial environment
%%!
%%HTML
%%SVG
%%bash
%%capture
%%debug
%%file
%%html
%%javascript
%%js
%%latex
%%markdown
%%perl
%%prun
%%pypy
%%python
%%python2
%%python3
%%ruby
%%script
%%sh
%%svg
%%sx
%%system
%%time
%%timeit
%%writefile

# Practical Examples

    Download dataset from notebook and uncompress it

        %mkdir ../data
        !wget -O ../data/dataset.tar.gz http://address/to/dataset.tar.gz
        !tar -zxf ../data/dataset.tar.gz -C ../data


----------------
NUMPY
----------------

Basic library for most common numerical operations - used by almost all other packages
Very fast, because written in C
Features: arrays and matrices, simple linear algebra
	Basically we have two classes: array and matrix and methods associated
    	np.array can be 0-D (a scalar), 1-D (row/column vectors), 2-D (matrices) or N-D (hypermatrices = tensors) and operations are element-wise
            np.arrays are ndarrays
            every item in it has same type
    	np.matrix can be 1-D (vector) or 2-D (matrix) and operations are like with regular matrices/vectors
            the main difference with 2-D arrays is that 2-D arrays have element-wise operations
    Scalars have more definition precission; we can have
        uint8, int8, uint16, int16, ...
Install
	conda install numpy
	pip install numpy
Use
	import numpy as np
If only concrete modules required
	from numpy.random import randint
	randint(1, 100)

Note: although I use [x, x, x] notation, it's really array([x, x, x])

# BASICS: definition, generation, properties

	import numpy as np
	a = np.array([[1, 2, 3], [4, 5, 6]])
	a.shape
		(2, 3) # tuple returned with (# rows, # columns)
        if we have np.array(3) (a scalar)
            it would return (): 0-D -- this is not like that with Python scalars
        if we have np.array([1,2,3])
            it would return (3,): python needs a ',' to understand it is a tuple!
        if we had np.array([[[1,1,1],[2,2,2]],[[3,3,3],[4,4,4]]])
            (2,2,3): that's a tensor
	a.dtype
		dtype('int64') # other types: np.uint64, np.float64, np.complex, np.datetime64, ...
	a.ndim
		2 # rows and columns; 3 -> hypermatrix/tensor, etc
	a.size
		6 # number of elements
	a = np.array([[1, 2, 3], [4, 5, 6]], dtype = np.float) # type specified
	a[0]
		[1., 2., 3.]
	a[0][1]
		2.0
	a[0, 1] # prefer this notation for NumPy
		2.0
	np.concatenate([a[0][:2], a[0][1:]])
		[1., 2., 2., 3.]

    Element-wise operations

        values = [1,2,3,4,5]
        values = np.array(values) + 5
            [6,7,8,9,10]
        the same for 2-D arrys or matrices: multiplying a 2-D array by itself squares its values, so it's not like matrix product!

        if we want a typical math (matrix) multiplication:
            c = np.matmul(a, b) # better use .dot()
            a = a.dot(a) # prefer this one, since matmul gave me some errors...

        NOTE: if we have a np.matrix, then the operations are like math matrices, not element-wise

	l = [[1, 2, 3], [4, 5, 6]]
	np.array(l) # lists can be casted to arrays

	np.arange(10) # equivalent to range()
		array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
	np.arange(10, 100, 10)
		array([10, 20, 30, 40, 50, 60, 70, 80, 90])
	np.arrange(16).reshape(4,4) # 4 rows, 4 columns
		array([[ 0,  1,  2,  3], [ 4,  5,  6,  7],...)

    Reshaping is very IMPORTANT
        
        v = np.array([1,2,3,4])
        v.shape
            (4,)
            note we use no (), because shape is an attribute!
        x = v.reshape(1,4)
        x.shape
            (1,4): row vector
        x = v.reshape(4,1)
        x.shape
            (4,1): column vector

        reshape works for more cases than adding a dimension 1!
        additionally, if only 1 dimension is added, an alternative to reshape is
            x = v[None, :]
                (4,) -> (1,4), all elements preceeded by one empty dimension
            x = v[:, None]
                (4,) -> (4,1), one empty dimension after all elements

        .ravel()
            equivalent to reshape(-1,order=order)
            an array of a shape (a,b,c) is converted to a 1D array of shape (a*b*c,)

	np.linspace(10, 17, 40) # from 10 to 17 in 40 elements

	np.zeros((3,4)) # 3x4 of zeros - tuple necessary!
	np.ones((3,4)) # 3x4 of ones - tuple necessary!
	np.zeros((3, 3)) + 5 # 3x3 of 5 - tuple necessary!
	np.full((3, 3), 5.0) # 3x3 of 5 - tuple necessary!
	np.eye(3) # 3x3 identity

	np.where(a > 2, 1, 0) # replace all elements of a that are > 2 with 1, other with 0
	np.where(a > 2, a, 0) # replace all elements of a that are <= 2 with 0

	r = np.random.random(5) # array of 5 random elements
	r = np.random.rand(5) # array of 5 random elements - uniform distribution
    r = np.random.rand(5,5) # 5x5 matrix with random elements; note we don't pass a tuple, unlike before!
	r = np.random.randn(5) # array of 5 random elements - gaussian/normal distribution
	r = np.random.randint(1,100,10) # array of 10 random integers from 1 to 100
	r = np.random.randint(1,100) # random integer from 1 to 100
    np.random.seed(42) # set of random numbers is fixed to value 42

    r[r > 0.5] # reduce to array of elements which are > 0.5; very fast!
	r[r > 0.5].size # how many elements are > 0.5?
	np.sum(np.where(r > 0.5, 1, 0)) # how many elements are > 0.5?
        sum() can get the param axis for 2D arrays (also for higher dim?)
        axis=0 -> sum ACROSS rows (column values are summed)
        axis=1 -> sum ACROSS cols (row values are summed)
    np.mean()
    np.min()
    np.max()

    r.dtype # type of elements in r
    r.shape # shape tuple; (X,) means row/column vector of size X
	r.max() # maximum value in array r
	r.argmax() # index location of maximum value in array r
	r.min() # minimum value in array r
    np.min(), np.max() -> they act on ndarrays and returns an ndarray, otherwise we get a list!!
	r.argmin() # index location of minimum value in array r

    np.allclose(X1,X2) # check is X1 and X2 have same values (with tolerance)

# SELECTION, SLICING & BROADCASTING
	
	Broadcasting: Arrays are class instances that have overloaded operators which extend the natural operations IFF the dimensions match somehow
		NumPy operations are usually done on pairs of arrays on an element-by-element basis
			np.array([1, 2, 3]) * np.array([1, 2, 3]) -> np.array([1, 4, 9]) 
		Broadcasting: operations with arrays of different shapes: one array is used to fit the other
			np.array([1, 2, 3]) * 2 -> np.array([2, 4, 6])
			np.array([1, 2, 3]) + 3 -> np.array([4, 5, 6])
			a[1][1:5] = 0
			...
			a = np.arange(3)
			b = np.arange(9).reshape(3, 3)
			c = np.arange(27).reshape(3, 3, 3)
			a + b + c
				a summed element-wise to each row of b and c
				b summed element-wise to each matrix of c

	Slicing: As always in Python
		a = np.arange(10)
		a[2:4] -> [2, 3] 
		a[-1] -> 9
		... all python slicing methods work
        NOTE:
            a = np.array([[1,2],[3,4]])
            a[1:2] preserves the underlying structure,
            while a[1] returns an array
                a[1] -> array([3, 4])
                a[1:2] -> array([[3, 4]]), note double []

	Slicing: Specific Ways for NumPy
		a = np.arange(25).reshape(5,5)
		a[0, 1:3] -> [1, 2], as a[0][1:3], prefer this notation

	Note: when slicing, we create pointers! (That's automatically done in NumPy to avoid memory deficiencies)
		v = b[1:-1, 1:-1]
			v is pointing to bs elements
			a change in v will change b!
		To avoid that:
			v = b[1:-1, 1:-1].copy()

	Advanced selection: when arrays passed as selection indices - advanced selection returns copies, not pointers!	
		a = np.arange(10)
		i = np.array([1, 4, 7, 7])
		a[i] -> [2, 5, 8, 8]
		a[i] = 77 -> [0, 77, 3, 4, 77, ...]
		a[i] += 77 -> [0, 78, 3, 4, 82, ...]

		a = np.arange(5)
		a > 3
			array([False, False, False, False,  True], dtype=bool)
		a[a > 3]
			array([4])
		(a < 3) & (a > 1)
			array([False, False,  True, False, False], dtype=bool)

    Special selection

        np.tril_indices_from(df)
            get indices / row-col values in a tuple of two arrays ([row_indices],[col_indices])
            of the diagonal + lower triangle

# UNIVERSAL FUNCTIONS

	Example: np.add(.)
		a = np.array([1, 2])
		b = np.array([3, 4])
		c = np.add(a, b) # returned sum
		np.add(a, b, c) # sum stored in optional argument c - ALL universal functions have this optional arg
	
	83 universal functions, grouped in
		math opretations: add, substract, multiply, log, exp, std, ...
		trigonometric: sin, cos, ...
		comparison: greater, less, equal, ...
		floating point functions: isreal, isinf, floor, ...

	All 2-args-1-result ufuncs have four extra methods: reduce, accumulate, reduceat, outer
		
		a = np.array([0, 1, 2])
		b = np.array([4, 5, 6])

		reduce: all elements reduced to a value with function
		np.add.reduce(a) -> 3
		np.add.reduce(b) -> 15
		np.multiply.reduce(a) -> 0
		np.multiply.reduce(b) -> 120

		accumulate: elements reduced to a value with function in each cell
		np.add.accumulate(a) -> array([0, 1, 3])
		np.add.accumulate(b) -> array([4, 9, 15])
		np.multiply.accumulate(a) -> array([0, 0, 0])
		np.multiply.accumulate(b) -> array([4, 20, 120])

		outer: outer-sums/products, rowxcolumn formed with array and elements operated in table
		np.multiply.outer(a)
			0 1 2
			1 1 2
			2 2 4

# MATRICES, MULTIPLICATION, ALGEBRA

	Multiplication with arrays: it occurs element-wise or broadcasting
		If you want dot products with arrays, you should use np.dot() or the operator @ (since version 3.5)
	Multiplication with matrices: regular matrix multiplication is done
		To work with algebra vectors and matrices use matrix

	M = np.array([[1, 2, 3], [4, 5, 6]])
	M = np.matrix([[1, 2, 3], [4, 5, 6]])
		equivalent to array, but multiplications are like with matrices and specific methods available
		matrices can be generated from strings too
			M = np.matrix('1, 2, 3; 4, 5, 6')

	M.I # inverse
	M.T # transpose (also of arrays) - BUT a pointer is returned! so the transpose is just another view of the data
	M.H # conjugate transpose

	A = np.matrix([[1, 2], [3, 4]])
	b = np.matrix([1, 10]) # row vector -> needs to be transposed for A*b = x equation
	np.linalg.solve(A, b.T)

	np.linalg.eig(A)
	np.linalg.det(A)
	np.linalg.inv(A)
	np.linalg.pinv(A)



----------------
MATPLOTLIB
----------------

There are several modules in it; the one to plot is pyplot (similar to Matlab)
https://matplotlib.org

Install
	conta install matplotlib
	or
	pip install matplotlib

For interactive plots in jupyter(-lab) install
    conda install -c conda-forge ipympl
    and then,
        %matplotlib notebook

In Terminal, after coding plot, run
	plt.show()
		assuming: from matplotlib import pyplot as plt
        ALSO: if we call several times plt.plot() and then
        plt.show() iit's equivalent to the 'hold on' in Matlab
In older Jupyter versions, you need to specify either
	%matplotlib inline
		plots embedded as PNGs in notebook
	%matplotlib notebook
		interactive mode: you can zoom, etc.
		sometimes you need to run 2x the commands


Look at the official matplotlib page - very interesting examples!!
There is an rc file where you can customize/configure properties (standard font, etc.)

Quick summary of most common ways of plotting
    
    x = np.arange(1,10,0.1)
    y = x**2

    1) simple
    plt.plot(x, y, color='r', lw=2, ls='-') # color hex can be used
    plt.xlabel('x label')
    plt.xlim([2,8])
    plt.ylim([3,7])
    plt.title('my plot')
    plt.legend(...)
    ...

    2) object-oriented
    fig = plt.figure(figsize=(10,8))
    ax = fig.add_axes([0, 0, 1, 1]) # left, bottom, width, height
    ax.plot(x, y, color='r', lw=2, ls='-') # linewidth, linestyle
    ax.set_xlabel('x label')
    ...

    3) object-oriented, several plots
    fig, axes = plt.subplots(1,2) # rows, cols; axes is a list!
    for ax in axes:
        ax.plot(x, y, color='r', lw=2, ls='-')
    ...

    4) several plots
    plt.subplot(1,2,1)
    plt.plot(x,y, 'r--')
    plt.subplot(1,2,2)
    plt.plot(x,y, 'g.')
    ...


# BASICS

	# Basic Example 1: 1 curve

		from matplotlib import pyplot as plt
			or: import matplotlib.pyplot as plt
		%matplotlib inline

		x = np.arange(10)
		y = x**2
		plt.plot(x, y)
		plt.xlabel('Measured')
		plt.ylabel('Calculated')
		plt.title('Measured vs. Calculated')
        #plt.show() # use it if we are not in the jupyter notebook, or to hold on

	# Basic Example 2: 2 curves + properties + text + ticks + save

		from matplotlib import pyplot as plt
		%matplotlib inline

		x = np.arange(10)
		y = x**2
		plt.plot(x, y, x, x)
		plt.xlabel('Measured')
		plt.ylabel('Calculated')
		plt.title('Measured vs. Calculated', fontname = 'CMU Bright', fontsize = 20)
		plt.legend(('square', 'linear'), loc = 'upper left') # locations: 'best', 'center', (0.1, 0.2), ...
		plt.grid(True)

		# Very important: Get Current Axis reference
		my_plot = plt.gca() # get current axis reference

		# Properties
		line_1 = my_plot.lines[0] # get object of first line/curve (x, y)
		line_2 = my_plot.lines[1] # get object of first line/curve (x, x)
		plt.setp(line_1, color = 'g', marker = 'o') # You can use also RGB Hex codes
		plt.setp(line_2, color = 'r', linestyle = '--')
		line_1.set_alpha(0.5) # transparency: also plt.setp(line_1, alpha = 0.5)

		# Text
		plt.text(1, 20, 'Data-referenced text') # Data-referenced text [min, max]
		plt.figtext(0.5, 0.5, 'Figure-referenced text') # Figure-referenced text [0, 1]

		# Ticks: Locating & Formatting: Tere are much more possibilities
		my_plot.xaxis.set_major_locator(plt.MultipleLocator(2))
		my_plot.xaxis.set_minor_locator(plt.MultipleLocator(1))
		my_plot.xaxis.set_major_formatter(plt.FormatStrFormatter('%5.2f')) # width 5, 2 comma positions, float
		my_plot.yaxis.set_major_locator(plt.MultipleLocator(10))
		my_plot.yaxis.set_minor_locator(plt.MultipleLocator(5))
		my_plot.yaxis.set_major_formatter(plt.FormatStrFormatter('%5.2f')) # width 5, 2 comma positions, float

		# Save
		plt.savefig('test_figure.png', dpi = 600)
			or
				fig = plt.figure()
				fig.savefig(...)

	# Change properties

		Three ways to set them (example: for my_line = plt.gca().lines[0])

			1) ax/plt.plot(x, y, 'r--', marker='o')
			2) plt.setp(my_line, color = 'r', linestyle = '--')
			3) .set_<property>(<value>)

		List of common properties

			color: r, g, b, c, m, y, k, w...
                we can also pass a hex color:
                google('color picker'): slide and take hex value
            linestyle (ls): -, --, :, -., steps, ...
			marker: o, +, x, <, >, ...
			antialised: True / False
			linewidth (lw): points (float)
			markersize: points (float)
			alpha: transparency [0,1]
			fontname: 'Arial', 'Courier New'
				To see all fonts
					import matplotlib.font_manager
					matplotlib.font_manager.fontManager.ttflist 
			fontsize: points (float)
			xlim([0, 1]): min x = 0, max x = 1
			ylim([0, 1]): min x = 0, max x = 1
			...

            plt.axis('equal') # maintain aspect ratio / same scale for both axes

	# Other important commands

		plt.clf() # clean plot
		plt.draw() # draw after changes done
		plt.show() # run this if we are working on Terminal or to hold on

        # Setup the dual y-axes: plot left and right different y axes
        ax1 = plt.axes()
        ax2 = ax1.twinx()
        ax1.plot(x,y1)
        ax1.plot(x,y2)
        ax1.set(ylabel='y1')
        ax2.set(ylabel='y2')
        # Combine the legends
        h1, l1 = ax1.get_legend_handles_labels()
        h2, l2 = ax2.get_legend_handles_labels()
        ax1.legend(h1+h2, l1+l2)


# OBJECT ORIENTED PLOTTING: prefer this style!!!

	You create an empty figure -> add axes -> plot in the axes

		fig = plt.figure() # empty canvas is created, now we can add axes

		axes = fig.add_axes([0, 0, 1, 1]) # canvas/figure-related sizes: left, bottom, width, height; we can go beyond 1,1 too!
		# We can add other axes on canvas! -> see subplots/plot-in-plot

		axes.plot(x,y)
		axes.set_xlabel('X Label')
		axes.set_ylabel('Y Label')
		...
	
	Plots are treated as instantiated objects
	Properties are set with
		.set_<property>(<value>)
			axes.set_title('Title')
			axes.set_xlabel('X Label')
			...

	IMPORTANT: if we want to show the figure, we just call it and it's displayed!
		fig

# SUBPLOTS

    # Example 0: set how many plots we want wherever we want

        fig = plt.figure()
        axes1 = fig.add_axes([0.1,0.1,0.8,0.8])
        axes2 = fig.add_axes([0.2,0.5,0.4,0.3])

        axes1.plot(x,y)
        axes2.plot(y,x)

	# Example 1: 1x2 grid, functional style

		plt.subplot(1,2,1)
		plt.plot(x,y, 'r--')

		plt.subplot(1,2,2)
		plt.plot(x,y, 'g.')

	# Example 2: 2x3 grid, Object Oriented style

		fig = plt.figure()
		sub1 = fig.add_subplot(2, 3, 1) # 2x3 grid, 1st plot
		sub2 = fig.add_subplot(2, 3, 2) # 2x3 grid, 2nd plot
		sub3 = fig.add_subplot(2, 3, 3)
		sub5 = fig.add_subplot(2, 3, 5)
		sub5.plot(x, y)
		plt.tight_layout() # avoid tick/label overlap

	# Example 3: plot-in-plot, Object Oriented style

		plt.plot(x, y)
		small = plt.axes([0.2, 0.25, 0.25, 0.25]) # in figure-relative coords: x_start, y_start, x_end, y_end
		small.plot(x, -y)

	# Example 4: handle plots with for loop, Object Oriented style

		fig, axes = plt.subplots(nrows = 1, ncols = 2)
        # axes is a list of 2 elements
        # we can index them: axes[0].plot(x,y)
        # or iterate them:
		for ax in axes:
    		ax.plot(x,y)
		plt.tight_layout() # avoid tick/label overlap

# FIGURE PROPERTIES: size, dpi

	# Example 1

		fig = plt.figure(figsize = (8,2), dpi = 100) # 8x2 inches, 100 dots per inch
		ax = fig.add_axes([0,0,1,1])
		ax.plot(x,y)

	# Example 2

		fig, axes = plt.subplots(figsize = (8,2), dpi = 100)
		axes.plot(x,y)

# ADVANCED OPTIONS

	# Logarithmic scale in one axis
		axes[1].set_yscale("log")

	# Latex notation in text
		r'$\pi$'

	# Set tick labels properly
		ax.set_xticks([1, 2, 3, 4, 5])
		ax.set_xticklabels([r'$\alpha$', r'$\beta$', r'$\gamma$', r'$\delta$', r'$\epsilon$'], fontsize=18)

	# Scientific notation (1e5)
		from matplotlib import ticker
		formatter = ticker.ScalarFormatter(useMathText=True)
		formatter.set_scientific(True) 
		formatter.set_powerlimits((-1,1)) 
		ax.yaxis.set_major_formatter(formatter) 

	# Grid properties
		axes[1].grid(color='b', alpha=0.5, linestyle='dashed', linewidth=0.5)

	# Twin axes face-to-face (two plots in one with different axes left-right)
		fig, ax1 = plt.subplots()

		ax1.plot(x, x**2, lw=2, color="blue")
		ax1.set_ylabel(r"area $(m^2)$", fontsize=18, color="blue")
		for label in ax1.get_yticklabels():
		    label.set_color("blue")
		    
		ax2 = ax1.twinx()
		ax2.plot(x, x**3, lw=2, color="red")
		ax2.set_ylabel(r"volume $(m^3)$", fontsize=18, color="red")
		for label in ax2.get_yticklabels():
		    label.set_color("red")

# OTHER TYPES OF PLOTS: instead of .plot()

	A better idea is probably to use Seaborn

	# Bar Charts
		plt.bar([1, 2, 3],[4, 3, 7])
	
	# Horizontal Bar Charts
		plt.barh([1, 2, 3],[4, 3, 7])

	# Boxplot
		plt.boxplot((np.arange(2, 10), np.arange(1, 5)))

	# Contour plots
		x = y = np.arange(10)
		z = np.ones((10,10))
		z[5,5] = 7
		z[2,1] = 3
		z[8,7] = 4
		plt.contour(x, x, z)
		v = x # fill colour from 0 to 9
		plt.contourf(x, x, z, v)

	# Histograms
		plt.hist(np.random.randn(100))

	# Log Plots
		plt.loglog(np.arange(1000))
		plt.grid(True, which = 'minor')

	# Pie Charts
		data = [500, 700, 300]
		labels = ['cats', 'dogs', 'others']
		plt.pie(data, labels = labels)

	# Polar Polts
		plt.polar(theta, r)

	# Arrow Plots
		plt.quiver(x, y, u, v)

	# Scatter Plots
		plt.scatter(x, y)
        # We can assign class (z) colors (c=z) to our scatterplots
        plt.scatter(x, y, c=z, cmap='rainbow')
        # If we want to colorize classes with hue,
        # we need to use sns.FacetGrid()
        # or sns.lmplot(...,fig_reg=False)

	# Sparsity Plots
		M = np.identity(10) # 2D Array/matrix
		plt.spy(M)

# CHANGE DEFAULT PROPERTIES OF PLOTS

	# Example: Change default font

		from matplotlib import rcParams
		rcParams['font.family'] = 'sans-serif'
		rcParams['font.sans-serif'] = ['Helvetica', 'Arial'] # If Helvetica not found, Arial used
		rcParams['font.size'] = 15 # If not changed explicitly, default size is 15

	# See all possible properties/parameters
		
		rcParams.keys()


    # Style sheets

        google(matplotlib style sheets)
            https://matplotlib.org/3.1.1/gallery/style_sheets/style_sheets_reference.html

        plt.style.use('ggplot')

----------------
PANDAS
----------------

Natural extension of NumPy - it builds on it.
It comes from the finance world.
Two basic data structures: Series and Data Frames.
Very good for: handling missing data & time series.
Install
	conda install pandas
	pip install pandas
Use
	import pandas as pd


# SERIES: Like an extended dictionary

	# Example 1: Built from an array/list

		import pandas as pd

	    s = pd.Series([23, -10, 46, -3, 27])
	    s
	    	# Series structure is displayed: keys (indices) and values (list)
	    s[2] -> 46 # same arithmetic as numpy arrays
	    s[s > 2]
	    s * 2
	    2 in s
        s1 + s2
            matching indices would be operated, others get NaN
            ints are converted to float
        other ways of generating series

            labels = ['a','b','c']
            my_data = [10,20,30]
            arr = np.array([10,20,30])
            d = {'a':10,'b':20,'c':30}

            # indices autmatically generated
            pd.Series(data = my_data)

            # custom indices
            pd.Series(data = my_data, index = labels)

            # series from numpy array
            pd.Series(arr)

            # If we pass a dictionary, the indices are the keys
            pd.Series(d)

            # A Series can contain any kind of objects
            # Even functions!
            pd.Series([sum, print, len])

            # We can pass data & indices without param name in order
            ser1 = pd.Series([1,2,3,4],['USA', 'Germany','USSR', 'Japan'])

            # IMPORTANT: We should use the indices/keys to access the data much faster
            ser1['USA']

	    s.index
	    	index generator is shown; indices can be especified when creating series:
		        s = pd.Series([23, -10, 46, -3, 27], index = ['a', 'b', 'c', 'd', 'e'])
	
	    s.mean()
	    s.std()
	    s.median()
	    ...
	    s.is_monotonic
	    s.is_<TAB>

    # Example 2: Built from a dictionary (usual case)

		import pandas as pd

		mountains = {'Mount Everest': 8850, 'Kilimanjaro': 5895,
	             'Vinson Massif': 4897, 'Elbrus': 5642,
	             'Puncak Jaya': 5040, 'McKinley': 6194,
	             'Aconcagua': 6960}

		index = ['Mount Everest', 'Kilimanjaro','Vinson Massif', 'Elbrus',
	             'Puncak Jaya', 'McKinley', 'Aconcagua', 'Mont Blanc']

	    ms = pd.Series(mountains, index = index) # note: Mont Blanc has no height -> NaN

	    ms.isnull()
	    ms[ms.isnull()]
	    ms + ms # indices are aligned for arithmetics!
	    ms.name = 'Mountain height'
		ms.index.name = 'Mountain'

	# REINDEXING

	    s = pd.Series([23, -10, 46, -3, 27], index = list('abcde'))
		s.reindex(list('abdcef'), method = 'ffill')
			# new series returned out of previous adding index f and setting it to value of previous index e

	# DELETING

		s.drop('d')
			# returns series without element with index 'd'
		s.drop(list('abc'))
			# returns series without indices 'a', 'b' and 'c'


# DATA FRAMES: Tables of rows & columns, or dictionaries of series

	# Basic Example

		import pandas as pd
		import datetime as dt

		data = {'name': ['Mount Everest', 'Kilimanjaro', 'Vinson Massif',
	                 'Elbrus', 'Puncak Jaya', 'McKinley', 'Aconcagua'],
	        'height': [8850, 5895, 4897, 5642, 5040, 6194, 6960],
	        'continent': ['Asia', 'Africa', 'Antarctica', 'Europe',
	                      'Oceania', 'North America', 'South America']}

        # In data frames, columns are the most important elements
        # Each column has rows
        # Both columns and rows can have indices
		m_frame = pd.DataFrame(data,
	                       columns=['name', 'height', 'continent', 'first_climbed'],
	                       index=list('ABCDEFG'))

	    # Note: no column first_climbed in data -> set to NaN
        # if index not specified, it is automatically created: 0 ... N - 1

        # IMPORTANT:
        # To access row-data, use df.loc[]
        # To access column-data, use df[]
        # To access cell values or table subsets, use df.loc[,], df.loc[[],[]]

		m_frame # Visualize table
	    m_frame.name # Column 'name' -> EACH COLUMN IS A SERIES! EACH ROW IS A SERIES TOO!
	    m_frame['name'] # Column 'name' -> PREFER THIS OVER PREVIOUS NOTATION
        m_frame[['name', [height]]] -> two columns returned - note: list of column names passed
        m_frame['new'] = m_frame['height'] + m_frame['height'] * 0.3 -> new column created!
	    m_frame.loc['C'] # All row/column values of element with index 'C'
	    m_frame.first_climbed = dt.datetime(2099, 12, 31) # All elements in column first_climbd are changed
	    m_frame.loc[m_frame.name == 'Mount Everest', 'first_climbed'] = dt.datetime(1959, 5, 29) # Cell access
	    m_frame.loc[m_frame.name != 'Mount Everest', 'first_climbed'] = pd.NaT # Several cell access; NaT = Not a Time
	    m_frame = m_frame.drop('Killimanjaro') -> remove index row/entry (axis = 0, default)
        m_frame.drop('Killimanjaro', inplace = True) -> same, but m_frame changed, no re-assignment required
            'inplace = True' is needed almost everywhere to change data - data protection mechanism
        m_frame = m_frame.drop('new', axis = 1) -> remove entire column
        m_frame.T # Transpose
	    m_frame.describe() # Statistics: mean, ...
            BUT: of the quantitative variables!
	    m_frame.info() # Storage space, ...
        m_frame.hist() # histograms of quantitative variables
	    
        m_frame.loc['A', 'height'] # Show cell / DIRECT ACCESS: row, column
	    m_frame.loc[['A','B'], ['height','continent']] # subset table/dataframe
        m_frame.loc['A', 'height'] = 8848 # Modify cell / DIRECT ACCESS: row, column
        m_frame.iloc[2] -> like loc, but numerical indices are accepted instead of labels
        
        m_frame['gender'] = m_frame['gender'].replace({0: "Female", 1: "Male"}) # replace dict keys by values
        m_frame.rename(columns={'gender': 'GEN', 'years': "AGE"}) # rename column names

        m_frame.shape
	    m_frame.columns
	    m_frame.values
            VERY IMPORTANT: dataframe is converted to numpy array!
            This is essential if we use Keras
            Not necessary if we use scikit learn
            X = df[features].values
            y = df[target].values
	    m_frame.height.values # Numpy array of column height
	    print(m_frame.to_latex()) # Print LaTeX table script
	    print(m_frame.to_html()) # Print HTML table script

        m_frame['col'].map({'val1':0,'val2':1}) # map categorical string values to numbers
            col contains val1 / val2 -> we map the values to 0 / 1

        pd.to_datetime(string) # convert a column containing strings into date-time type, eg:
            df['timeStamp'] = pd.to_datetime(df['timeStamp'])
            '2015-12-10 17:40:00' -> Timestamp('2015-12-10 17:40:00')
            and then, we could do:
                df['Hour'] = df['timeStamp'].apply(lambda time: time.hour)
                df['Month'] = df['timeStamp'].apply(lambda time: time.month)
                df['Day of Week'] = df['timeStamp'].apply(lambda time: time.dayofweek)
                dmap = {0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'}
                df['Day of Week'] = df['Day of Week'].map(dmap)

            we can also use different formats

                https://strftime.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01

                https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01

                data['DATE'] = pd.to_datetime(data['DATE'], format='%b-%y')
                data['Month'] = data['DATE'].dt.month_name().str.slice(stop=3)
                data['Year'] = data['DATE'].dt.year

        data[['City', 'Province']] = data['GEO'].str.split(',', n=1, expand=True)
            Split GEO column where ',' with n=1 split and expand toa dataframe

    # ANOTHER BASIC EXAMPLE

        from numpy.random import randn
        df = pd.DataFrame(randn(5,4), list('ABCDE'), list('WXYZ'))
            # A-B: indices, W-Z: columns, 5x4 rand matrix

	# DATA FRAME SOURCES

		Dictionaries of lists
			See above
		Dictionaries of dictionaries
			outer keys = columns
			inner keys = index
			example: yearly GDP evolution for countries
		...

	# REINDEXING EXAMPLE

		import pandas as pd

		gdp_data = {'USA': {2008: 46760, 2009: 45305, 2010: 46616,
                    2011: 48113, 2012: 49965},
            'Liechtenstein': {2008: 138537, 2009: 134617},
            'Haiti': {2008: 665, 2009: 663, 2010: 670,
                      2011: 732, 2012: 771},
            'Germany': {2008: 44132, 2009: 40275, 2010: 40164,
                        2011: 44021, 2012: 41514}
        }

        gdp = pd.DataFrame(gdp_data)

		gdp.reindex([2007, 2008, 2009])
			# 2007 index is added; method can be specified...
			# new data frame is returned only with indices 2007, 2008, 2009!

		gdp.reindex(columns=['USA', 'Germany'])
			# new data frame is returned only specified columns!
			# watch out notation

		gdp.loc[[2007, 2008, 2009], ['USA', 'Germany']]
			# Select this part of 3x2 table
			# Note: index 2007 is not in table

        gdp.reset_index()
            # index is set to be a column and a numerical indexing is created: 0, 1, 2, ...

        # How to add a new index columns
            year_from_start = [1, 2, 3, 4, 5]
            gdp['year_from_start'] =  year_from_start
                # add new column  
            gdp.reset_index(inplace = True)
                # index is set to be a column and a numerical indexing is created: 0, 1, 2, ...
            gdp.set_index('year_from_start')
                # now column 'year_from_start' is set to be index column

    # MULTI-INDEXING & INDEX HIERARCHIES

        Multi-index dataframes contain rows in rows.

        # Stack
        df_single_level_cols = pd.DataFrame([[0, 1], [2, 3]],
                                    index=['cat', 'dog'],
                                    columns=['weight', 'height'])
        df_single_level_cols
                 weight height
            cat       0      1
            dog       2      3
        df_single_level_cols.stack()
            cat  weight    0
                 height    1
            dog  weight    2
                 height    3

        # Index Levels
        outside = ['G1','G1','G1','G2','G2','G2']
        inside = [1,2,3,1,2,3]
        hier_index = list(zip(outside,inside)) # [('G1',1),('G1',2),...]
        hier_index = pd.MultiIndex.from_tuples(hier_index)

        # Two outer rows: G1, G2; 3 inner rows each: 1, 2, 3
        hier_index

        # Data (2x3x2), row-indices, column indices
        df = pd.DataFrame(randn(6,2),hier_index,['A','B'])

        # Get an outer row
        df.loc['G1']

        # Get a row for an outer row
        df.loc['G1'].loc[1]

        # Index names for each inner & outer level are empty
        df.index.names

        df.index.names = ['Groups', 'Num']

        # How to grab cell values:
        # loc for rows (also in different levels) and then ['column']
        df.loc['G2'].loc[2]['B']

        # We can also access to values with cross-section
        df.xs('G1')

        # xs() is very versatile
        # we pass: value, level name
        # and new cross-table is generated
        df.xs(1,level='Num')

        More on xs
        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.xs.html

        Crosstab
            # Cross Table: Count samples that belong to each category level pair
            x = pd.crosstab(df.Gender, da.Education)
            # Normalize: get percetanges
            # Watch out: select to normalize in column/row with axis
            x.apply(lambda z: z/z.sum(), axis=1) # axis = 1: apply to rows        


	# ACCESS, SELECTION/FILTERING, MODIFICATION, DELETING (continuing with previous df)
        
        # Display all the columns of the dataframe in the notebook
        pd.pandas.set_option('display.max_columns', None)

		gdp.drop([2010, 2011, 2012])
			# returns df without elements with specified indices

		gdp.drop(['USA', 'Germany'], axis = 1)
			# returns df without columns (axis = 1) with specified names
			# Note: notation is different than with reindex!

        gdp['Germany'] # Column Germany
        gdp[['Germany','USA']] # Two columns - note double [[]]: list passed

        gdp > 30000
            # Series with booleans returned according to condition
        gdp[gdp > 30000]
            # Conditional selection: NaN written for all cells <= 30000, other values shown
        gdp[gdp['USA'] > 46000]
            # All rows (years) in which USA had a GDP bigger than specified
        gdp[(gdp['USA'] > 46000) & (gdp['Haiti']>700)]
            # multiple conditions -> note: & used, not 'and'; () for each condition

        gdp[gdp.Liechtenstein.notnull()]
            # All entries and their columns being the index value of column Liechtenstein not null

        gdp.loc[[2008, 2009, 2011],['Germany', 'USA']]
        gdp.loc[2008:2011, 'Germany':'USA']
            # .loc allos DIRECT ACCESS to cell and its display/modification
            # note: see diferent notation: list and :

        # Query: Query the columns of a DataFrame with a boolean expression
        df.query('col1 > col2') # df[df.col1 > df.col2]

        # Replace values of a variable by a new value
        df['var'] = df['var'].replace(['val1','val2'],'val3')

        # isin()
        cities = ['Calgary', 'Toronto', 'Edmonton']
        CTE = data[data.City.isin(cities)]

        # Several conditions for selection: &, | -- not and, or!
        data[((data.Year == 1990) & (data.City == 'Vancouver')]

    # WORKING WITH DATA: read, plot, merge, process, save (all same files)

        # READ FROM FILE

            import pandas as pd

            cap = pd.read_csv('capitals.csv', sep = ';')
                # cap is automatically a df
                # 244 x 3: (index,) country, capital, population
            cap.head()
                # first entries shown
                # head(n) -> first n entries
            cap.info()
                # meta-data
            cap.describe()
            cap.tail()

            cont = pd.read_csv('continents.csv', sep = ',')
                # 205 x 2: (index,) country, continent

        # PLOT

            from matplotlib import pyplot as plt
            %matplotlib inline
            cap.plot()
                # index vs population plotted without order
            sor = cap.sort_values('population', ascending = False)
                # sort bu population
            sor.index = pd.RangeIndex(len(sor))
                # re-asign indices in order
            sor.plot()
                # index vs population plotted in order given by population

        # MERGE

            merged = pd.merge(cap, cont)
                # if not matching population, entry lost

        # PROCESS DATA: duplicates and summaries

            merged[merged.duplicated('country')]
                # show duplicate countries
            clean = merged.drop_duplicates('country')
                # df without duplicate country values returned
            clean.population.sum()
                # display population column sum

        # GROUPBY

            We group by ID value (e.g., ID value in column) and apply an aggregate function on the set of data with same ID (e.g., sum, mean, etc.).
            import pandas as pd
            
            # Create dataframe
            data = {'Company':['GOOG','GOOG','MSFT','MSFT','FB','FB'],
                   'Person':['Sam','Charlie','Amy','Vanessa','Carl','Sarah'],
                   'Sales':[200,120,340,124,243,350]}

            df = pd.DataFrame(data)

            # We always group by value in column, eg. 'Company'
            # With that, we create a groups object
            company_group = df.groupby('Company')

            # We can apply functions to groups object, eg:
            # Mean is compute for each group
            # Note that only Sales column is averaged, Persons cannot the averaged!
            
            company_group.sum()
            company_group.sum().loc['FB']

            company_group.mean()
            company_group.std()
            company_group.count()

            company_group.max()
            company_group.min()

            # very interesting
            company_group.describe()

            # Movie ratings
            # Group by title and then themean rating; sort descending
            df.groupby('title')['rating'].mean().sort_values(ascending=False)

        # CUT

            df['timezone'] = pd.cut(df.hour, [0,6,12,18,24], labels=['Night','Morning','Afternoon','Evening'])

        # CUSTOM QUERY FUNCTIONS / CUSTOM FILTERING

            grouped.apply(lambda g: g[g.population == g.population.max()])
                # g = grouped
                # filtering applied according to lambda function

            def cap5(group):
                res = group[group.population > 5e6]
                res.index = pd.RangeIndex(1, len(res) + 1) # new indices necessary after filtering
                return res

            grouped.apply(cap5)
                # filtering applied according to cap5 function

            def cap_limit(group, limit = 5e6):
                res = group[group.population > limit]
                res.index = pd.RangeIndex(1, len(res) + 1) # new indices necessary after filtering
                return res

            grouped.apply(cap_limit, limit = 2e6)
                # filtering applied according to cap_limit function (with argument)

            Apply can be used to handle information of several columns
            and to fill in missing or bad data:
                    
                Example: df['mort_acc'] has missing data, which is being replaced
                with values in look-up table total_acc_avg[]: total_acc -> mort_acc

                def fill_mort_acc(total_acc,mort_acc):
                    if np.isnan(mort_acc):
                        return total_acc_avg[total_acc]
                    else:
                        return mort_acc
                df['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'],x['mort_acc']),axis=1)

        # DUMMY VARIABLES / ONE-HOT ENCODING

            Often, categorical data needs to be converted to continuous values
            or dummy variables with one-hot encoding
                ['low','medium','high'] -> Low: 0/1, Medium: 0/1, High: 0/1

            sex_dummy = pd.get_dummies(df['Sex'])

            # However, we get one column for wach of the sexes, whereby one value predicts the other
            # That is bad...
            # To avoid that, we use the option drop_first=True
            sex_dummy = pd.get_dummies(df['Sex'],drop_first=True)

            df = pd.concat([df,sex_dummy],axis=1)

            # Summary of how to use dummy variables
            dummy_var = pd.get_dummies(df['var'],drop_first=True)
            df = pd.concat([df.drop('var',axis=1),dummy_var],axis=1)
            # Note: if we specify the columns in get_dummies, we get the concatenated df!
            # This df will contain the columns that where not specified + the dummies of the specified!
            df = pd.get_dummies(data=df, columns = ['Airline', 'Source', 'Destination'])

        # SAVE

            clean.to_csv('test.csv')
                # save CSV to file

    # MISSING DATA

        import pandas as pd
        import numpy as np

        df = pd.DataFrame({'A':[1,2,np.nan],
                  'B':[5,np.nan,np.nan],
                  'C':[1,2,3]})

        df.dropna() # remove all ROWs with at least a NaN (axis = 0, default)
        df.dropna(axis = 1) # remove all COLUMNSs with at least a NaN (axis = 1)
        df.dropna(thres = 2) # remove all ROWs with 2 or more NaNs
        df.dropna(how='all') # remove all ROWs which have NaN for all columns
        df.dropna(subset=['col1','col3']) # remove all ROWs which have NaN in col1 or col3

        df.fillna('FILL VALUE')
        df['A'].fillna(df['A'].mean())
        df['B'].fillna(df['B'].mean())
            # That's one use-case, but BE CAREFUL, YOU ARTIFICIALLY INCREASY YOUR DATA DIMENSIONALTY!

        IMPORTANT: SEE ALSO
            # CUSTOM QUERY FUNCTIONS / CUSTOM FILTERING

    # USEFUL OPERATIONS - SUMMARY

        import pandas as pd
        df = pd.DataFrame(
                {'col1':[1,2,3,4],
                 'col2':[444,555,666,444],
                 'col3':['abc','def','ghi','xyz']
                }
            )

        # Get column names of categorical data
        # Other types:
        # 'object': strings
        # 'number'
        # 'datetime'
        # 'bool'
        df.select_dtypes(['object']).columns

        # Unique values
        df['col2'].unique()
            # array of all unique values
        len(df['col2'].unique())
            # how many unique values
        df['col2'].nunique()
            # how many unique values
        df['col2'].value_counts()
            # VERY IMPORTANT, as much a .unique() and .nunique()
            # how many times does a value appear
            # result is ordered
        df['col2'].argmax()
            # location index with max value for col2

        # Conditional selection
        df[(df['col1'] > 2) & (df['col2'] == 444)]

        # We have access to useful operations on columns
        df['col2'].sum()
        df['col2'].mean()
        df['col2'].meadian()
        ...
        df['price'].quantile(0.9) # the price value which marks the percentile/quantile 90%

        # Apply: custom functions applied to cols, especially useful with lambda expressions
        df['col2'].apply(times2) # times2 is a function defined beforehand
            # def times2(x):
            #     return x**2
            # IMPORTANT: if we pass several cols, we need to specify on which axis to apply!
            # axis = 0 -> rows/indices (default), 1 -> cols
            df['col_3'] = df.apply(lambda x: f(x.col_1, x.col_2), axis=1)
        df['col2'].apply(lambda x: x**2)
        df['col3'].apply(len)
            # we can pass any function we write or standard functions

        # Remove rows (axis = 0, default) / columns (axis = 1)
        df.drop('col3', axis = 1)
        df.drop('col3', axis = 1, inplace = True)
            # 'inplace = True' modifies df itself, otherwise it's not

        # Get information on attributes
        df.columns
        df.index

        # Sorting: Sort by value in col2
        df = df.sort_values(by='col2') # sort according to col2
        df.index = pd.RangeIndex(len(df)) # re-assign new indices in order

        # Null: Is there any null?
        df.isnull()

        # Pivot tables: select title rows and columns in a hierarchical manner
        data = {'A':['foo','foo','foo','bar','bar','bar'],
                'B':['one','one','two','two','one','one'],
                'C':['x','y','x','y','x','y'],
                'D':[1,3,2,5,4,1]}

        df = pd.DataFrame(data)
        
        # We can create multi-index tables with pivot_table
        # values: the real data-points we want to have, rest are conditions
        # index: two levels
        # columns: columns
        # If no values found for certain cells, filled with NaNs
        df.pivot_table(values='D',index=['A', 'B'],columns=['C'])
            # D: real values in table
            # A, B: row titles
            # C: column titles

        # Correlations
        df[['col1','col2']].corr()
            correlation between values in col1 & col2

        correlations = df[fields].corrwith(y) # correlations with target array y
        correlations.sort_values(inplace=True)

    # ADVANCED DATA I/O: CSV, Excel, HTML, SQL

        # INTALLS

            conda install sqlalchemy
            conda install lxml
            conda install html5lib
            conda install BeautifulSoup4
            conda install xlrd

        # Numpy

            # Convert dataframe to numpy array
            X = df.to_numpy()

        # General

            pd.read_<TAB>(.) # see all possibilities
            pd.to_<TAB>(.) # see all possibilities
            Only data can be read, not formulas...

        # CSV

            df = pd.read_csv('my_file.csv')
            df.to_csv('my_file.csv')
                # optional argument: index = False (avoid saving index as column)

        # Excel

            df = pd.read_excel('my_file.xlsx', sheetname='Sheet1')
            df.to_excel('my_file.xlsx', sheet_name='NewSheet')
                # notice different notation for sheet name...

        # HTML: VERY IMPORTANT

            d = pd.read_html('https://coinmarketcap.com') # a list of found table is returned
            df = d[0] # usually first element in returned list is the df of the table
            df.shape()
            df.head()
            ...
            # NaNs are going to appear if there are images, formulas, etc. in cells

        # SQL: Requires use of specific libraries and knowledge...


    # ADVANCED: GROUPBY: It comes from SQL

        import pandas as pd
        
        data = {'Company':['GOOG','GOOG','MSFT','MSFT','FB','FB'],
                'Person':['Sam','Charlie','Amy','Vanessa','Carl','Sarah'],
                'Sales':[200,120,340,124,243,350]} 
        df = pd.DataFrame(data)

        df.groupby('Company')
            # DataFrameGroupBy created -> it can be used with summarizing functions

        df.groupby('Company').mean()
            # Example of summarizing function. Others: sum, count, min, max, std
        df.groupby('Company').describe()
            # Very IMPORTANT summarizing function

        df.groupby('Company').sum().loc['FB']
            # The result of a summarizing function can be accessed to specific Company types

    # ADVANCED: CONCATENATE, MERGE, JOIN

        pd.concat([df1, df2, df3])
            # dfs put one BELOW (axis = 0, default - ROWs) the other
            # indices don't need to match
        
        pd.concat([df1, df2, df3], axis = 1)
            # dfs put one AFTER (axis = 1, COLUMNSs) the other
            # indices don't need to match        
            # if indices do not match, outer-style is followed expanding the matrix in 2D and filling diagonal

        pd.merge(df1, df2)
            # similar merge function as in SQL
            # indices don't need to match - a key column can be selected for matching
            # many options or ways are possible
            #   - just paste left-to-right assuming same key column to match and same sizes (inner)
            #   - expand (and fill missing with NaN) to match key column indices (outer)
            # two important arguments (but much more to explore)
            #   - how = inner | outer | left | right ...
            #   - on = <common column-name> | [<common column-names>] ... 

            # We merge according to matching values on column 'key'
            pd.merge(left,right,how='inner',on='key')

        df1.join(df2)
            # similar to merge, but indices are used as key column
            # similar arguments are possible: how, ...
            # - inner join (default): intersection
            # - outer join: union

            left.join(right)
            left.join(right, how='outer')


    # ADVANCED: MULTIINDEXING: Use-case: user studies / measurements under different conditions

        outside = ['G1','G1','G1','G2','G2','G2']
        inside = [1,2,3,1,2,3]
        hier_index = list(zip(outside,inside))
        hier_index = pd.MultiIndex.from_tuples(hier_index)
        hier_index
            -> 
            MultiIndex( levels=[['G1', 'G2'], [1, 2, 3]],
                        labels=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]])
            # It's like when you're doing user studies with several treatments/conditions!
            # We have two indices: G1-2 (outer), 1-3 (inner)
        df = pd.DataFrame(np.random.randn(6,2),index=hier_index,columns=['A','B'])
        df
            -> 
                    A           B
            G1  1   0.147027    -0.479448
                2   0.558769    1.024810
                3   -0.925874   1.862864
            G2  1   -1.133817   0.610478
                2   0.386030    2.084019
                3   -0.376519   0.230336
        
        df.loc['G1']
        df.loc['G1'].loc[1]
        df.loc['G1'].loc[1,'A']

        df.index.names = ['Groups', 'Num'] # set names for indices

        df.xs('G1', level = 'Groups')
            # cross-selection: in this case, equivalent to df.loc['G1']
            # all values with Groups == 'G1'
        df.xs(1, level = 'Num')
            # cross-selection: with loc[.] it would've been much trickier, because Num is an 'inside' (not 'outer') index
            # all values with Num == 1

# VISUALIZATION

    Pandas has built-in visualization capabilities. These are based on matplotlib, therefore, we can pass matplotlib params to the plotting functions. Note that pandas visualization module is basically for exploratory tasks, not for super cool cutomizable plots. We can use matplotlib style sheets though:
        import matplotlib.pyplot as plt
        plt.style.use('ggplot')
            then, we plot with pandas

    Basically, we do
        df.plot.<kind>([args])

    import numpy as np
    import pandas as pd
    # If we import seaborn, its styles will be used
    import seaborn as sns
    %matplotlib inline

    df1 = pd.read_csv('data/df1')
    df2 = pd.read_csv('data/df2')

    # Pandas uses matplotlib to plot data
    # We can use matplotlib plot functions with their params!
    # Usually we take a column and then apply th eplotting function
    df1['A'].hist(bins=20)

    # We can also use plot.kind:
    df1['A'].plot.hist(bins=30)

    # Area plot
    df2.plot.area(alpha=0.4)

    # Bar plot
    df2.plot.bar()
    # Bar plot, stacked
    df2.plot.bar(stacked=True)

    # Line plots
    df1.plot.line(x=df1.index,y='B',figsize=(12,3),lw=1)

    # Scatter plots
    df1.plot.scatter(x='A',y='B')

    # We can specify the color to be function of a column!
    df1.plot.scatter(x='A',y='B',c='C',cmap='coolwarm')

    # We can specify the size to be function of a column!
    # Now, we need to pass the df column
    df1.plot.scatter(x='A',y='B',s=df1['C']*100)

    # Boxplots
    df2.plot.box()
    # IMPORTANT: we can get boxplot distribution data with groupby:
    # df.groupby('x')['y'].describe()


    # Bi-variate plots
    df = pd.DataFrame(np.random.randn(1000,2),columns=['a','b'])
    # Bi-variate plots: Hexplot
    df.plot.hexbin(x='a',y='b',gridsize=25,cmap='coolwarm')

    # Kernel density estimations = KDEs
    df2['a'].plot.kde()
    # The same as kde
    df2['a'].plot.density()

    # We can plot kdes of several columns too
    df2.plot.kde()

# PANDAS DATAREADER

    pip/3 install pandas-datareader

    Documentation: https://pandas-datareader.readthedocs.io/en/latest/remote_data.html

    We can access many values from different sources: Wourld Bank, Alpha Vantage, Quandl, Yahoo, etc.

    In order to get the symbols, go for example to https://finance.yahoo.com/ and look for stock or financial values, e.g., Bitcoin, Nasdaq -- each data source has the ticker/symbol in parentheses.

    from pandas_datareader import data, wb
    import pandas as pd
    import datetime

    start = datetime.datetime(2019, 1, 1)
    end = datetime.datetime(2021, 1, 1)

    # Crypto
    BTC = data.DataReader("BTC-USD", 'yahoo', start, end)
    ETH = data.DataReader("ETH-USD", 'yahoo', start, end)

    # Market composites
    SP500 = data.DataReader("^GSPC", 'yahoo', start, end)
    NASDAQ = data.DataReader("^IXIC", 'yahoo', start, end)
    IBEX = data.DataReader("^IBEX", 'yahoo', start, end)
    DAX = data.DataReader("^GDAXI", 'yahoo', start, end)

    # Forex
    EURUSD = data.DataReader("EURUSD=X", 'yahoo', start, end)

    # Commodities
    OIL = data.DataReader("CL=F", 'yahoo', start, end)
    GOLD = data.DataReader("GC=F", 'yahoo', start, end)

    # Stocks
    TESLA = data.DataReader("TSLA", 'yahoo', start, end)



----------------
SEABORN
----------------

Statistical plotting library built on top of Matplotlib, thus, often few lines are enough!
Designed to work with pandas.
Really nice visualizations, look at the gallery online:

    https://seaborn.pydata.org/examples/index.html

Install:

    conda install seaborn
    pip install seaborn

Overview
    0. Built-in datasets
    1. Distribution plots
    2. Categorical plots
    3. Matrix plots: heatmaps & clustermaps
    4. Grids
    5. Regression plots
    6. Style and color

# 0. BUILT-IN DATASETS

    import seaborn as sns
    %matplotlib inline

    # We can set our favourite style
    sns.set_style('whitegrid')

    # Seaborn comes with built-in test datasets (often pandas data frames)
    # Example: tips
    tips = sns.load_dataset('tips')

    tips.head()

# 1. DISTRIBUTION PLOTS

    IMPORTANT note:
    Kernel Desity Estimation Plots = KDEs

        https://en.wikipedia.org/wiki/Kernel_density_estimation

        Method for estimating the underlying distribution of a sampled dataset.

        Basically, a kernel function K(x) is chosen (commonly, K is the normal distrbution), and set on each of the dashes of the rug plot (density dash marks along X distrubution lines).

        Then, these kernels are summed up to obtain the estimated distribution:

        KDE = (1/nh) * sum(K((x-xi)/h))

        An important parameter is the bandwidth h or smoothing parameter: if too small, no smoothing, if too big, over-smoothened.


    # Plot uni-variate distributions as histograms & kdes
    # KDE: Kernel-density estimation: a curve is shown; we can de-activate it with kde=False
    sns.distplot(tips['total_bill']) # kde=False
    # distplot is deprecated, better use sns.displot() or sns.histplot()

    # Change number of bins
    # Switch off kde: counts shown instead of density, and curve not visualized
    sns.distplot(tips['total_bill'],bins=30,kde=False)
    # distplot is deprecated, better use sns.displot() or sns.histplot()

    # Plot bi-variate data: scetterplot + X & Y histograms + Pearson
    # x='col1', y='col2', data=DataFrame
    # Note: if we want to further distinguish classes with colors (hue), we need sns.FacetGrid()
    sns.jointplot(x='total_bill',y='tip',data=tips)

    # We can change the scatterplot to be a hex plot with kind, or other types of plots
    sns.jointplot(x='total_bill',y='tip',data=tips,kind='hex')

    # kind='reg': linear regression fit!
    sns.jointplot(x='total_bill',y='tip',data=tips,kind='reg')

    # kind='kde': distribution estimation!
    sns.jointplot(x='total_bill',y='tip',data=tips,kind='kde')

    # Pair-plots: joint plots for all numerical pair combinations in data frame
    # Very nice overview of the dataset!
    sns.pairplot(tips)

    # Pair-plots + Hue: overlap effect of categorical values!
    # We pass the name of a column that contains categorical data
    # palette: there is a set of color combinations & styles we can use
    sns.pairplot(tips,hue='sex',palette='coolwarm')

    # Rug plots: density dash marks along X distrubution lines
    sns.rugplot(tips['total_bill'])

    # KDE of a sampled dataset
    sns.kdeplot(tips['tip'])

    IMPORTANT: we can always use matplotlib commands!
        plt.title('My Plot')

# 2. CATEGORICAL PLOTS

    import seaborn as sns
    import numpy as np
    %matplotlib inline
    tips = sns.load_dataset('tips')

    # Barplot
    # x='cat1', y='continuous', data=DataFrame
    # estimator: aggregator function applied to y; default is mean
    sns.barplot(x='sex',y='total_bill',data=tips)

    # estimator: aggregator function applied to y; default is mean
    # we can pass any function as estimator aggregator, eg np.std
    # then, continuous values of categories are aggregated with the estimator
    sns.barplot(x='sex',y='total_bill',data=tips,estimator=np.std)

    # Countplot: barplot with estimator=count
    sns.countplot(x='sex',data=tips)
    # To relocate the legend
    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
    # Set order of categories:
    # we create a list length_category_order = ['length1', 'length2', ...]
    sns.countplot(x='length',data=df,order=length_category_order)

    # Boxplot
    sns.boxplot(x='day',y='total_bill',data=tips)
    # IMPORTANT: we can get boxplot distribution data with groupby:
    # df.groupby('x')['y'].describe()
    # Also IMPORTANT:
    #Tit is different if we pass data or x & y
    # - data: we can pass several quantitative variables
    # - x & y: x quatitative variable is stratified/grouped according to qualitative/categorical y
    # sns.boxplot(data = da[["VAR1","VAR2"]].dropna())

    # Boxplot with hue (2 categories/layers)
    sns.boxplot(x='day',y='total_bill',data=tips, hue='smoker')
    # IMPORTANT: we can get boxplot distribution data with groupby:
    # df.groupby('x')['y'].describe()
    # Also IMPORTANT:
    #Tit is different if we pass data or x & y
    # - data: we can pass several quantitative variables
    # - x & y: x quatitative variable is stratified/grouped according to qualitative/categorical y
    # sns.boxplot(data = da[["VAR1","VAR2"]].dropna())

    # Violin plot: fancy boxplots with density estimations
    sns.violinplot(x='day',y='total_bill',data=tips)

    # Violin plot with hue and split: very interesting!
    # Male & Female distributions plotted for each day category!
    sns.violinplot(x='day',y='total_bill',data=tips,hue='sex',split=True)

    # Strip plot: rug plot with points for different categories
    # with jitter=True we separate points horizontally with random noise for better visualization
    # we can also add hue to have layers/categories
    # hue + dodge=True separates hue layers; dodge used to t´be split
    sns.stripplot(x='day',y='total_bill',data=tips, jitter=True, hue='sex', dodge=True)

    # Swarm plot: strip plot + violin plot = points scattered following density distribution
    # Sometimes it doesn't scale for large datasets
    sns.swarmplot(x='day',y='total_bill',data=tips)

    # Combining plots, eg: violin plot + swarm plot
    sns.violinplot(x='day',y='total_bill',data=tips)
    sns.swarmplot(x='day',y='total_bill',data=tips,color='black')

    # Factor plots:
    # general interface which takes kind as argument
    # specifying which kind of plot we'd like: bar, box, etc
    sns.factorplot(x='day',y='total_bill',data=tips,kind='bar')

    sns.factorplot(x='day',y='total_bill',data=tips,kind='box')

# 3. MATRIX PLOTS: HEATMAPS & CLUSTERMAPS
    
    We have mainly 2 options:
        - Hetmaps: we pass a matrix and the value of each cell is colored
        - Clustermaps: we pass a matrix and cells are grouped by similarity + colored

    import seaborn as sns
    %matplotlib inline
    tips = sns.load_dataset('tips')
    # Number of passengers that flew on a given month of a year
    flights = sns.load_dataset('flights')

    # In order to plot matrices
    # both rows and cols must have the same meaning!
    tips.corr()

    tc = tips.corr()
    # Heatmap of correlations
    sns.heatmap(tc)

    # Heatmap with params
    # value annotations and colormap
    sns.heatmap(tc, annot=True, cmap='coolwarm', center=0.0)

    # To convert a dataset into a matrix, we often use the pivot_table function
    flights.pivot_table(index='month',columns='year',values='passengers')

    # Another option is unstack: multi-indexed table is transformed to a matrix, eg day vs hour
    dayHour = df.groupby(by=['Day of Week','Hour']).count()['Reason'].unstack()

    fp = flights.pivot_table(index='month',columns='year',values='passengers')
    sns.heatmap(fp)

    # Options
    sns.heatmap(fp,cmap='coolwarm',linecolor='white',linewidth=1,center=0.0)

    # Clustermaps: rows and cols are clustered together based on the value similatity
    # It makes sense to use a cmap like coolwarm
    # What does the option standard_scale=1 do??
    sns.clustermap(fp,cmap='coolwarm'

# 4. GRIDS

    import seaborn as sns
    from matplotlib import pyplot as plt
    %matplotlib inline
    # Iris flowers' dataset
    iris = sns.load_dataset('iris')

    # 3 species
    iris['species'].unique()

    sns.pairplot(data=iris,hue='species')

    PairGrid

        PairGrid is like pairplot but we specify which plot types should be applied.

        g = sns.PairGrid(iris) # empty plots with axes created
        # g.map(plt.scatter) # we would get a scatterplot for each cell
        g.map_diag(sns.distplot) # distplot is deprecated, better use sns.displot() or sns.histplot()
        g.map_upper(plt.scatter) # note we pass matplotlib plot type!
        g.map_lower(sns.kdeplot)


    FacetGrid

        Similar to PairGrid, buut now we basically have categories in row & cols.
        Note: plt.figure(figsize=(,)) doesnt work, we need to pass argument size=2 to FacetGrid()

        tips = sns.load_dataset('tips')

        # We saparate dataset in two factors: col='time' (Lunch, Dinner), row='smoker' (Yes, No)
        # For each 2x2 combination, we plot drictribution of 'total_bill'
        g = sns.FacetGrid(data=tips,col='time',row='smoker')
        g.map(sns.distplot,'total_bill') # distplot is deprecated, better use sns.displot() or sns.histplot()

        # We can plot anaything really, eg scatterplots, which require 2 arguments passed to map()
        g = sns.FacetGrid(data=tips,col='time',row='smoker')
        g.map(plt.scatter,'total_bill','tip')

        # We can plot scatterplots with hue
        g = sns.FacetGrid(data=df,hue='condition')
        g.map(plt.scatter,'val1','val2')


# 5. Regression Plots

    import seaborn as sns
    %matplotlib inline
    tips = sns.load_dataset('tips')

    # lm: Linear Model
    # We basically specify x,y, and optionally hue layers
    sns.lmplot(x='total_bill',y='tip',data=tips,hue='sex')

    # We can pass matplotlib parameters: markers
    sns.lmplot(x='total_bill',y='tip',data=tips,hue='sex',markers=['o','v'])

    # We can pass matplotlib parameters: each plot type has its ways, look at the docs
    # Here, we pass scatter_kws, which is a dictionary with options
    sns.lmplot(x='total_bill',y='tip',data=tips,hue='sex',markers=['o','v'],
              scatter_kws={'s':100})

    # Instead of using hue layers/factors, we can use col and have different rows & cols!
    sns.lmplot(x='total_bill',y='tip',data=tips,col='sex')

    # We can also add hue!
    sns.lmplot(x='total_bill',y='tip',data=tips,hue='sex',col='time',row='day')

    # We can also remove the regression lines and leave only the scatter plot
    # This is important if we want to plot just a scatteplot with hue
    # Another option for that would be sns.FacetGrid(), but plt.scatter() doesn't work with hue...
    sns.lmplot(...,fig_reg=False)

# 6. Style and Color

    import seaborn as sns
    %matplotlib inline
    tips = sns.load_dataset('tips')
    tips.head()

    # We can set our favourite style
    sns.set_style('whitegrid')

    # We can pass different style types
    # Shift + TAB
    # style = darkgrid, whitegrid, dark, white, ticks
    # rc: dict with overriding params for styles
    sns.set_style('ticks')
    sns.countplot(x='sex',data=tips)
    sns.despine(top=True) # we can remove spines or lines

    # Change figure size
    plt.figure(figsize=(12,3)) # we can set figure size with plt, because sns uses it!
    sns.countplot(x='sex',data=tips)

    # We can modfy sizes with set_context
    # With option font_scale, we can even control more the font scaling
    sns.set_context('poster') # notebook < paper < talk < poster
    sns.countplot(x='sex',data=tips)

    # With option font_scale, we can even control more the font scaling
    sns.set_context('poster', font_scale=2)
    sns.countplot(x='sex',data=tips)

    # Palettes and colors
    # We can pass matplotlib colormaps to palette
    # https://matplotlib.org/3.1.1/gallery/color/colormap_reference.html
    sns.set_context('notebook', font_scale=1)
    sns.lmplot(x='total_bill',y='tip',data=tips,hue='sex',palette='seismic')

    # Set context: interesting function to load
    # specific sets of parameters.
    # Contexts: 'notebook', 'talk', 'poster', etc.
    sns.set_context('talks') # thicker lines and darker colors


----------------
PLOTLY & CUFFLINKS
----------------

    Plotly is a library for interactive visualizations and dashboards for webs. Cufflinks connects Plotly with pandas.

    Installation through pip (conda not available yet):

        pip/3 install plotly
        pip/3 install cufflinks
        pip/3 install chart-studio
        pip/3 install jupyterlab "ipywidgets>=7.5"
        jupyter labextension install jupyterlab-plotly@4.14.3
        jupyter labextension install @jupyter-widgets/jupyterlab-manager plotlywidget@4.14.3

    I had to shut down Jupyter (ps -ef | grep jup -> kill) and open it again.

    Plotly is open source but it's also a company which makes money by hosting dashboards and visualizations.

    import pandas as pd
    import numpy as np
    import plotly
    import matplotlib.pyplot as plt
    %matplotlib inline
    import chart_studio.plotly as py
    import cufflinks as cf

    # Check plotly version > 1.9
    plotly.__version__

    # Import plotly-offline modules
    from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot

    # Connect Javascript to our notebook - this is for notebook development
    init_notebook_mode(connected=True)

    # In order to work offline through cufflinks, we need to run this:
    cf.go_offline()

    # DATA
    df = pd.DataFrame(np.random.randn(100,4),columns='A B C D'.split())
    df.head()

    df2 = pd.DataFrame({'Category':['A', 'B', 'C'],'Values':[32,43,50]})
    df2

    # Regular pandas plots can be done with .plot()
    df.plot()

    # To plot with plotly, we just need to execute .iplot(), that's it!
    # When we hoover on the diagram, we see we can:
    # - zoom (in/out/home)
    # - export image as PNG
    # - highlight closest data, spike lines, etc.
    # - Click ON/OFF data!
    df.iplot()

    # Other types of plots we can render
    # - Scatterplots
    # - Bar plots (often with aggregated functions)
    # - Box plots
    # - 3d surface plots
    # - Histograms (of one column or several)
    # - Spread plots
    # - Bubble plots: scatter plot with point sizes given by another column
    # - Scatter matrix = sns.pairplot()

    # Scatterplot
    df.iplot(kind='scatter',x='A',y='B',mode='markers',size=5) # mode='markers' is necessary to avoid connecting points

    # Bar plot
    df2.iplot(kind='bar',x='Category',y='Values')

    # Bar plots are interesting with aggregate functions of groupbys
    df.sum().iplot(kind='bar')

    # Box plots
    df.iplot(kind='box')

    # 3D surface plots
    df3 = pd.DataFrame({'x':[1,2,3,4,5],'y':[10,20,30,20,10],'z':[5,4,3,2,1]})
    df3.iplot(kind='surface',colorscale='rdylbu') # look at the plotly docs for colorscales

    # Histograms
    df['A'].iplot(kind='hist',bins=50)

    # Histograms of each column overlaped
    df.iplot(kind='hist',bins=50)

    # Spread plots (common in finance)
    df[['A','B']].iplot(kind='spread')

    # Bubble plots: scatter plot with point sizes given by another column
    df.iplot(kind='bubble',x='A',y='B',size='C')

    # Scatter matrix = sns.pairplot() = scatter plots of all pairs
    # All columns need to be numerical
    df.scatter_matrix()

    Choropleths and Geographical Plotting

        Matplotlib has basemap for geographical plotting, but we're going to use plotly, which has interactive plotting capabilities (while basemap produces static plots).

        We use choropleth maps, which have a difficult and weird syntax:

            - Cheat sheet here.
            - Plotly reference link: https://plotly.com/python/reference/

        Basically, we need to create a data dictionary/object and a layoutdictionary/object; and the, create the map with go.Figure() and plot it with iplot().

        IMPORTANT the dataframe tables we load must be in plotly-conform format (e.g., state names, etc.).

        Choropleth is a technical term which means 'many regions'. Note that plotly has only a reduced set of choropleths: USA-states, world, etc. If we want smaller locations within a country, we could create the JSON defining them and then the import the JSON to our script with plotly. See https://geojson.io/

        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        %matplotlib inline
        import chart_studio.plotly as py
        import cufflinks as cf
        import plotly.graph_objs as go

        # Import plotly-offline modules
        from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot

        # Connect Javascript to our notebook - this is for notebook development
        init_notebook_mode(connected=True)

        # We create a data dictionary using dict()
        # We need the key-value pairs as defined
        data = dict(type = 'choropleth',
                    locations = ['AZ','CA','NY'], # list of states/locations of locationmode
                    locationmode = 'USA-states', # look in the docu for other possibilities
                    colorscale = 'Jet', # Look in the docu for more options
                    text = ['text1','text2','text3'], # text to show for each location
                    z = [10.0,2.0,3.0], # data of each location: locations colored with data
                    colorbar = {'title':'Colorbar Title'})

        # Layout variable: nested dictionary
        layout = dict(geo = {'scope':'usa'})

        # We create the choropleth map with the two objects we created: (1) data and (2) layout
        # the dictionary is weirdly passed in a list
        choromap = go.Figure(data = [data],layout = layout)

        # Plot
        iplot(choromap)

        # If we execute plot() instead of iplot()
        # A new HTML file is created and opened in the browser - we can save it
        plot(choromap)

        Maps with real data

            # Load data
            # IMPORTANT: table must be in plotly-conform format...
            df = pd.read_csv('data/2011_US_AGRI_Exports')
            df.head()

            # We create a data dictionary using dict()
            data = dict(type = 'choropleth',
                        locations = df['code'],
                        locationmode = 'USA-states', # look in the docu for other possibilities
                        colorscale= 'ylorrd', # yellow-orange-red
                        text = df['text'], # text to show for each location
                        z = df['total exports'], # all columns shown when hoovering, total exports used for color
                        marker = dict(line = dict(color = 'rgb(255,255,255)',width=2)), # plotly notation, line between states
                        colorbar = {'title':'Millions USD'}
                       )

            # We create the layout dictionary again in plotly (weird) format
            layout = dict(title = '2011 US Agriculture Exports by State',
                          geo = dict(scope='usa',
                                     showlakes = True,
                                     lakecolor = 'rgb(85,173,240)') # grab a color tuple from the Internet
                         )

            # We create the choropleth map with the two objects we created: (1) data and (2) layout
            choromap = go.Figure(data = [data],layout = layout)

            # Plot
            iplot(choromap)

        World maps

            # Load data
            # IMPORTANT: table must be in plotly-conform format...
            df = pd.read_csv('data/2014_World_GDP')
            df.head()

            # Choropleth data dictionary
            data = dict(
                    type = 'choropleth',
                    locations = df['CODE'],
                    z = df['GDP (BILLIONS)'],
                    text = df['COUNTRY'],
                    colorbar = {'title' : 'GDP Billions US'},
                  ) 

            # Choropleth layout dictionary
            layout = dict(
                title = '2014 Global GDP',
                geo = dict(
                    showframe = False,
                    projection = {'type':'natural earth'} # mercator, ... look at the docu
                )
            )

            choromap = go.Figure(data = [data],layout = layout)
            iplot(choromap)


----------------
PLOTLY EXPRESS
----------------

https://plotly.com/python/plotly-express/

See usage examples in the IBM Machine Learning repository:
    
    ~/git_repositories/machine_learning_ibm/01_Exploratory_Data_Analysis/lab
        EDA_Lab.ipynb

        - Interactive line plots
        - Animated bar plots
        - Chloropeth: maps

----------------
FINANCE DATA EXPLORATION
----------------

Install panda's datareader, for reading directly financial and economic fata from the web:

    pip/3 install pandas-datareader

Documentation: https://pandas-datareader.readthedocs.io/en/latest/remote_data.html

We can access many values from different sources: Wourld Bank, Alpha Vantage, Quandl, Yahoo, etc.

In order to get the symbols, go for example to https://finance.yahoo.com/ and look for stock or financial values, e.g., Bitcoin, Nasdaq -- each data source has the ticker/symbol in parentheses.

    
    from pandas_datareader import data, wb
    import pandas as pd
    import numpy as np
    import datetime
    import seaborn as sns
    %matplotlib inline

    start = datetime.datetime(2019, 1, 1)
    end = datetime.datetime(2021, 1, 1)

    # Crypto
    BTC = data.DataReader("BTC-USD", 'yahoo', start, end)
    ETH = data.DataReader("ETH-USD", 'yahoo', start, end)

    # Marjet composites
    SP500 = data.DataReader("^GSPC", 'yahoo', start, end)
    NASDAQ = data.DataReader("^IXIC", 'yahoo', start, end)
    IBEX = data.DataReader("^IBEX", 'yahoo', start, end)
    DAX = data.DataReader("^GDAXI", 'yahoo', start, end)

    # Forex
    EURUSD = data.DataReader("EURUSD=X", 'yahoo', start, end)

    # Commodities
    OIL = data.DataReader("CL=F", 'yahoo', start, end)
    GOLD = data.DataReader("GC=F", 'yahoo', start, end)

    # Stocks
    TESLA = data.DataReader("TSLA", 'yahoo', start, end)

    start = datetime.datetime(2006, 1, 1)
    end = datetime.datetime(2016, 1, 1)

    # Bank of America
    BAC = data.DataReader("BAC", 'yahoo', start, end)
    # CitiGroup
    C = data.DataReader("C", 'yahoo', start, end)
    # Goldman Sachs
    GS = data.DataReader("GS", 'yahoo', start, end)
    # JPMorgan Chase
    JPM = data.DataReader("JPM", 'yahoo', start, end)
    # Morgan Stanley
    MS = data.DataReader("MS", 'yahoo', start, end)
    # Wells Fargo
    WFC = data.DataReader("WFC", 'yahoo', start, end)

    # We can create a panel object too
    df = data.DataReader(['BAC', 'C', 'GS', 'JPM', 'MS', 'WFC'],'yahoo', start, end)

    # Ticker symbols in alphabetical order
    tickers = ['BAC', 'C', 'GS', 'JPM', 'MS', 'WFC']

    # Concatenate all bank stocks
    bank_stocks = pd.concat([BAC, C, GS, JPM, MS, WFC], axis=1, keys=tickers)

    # Multi-level column heads
    bank_stocks.head()
    # Multi-level names
    bank_stocks.columns.names = ['Bank Ticker','Stock Info']

    Histograms of Returns

        # The max Close price for each bank's stock throughout the time period
        bank_stocks.xs(key='Close',axis=1,level='Stock Info').max()
        # Returns: $$r_t = \frac{p_t - p_{t-1}}{p_{t-1}} = \frac{p_t}{p_{t-1}} - 1$$

        returns = pd.DataFrame()
        # Return of the banks
        for tick in tickers:
            returns[tick+' Return'] = bank_stocks[tick]['Close'].pct_change()
        # The return percent of the first day is NaN
        returns.head()

        # Plot everything which is not NaN
        import seaborn as sns
        sns.pairplot(returns[1:].dropna())

        # Worst historical returns
        returns.min()

        # Dates of worst historical returns
        # Google what happened on that day...
        returns.idxmin()

        # Highest returns
        returns.max()

        # Dates of highest returns
        # Google what happened on that day...
        returns.idxmax()

        # Risk: it can be computed with the std. deviation
        # High std. dev., more up/down, riskier
        returns.std()

        # Slicing of column values
        returns.loc['2015-01-01':'2015-12-31'].std()

        # Histrogram of returns in a time period
        sns.histplot(returns.loc['2010-01-01':'2010-12-31']['MS Return'],
                     color='green',
                     bins=100,
                     kde=True)
        sns.histplot(returns.loc['2010-01-01':'2010-12-31']['C Return'],
                     color='red',
                     bins=100,
                     kde=True)

    Time Series

        import pandas as pd
        import numpy as np
        import plotly
        import matplotlib.pyplot as plt
        %matplotlib inline
        import chart_studio.plotly as py
        import cufflinks as cf

        # Import plotly-offline modules
        from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
        # Connect Javascript to our notebook - this is for notebook development
        init_notebook_mode(connected=True)
        # In order to work offline through cufflinks, we need to run this:
        cf.go_offline()

        # Time Series for Close prices: all banks/tickers
        for tick in tickers:
            bank_stocks[tick]['Close'].plot(figsize=(12,4),label=tick)
        plt.legend()
        # Time Series for Close prices: all banks/tickers
        # Here, we do it with cross-section: xs()
        bank_stocks.xs(key='Close',axis=1,level='Stock Info').plot(figsize=(12,4))
        # Same thing with iplot/plotly
        # We can zoom, hoover on curves and see values...
        bank_stocks.xs(key='Close',axis=1,level='Stock Info').iplot()

    Moving Averages and Correlation Heatmaps

        # Moving averages acen be computed with .rolling(window=w).mean()
        plt.figure(figsize=(12,6))
        BAC['Close'].loc['2008-01-01':'2009-01-01'].rolling(window=30).mean().plot(label='30 Day Avg')
        BAC['Close'].loc['2008-01-01':'2009-01-01'].plot(label='BAC CLOSE')
        plt.legend()

        # Heatmap of the correlations between Close prices of different banks
        corr = bank_stocks.xs(key='Close',axis=1,level='Stock Info').corr()
        corr

        # Seaborn Heatmap
        sns.heatmap(corr, cmap='coolwarm', annot=True, center=0.0)

        # Seaborn Clustermap
        sns.clustermap(corr, cmap='coolwarm', annot=True)

        # Plotly Heatmap
        corr.iplot(kind='heatmap', colorscale='rdylbu') # Red-Yellow-Blue

    Technical Analysis Charts: Moving Averages (Plotly)

        # Candlestick plots: 'Open', 'High', 'Low', 'Close': We pass columns in this order
        BAC[['Open', 'High', 'Low', 'Close']].loc['2015-01-01':'2016-01-01'].iplot(kind='candle')

        # Simple Moving Averages: These can be computed automatically with Plotly
        MS['Close'].loc['2015-01-01':'2016-01-01'].ta_plot(study='sma',
                                                           periods=[13,21,55],
                                                           title='Simple Moving Averages')

        # Bollinger Band Plot
        # Bollinger bands are +- X standard deviations of the chosen SMA
        # X is a parameters, usually chosen between 1.5 and 2.5 (e.g., 2)
        # When the bands tighten during a period of low volatility,
        # it raises the likelihood of a sharp price move in either direction.
        # When the bands separate by an unusual large amount,
        # volatility increases and any existing trend may be ending
        # Prices have a tendency to bounce within the bands' envelope
        BAC['Close'].loc['2015-01-01':'2016-01-01'].ta_plot(study='boll')

    Pandas Time Series Visualization

        import pandas as pd
        import matplotlib.pyplot as plt
        %matplotlib inline

        mcdon = pd.read_csv('data/mcdonalds.csv',index_col='Date',parse_dates=True)

        # Plot columns separately
        # If we have date index/x, we can pass xlim=[,] & ylim=(,)
        mcdon['Adj. Close'].plot(xlim=['2007-01-01','2009-01-01'],ylim=(20,60),ls='--',c='red')

        # Look for peaks
        mcdon['Adj. Volume'].plot(figsize=(10,4))

    Formatting X axis ticks for dates

        # Import dates module
        import matplotlib.dates as dates
        import pandas as pd
        import matplotlib.pyplot as plt
        %matplotlib inline

        mcdon = pd.read_csv('data/mcdonalds.csv',index_col='Date',parse_dates=True)
        idx = mcdon.loc['2007-01-01':'2007-05-01'].index
        stock = mcdon.loc['2007-01-01':'2007-05-01']['Adj. Close']

        # OPTION 1: autofmt_xdate
        fig,ax = plt.subplots()
        # we use plot_date: we specify matplotlib we're dealing with dates
        ax.plot_date(idx,stock,'-')
        # grid-lines
        ax.yaxis.grid(True)
        ax.xaxis.grid(True)
        # autoformat x axis for date data
        fig.autofmt_xdate()
        # tight layout does not always work
        plt.tight_layout()

        # OPTION 2: set_major/minor_locator
        fig,ax = plt.subplots()
        # we use plot_date: we specify matplotlib we're dealing with dates
        ax.plot_date(idx,stock,'-')
        # LOCATING: major and minor xtick values for dates
        ax.xaxis.set_major_locator(dates.MonthLocator())
        # look docu on strings; \n pushes ticks down
        ax.xaxis.set_major_formatter(dates.DateFormatter('\n\n%b-%Y'))
        ax.xaxis.set_minor_locator(dates.WeekdayLocator(byweekday=0)) # mondays
        ax.xaxis.set_minor_formatter(dates.DateFormatter('%d')) # day number
        # autoformat x axis for date data
        fig.autofmt_xdate()
        # tight layout does not always work
        plt.tight_layout()

----------------
FINANCIAL ANALYSIS with python
----------------

José Portilla, Udemy.
Python for Financial Analysis and Algorithmic Trading
https://www.udemy.com/course/python-for-finance-and-trading-algorithms/

List of sections **not** covered here:
- Intro
- Materials
- Python Crash Course
- Numpy
- General Pandas Overview
- Visualization with Matplotlib and Pandas

List of sections covered:
0. Basic Data Sources and Plotting
1. Data Sources
2. Pandas with Time Series data
3. Market Analysis Project (see notebook link)
4. Time Series Analysis
5. Python Finance Fundamentals
6. Algorithmic Trading with Quantopian

# 0. Basic Data Sources and Plotting

    See previous section: FINANCE DATA EXPLORATION

# 1. Data Sources

    We are going to use two main sources:

        - pandas-datareader: Google, Yahoo APIs accessed for free
        - Quandl API: company with free tier access available with an API key

    Note for Pandas DataReader: Yahoo and Google have updated their APIs; if they're unstable, use the codes iex, morningstar or yahoo instead (only yahoo worked for me). More on string codes:

        https://pandas-datareader.readthedocs.io/

    Optional readings

        https://www.investopedia.com/options-basics-tutorial-4583012
        https://www.investopedia.com/terms/o/order-book.asp

    ## 1.1 Pandas DataReader

        Import Stock Time Series

            import pandas_datareader.data as web
            import datetime

            # Define always start and end date to fetch data
            start = datetime.datetime(2015,1,1)
            end = datetime.datetime(2017,1,1)

            # Select stock & source
            # yahoo seems to be the only source that works
            facebook = web.DataReader('FB','yahoo',start,end)

            # High, Low, Open, Close
            # Adj Close = Close adjusted in case a stock split or similar has occurred
            facebook.head()

        Import Options

            Requesting for option time series is deprecated as of now. It would have worked as follows:

            from pandas_datareader.data import Options
            fb_options = Options('FB','yahoo')
            options_df = fb_options.get_options_data(expiry=fb_options.expiry_dates[0])
            options_df.head()


    ## 1.2 Quandl

        Quandl is a company and it charges for advanced options, but we're going to use the free tier. Many information is available. Create account and get API key from account settings. After logged in on the web, press Explore and browse free/premium datasets; we can

            - browse and filter them with the web interface
            - download them as CSVs
            - copy the python code to access them

        Some of the indicators I've found browsing Quandl

            - Stocks (= Equity, USA)
            - Real Estate (USA)
            - World Bank Data
            - Sentiment Data
            - Yale Indicators
            - ...

        Installation:

            pip/3 install quandl

        However, I had issues with the python versions, so I installed quandl through the notebook.

            !pip install quandl


        # I have problems with th epython version used in the conda environment
        # and in the jupyter notebook, which seems to be different (3.6.1 vs 3.9)
        # A quick & dirty fix consists in installing quandl from the notebook
        !pip install quandl

        import quandl
        import datetime
        import matplotlib.pyplot as plt
        %matplotlib inline

        # Load Quandl API key obtained from quandl.com and saved into a TXT file (local)
        # Make sure TXT file is only locally: add it to .gitignore to avoid uploading it!
        key_file = open('quandl_api_key.txt')
        lines = key_file.readlines()
        key = lines[0].split('\n')[0]
        quandl.ApiConfig.api_key = key

        # Define start and end date to fetch data
        # If no start & end passed, complete series is retreived
        start = datetime.datetime(2015,1,1)
        end = datetime.datetime(2017,1,1)

        # Time Series: quandl.get()
        # Check ticker/code in the official quandl.com page, while browsing datasets
        # There is also a description of the data
        # In this case: FRED US quarterly GDP
        usa_gdp = quandl.get("FRED/GDP", start_date=start, end_date=end)
        usa_gdp

        # CO2 emissions in Spain according to BP
        co2_emissions_spain = quandl.get("BP/C02_EMMISSIONS_ESP")
        co2_emissions_spain.plot()

        # Spain Employment, Millions (IMF)
        # We can also get numpy arrays instead of pandas series
        employment_spain = quandl.get("ODA/ESP_LE",returns='numpy')

        # Numpy array of tuples returned: (date, value)
        employment_spain

        # IMPORTANT CODES/TICKETS
        # "WIKI/<TICKER>": stock values of ticker = AAPL, FB, etc.
        # WIKI is the not up-to-date but free database
        apple_stock = quandl.get("WIKI/AAPL")

        # We can also get specific columns putting their indices
        apple_stock_open = quandl.get("WIKI/AAPL.1")

        # European Commission Business and Consumer Surveys
        # Looking in the documentation, many data can be obtained
        # Example: Consumer Inflation Perceptions
        ecbs_table = quandl.get("ECBCS/INF_MAL_5_MU_EU")

        # Table = Entire database: quandl.get_table()
        # More complex filtering needs to be applied
        # Look in Quandl documentation
        # https://docs.quandl.com/docs/python-tables
        data = quandl.get_table('WIKI/PRICES', qopts = { 'columns': ['ticker', 'date', 'close'] }, ticker = ['AAPL', 'MSFT'], date = { 'gte': '2016-01-01', 'lte': '2016-12-31' })

# 2. Pandas with Time Series data

    ## 2.1 DateTime Index = Datetime objects in Index

        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        %matplotlib inline

        # weird way od loading it, but it is so!
        from datetime import datetime

        my_year = 2017
        my_month = 1
        my_day = 2
        my_hour = 13
        my_minute = 30
        my_second = 15

        # y, m, d, h, m, s
        # if values not enetered, defaulted to 0
        my_date = datetime(my_year, my_month, my_day)
        my_date = datetime(my_year, my_month, my_day, my_hour, my_minute, my_second)

        # datetime.datetime is a data strcuture from which we can extract attributes
        # like day, year, etc.
        my_date.year
        my_date.month

        # We can create a list of datetime objects
        # and then convert it to an index of datetimes: DatetimeIndex
        # It is very common to have indices of type DatetimeIndex
        # and we can do special operations with them
        first_two = [datetime(2016,1,1),datetime(2016,1,2)]

        # We have a list of datetimes
        first_two

        # We now convert our list of datetimes
        # to a DatetimeIndex
        dt_ind = pd.DatetimeIndex(first_two)

        # We attach some random data to it
        # and assemble a dataframe
        data = np.random.rand(2,2)
        data

        cols = ['a', 'b']

        df = pd.DataFrame(data,dt_ind,cols)

        # Index with max value? Location of max date.
        df.index.argmax()

        # Max value among indices? Max date.
        df.index.max()

    ## 2.2 Time Resampling = GroupBy for Dates

    We usually get data that has a DateTime Index on a smaller time scale (every day, every hour, etc.), but we want to aggregate to every month/quarter, etc.

    Instead of working with groupby, we can use some resampling tools, which are more useful for time data. resample is basically like a layer over groupby; with it, we group periods of entries according to an offsetting/frequency rule (eg., yearly, quarterly, etc.), and then a function is applied on the perdiods: mean, max, ..., or a custom lambda/function.

        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        %matplotlib inline

        df = pd.read_csv('data/walmart_stock.csv')

        # When we check .info() we see that
        # the date is a string (object) -> it needs to be converted to datetime!
        df.info()

        # Convert strings of dates to datetime
        # In our case we have the standard formatting already
        # but if we had another format, we could address it with the format argument
        # look docu (shift+TAB):
        # %d/%m/%Y, %d/%m/%y, ...
        df['Date'] = pd.to_datetime(df['Date'])

        # We usually set the date as index
        df.set_index('Date',inplace=True)
        df.index

        # Another option is to set the index when reading
        # and letting the automatic date parser try its best.
        # This works when dates are in a regular format
        df = pd.read_csv('data/walmart_stock.csv', index_col='Date', parse_dates=True)

        # For datetime resampling we need a datetime index
        # When we resample, data rows are grouped according to a rule frequency/offset
        # After grouping, a function can be applied, eg: mean, max, or a custom lambda/function
        # Offset/Frequency rule aliases can be found here:
        # https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases
        # Examples
        # A, Y: year end frequency
        # Q: quarterly
        df.resample(rule='A').mean()

        # Custom function: first day values of the grouped entries,
        # eg, first instance of the period
        def first_day(entry):
            return entry[0]
        df.resample('A').apply(first_day)

        # Example application: Yearly end mean
        # We get the mean of each period ending every year end
        df['Close'].resample('A').mean().plot(kind='bar')

        # Monthly mean close prices
        df['Close'].resample('M').mean().plot(kind='bar',figsize=(16,6))

    ## 2.3 Time Shifts

    Sometimes we want to shift our data back or forwards in time; we use shift for that.

        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        %matplotlib inline

        # Load CSV
        # Parse dates and set them to be index
        df = pd.read_csv('data/walmart_stock.csv', index_col='Date', parse_dates=True)

        # If we want to shift all data one day forward: periods=1
        # First row becomes NaN, second row gets the values of the first, last row values are lost
        df.shift(periods=1).head()

        # Shift one day back
        df.shift(periods=-1).tail()

        # freq assigns the selected datetime to the index values
        # for instance: make all days of a month have the last day as index
        df.shift(freq='M').head()

        # all entries have their end-of-year index value
        df.shift(freq='A').head()

    ## 2.4 Pandas Rolling (= Moving Averages) and Expanding

    Rolling Mean = Moving Average. The rolling mean is often used to remove noise.

        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        %matplotlib inline

        # Load CSV
        # Parse dates and set them to be index
        df = pd.read_csv('data/walmart_stock.csv', index_col='Date', parse_dates=True)

        # We see the data is quite noise
        df['Open'].plot(figsize=(16,6))

        # We can smooth the data or remove noise averaging by week
        # window: entries to group (days)
        # we need to use an aggregate function: mean(), max(), etc.
        # in our example, first 7 days are NaN, because no 7-day MA can be computed for them
        df.rolling(window=7).mean().head(14)

        # Compare both original and rolled/moving-averaged data in a plot
        df['Open'].plot(figsize=(16,6))
        df.rolling(window=7).mean()['Close'].plot()
        df.rolling(window=30).mean()['Close'].plot()
        df.rolling(window=90).mean()['Close'].plot()

        # If we want to have a legend, we can create new columns
        df['Close 30 Day MA'] = df.rolling(window=30).mean()['Close']
        df[['Close','Close 30 Day MA']].plot(figsize=(16,6))

        # Expanding: each datapoint is a function of the complete history to till that point
        # The function can be chosen: min, max, mean, etc.
        # With these plots, we can see major trends
        df['Close'].expanding().mean().plot(figsize=(16,6))
        df['Close'].expanding().max().plot(figsize=(16,6))
        df['Close'].plot(figsize=(16,6))

    ## 2.5 Bollinger Bands

    Volatility is measured with the standard ddeviation of a series. Bollinger bands are volatily bands placed below and above the moving average of a series. The bands widen when the volatility increases and narrow the volatility decreases. Bollinger bands are often used as expected tops and bottoms. Bollinger bands are typically used with 20-day moving averages (I guess because 20 work days are month and stock/security exchanges close at weekends). We can use the bands to determine how big are price movements.

        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        %matplotlib inline

        # Load CSV
        # Parse dates and set them to be index
        df = pd.read_csv('data/walmart_stock.csv', index_col='Date', parse_dates=True)

        # Close 20-Day MA
        df['Close: 20-Day Mean'] = df['Close'].rolling(20).mean()
        # Upper band = 20MA + 2*std(20-day)
        df['Upper'] = df['Close: 20-Day Mean'] + 2*df['Close'].rolling(20).std()
        # Lower band = 20MA - 2*std(20-day)
        df['Lower'] = df['Close: 20-Day Mean'] - 2*df['Close'].rolling(20).std()

        # Plot everything: original, MA, Bollinger bands
        df[['Close','Close: 20-Day Mean','Upper','Lower']].tail(200).plot(figsize=(16,6))

# 3. Stock Market Analysis Project

    See notebook:
        github/mxagar/finance_accounting_courses/python_finance/03_StockMarketAnalysisExample.ipynb

# 4. Time Series Analysis

    ## 4.1 Time Series Basics

    Properties of time series:

    - Value that changes over time: (time point, value)_t
    - Trends: on average, after filtering, what is the value doing? The mean is taken for a period: Mooving Average
        - Upward
        - Horizontal/Stationary
        - Downward
    - Seasonality: repeating trend, periodic trends, related in origin to the seasons
        - Example: google trend for "snowboarding" peaks every winter
    - Cyclical trends: trends repeat but there is no fixed seasonality, ie., we cannot say when the pattern is going to repeat
        - Example: SP-500 over 20-30 years seems to have repetitive patterns upwards, but there no clear or fixed frequency or seasonality properties

    ## 4.2 Introduction to Statsmodels

    - Very popular for time series in python
    - statsmodels is hevaily inspired by R
    - Installation: conda install statsmodels
    - Very good documentation with notebook examples online
        https://www.statsmodels.org/stable/index.html
        https://www.statsmodels.org/stable/examples/index.html#time-series-analysis

    WARNING: I had some nasty issues trying to import statsmodels. I think the caus is linked to two factors:
        1. My Anaconda installation seems messed up, I probably need to take one morning free for fixing it, ie., re-installing everything...
        2. Jupyter was not installed in the pyfinance environment, thus the jupyter from another environment was being taken; that caused incompatibilities due to old versions.

    After installing jupyter in the used environment I had to fix another issue of missing library with some nasty commands, which are not really a fix, but a botcher:

        cd ~/anaconda3/envs/pyfinance/lib
        ln -is libffi.dylib libffi.6.dylib

    Conclusion: I need to uninstall Anaconda and re-install it again to fix everything.

    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    %matplotlib inline

    # That is how statsmodels is always imported
    import statsmodels.api as sm

    # We have several datasets to work with
    # We can load them with sm.datasets.TAB
    # macrodata is a dataset about USA economic data
    # .load_pandas().data yields the dataframe
    # .NOTE yields a docu string
    df = sm.datasets.macrodata.load_pandas().data

    print(sm.datasets.macrodata.NOTE)
    df.head()

    # We have quarter data from 1959 to 2009 (look at head & tail)
    # We can arrange rows with quarter end data
    # and make those dates be the index
    # statsmodels has a functionality for that, but we could do it with pandas, too
    index = pd.Index(sm.tsa.datetools.dates_from_range('1959Q1','2009Q3'))
    df.index = index

    df['realgdp'].plot()
    plt.ylabel('Real GDP')

    ### Trend and Cycle with Hodrick-Prescott

        The Hodrick-Prescott filter separates a time-series $y_t$ into a trend $\tau_t$ and a cyclical component $\zeta_t$ (and the error or residual $\epsilon_t$):

        $y_t = \tau_t + \zeta_t \,\,\,(+\,\, \epsilon_t)$

        The components are determined by minimizing the following quadratic loss function

        $\min_{\\{ \tau_{t}\\} }\sum_{t}^{T}\zeta_{t}^{2}+\lambda\sum_{t=1}^{T}\left[\left(\tau_{t}-\tau_{t-1}\right)-\left(\tau_{t-1}-\tau_{t-2}\right)\right]^{2}$

        Wikipedia: [https://en.wikipedia.org/wiki/Hodrick–Prescott_filter](https://en.wikipedia.org/wiki/Hodrick–Prescott_filter)

        # We can use the Hodrick-Prescott filter in statsmodels
        # to obtain the trend and the cyclical components
        # The tsa package stands for 'time series analysis'
        # Note that we need to pass a lambda value (see Wikipedia)
        # lamb = 1600, quarterly data (default)
        # lamb = 6.25, annual data
        # lamb = 129600, monthly data
        gdp_cycle, gdp_trend = sm.tsa.filters.hpfilter(df['realgdp'])

        df['trend'] = gdp_trend
        df['cycle'] = gdp_cycle

        df[['realgdp','trend','cycle']].plot()

        # A closer look
        df[['realgdp','trend']]['2000-03-31':].plot()

    ## 4.3 EWMA Models = Exponentally Weighted Moving Averages

    If we apply **simple moving averages** to a time series we can get the trend and seasonality components if the appropriate averaging window is chosen:
    - The larger the window, the more clear is the trend
    - For small windows redisual/error is removed and the seasonality lower frequency component is visible
    - BUT we have some problems:
        - we add a delay/lag to the filtered series (of the size of the window)
        - smaller windows will lead to more noise rather than signal
        - due to the averaging, it will never reach a full peak/valley
        - no information about the future given
        - extreme historical values skew the SMA significantly
    - The **exponentially weighted moving average** alleviates those issues!

    **Exponentially weighted moving averages (EWMA)** reduce the lag and put more weight on recent events.

    Simple moving average with window size $n$:

    $s_t (n) = \frac{1}{n} (x_{t} + x_{t-1} + \ldots + x_{t-n-1})$

    Exponential moving average with window size $n$: it is equivalent to the SMA, but we multiply the $x_{t}$ with an exponentially decaying weight $w$ (w_{i} > w_{i-1}) as we go back in time from the present. The sum of all weights is supposed to be 1, so the exponent coefficient is adjusted for matching that given the window size:

    $ s_t = \frac{\sum\limits_{i=0}^{n-1} w_i x_{t-i}}{\sum\limits_{i=0}^{n-1} w_i}$

    As shown on Wikipedia, this is equivalent to performing the following with a correctly selected factor $\alpha$ matching with the window size:

    $s_t = s_{t-1} + \alpha (x_t - s_{t-1}) $

    The factors od $\alpha$ and $w$ are equivalent and their relationship is the following:

    \begin{split}w_i = \begin{cases}
        \alpha (1 - \alpha)^i & \text{if } i < t \\
        (1 - \alpha)^i        & \text{if } i = t.
    \end{cases}\end{split}

    In practice, several equivalent single parameters can be used for describing the EWA:

    - the alpha $\alpha$ smoothing factor
    - the **span** $s$ **= window** (that is the easiest one to understand)
    - the center of mass $c$, realted to the cneter of the weight distribution
    - and the half-life $h$, the period of time for the exponential weight to reduce to one half

    They are related as follows:

    $\alpha = \frac{2}{s+1}$

    $\alpha = \frac{1}{1+c}$

    $\alpha = 1 - \exp^{\frac{\log 0.5}{h}}$

        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        %matplotlib inline

        # We need to set the index to be the time data, in this case the month
        airline = pd.read_csv('data/airline_passengers.csv',index_col='Month')

        # We need to drop all unknown/missing values
        airline.dropna(inplace=True)

        # The index needs to be converted from string to datetime
        airline.index = pd.to_datetime(airline.index)

        # Plot
        airline.plot()

        # We compute simple moving averages
        airline['6-month-SMA'] = airline['Thousands of Passengers'].rolling(window=6).mean()
        airline['12-month-SMA'] = airline['Thousands of Passengers'].rolling(window=12).mean()

        # When we plot we see that
        # 12 month SMA removed oscillations leaving a clear upward trend
        # 6 month SMA removed residual and left clear oscillations around the trend
        # Conclusions:
        # - seasonality should be due to year seasons (holidays, weather), being the period around 6 months
        # - after applying a MA beyond that period we get the real trend
        airline.plot(figsize=(16,6))

        # We compute exponentially moving averages with .ewm().mean()
        # We can pass several one-variable definitions: com, span, alpha (look at docs)
        airline['12-month-EMA'] = airline['Thousands of Passengers'].ewm(span=12).mean()

        # Plot
        # We see that we fix the issues present in SMA: lag is reduced, etc. (see above)
        airline[['Thousands of Passengers','12-month-EMA']].plot(figsize=(16,6))

    ## 4.4 ETS Models and Decomposition = Error-Trend-Seasonality

    ETS models are a family of exponential smoothing filters able to extract the trend, seasonality and error components of a time series. Given the real time series $x_t$, the exponentially smoothened series $s_t$ would be:

    $s_0 = x_0$

    $s_t = \alpha x_t + (1 - \alpha)s_{t-1}$

    We perform a **time series decomposition** (= **ETS decomposition**) to a time series in order to break down the series to those three components: **error (residual)**, **trend**, and **seasonality**. The visualization of the components is usually very informative of what's happening to he series.

    Example diagram shown: Number of passengers of arilines from 1959 to 2010. We see there is a clear upward trend and seasonality effects.

        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        %matplotlib inline

        # We need to set the index to be the time data, in this case the month
        airline = pd.read_csv('data/airline_passengers.csv',index_col='Month')

        # We need to drop all unknown/missing values
        airline.dropna(inplace=True)

        # The index needs to be converted from string to datetime
        airline.index = pd.to_datetime(airline.index)

        # We plot the curve again
        # We notice we have clear trend and seasonality components
        # We want to separate them, and additionally,
        # we want to see if the trend increases linearly o exponentially
        airline.plot()

        # We load the decompositionutility from the time series analysis package (tsa)
        from statsmodels.tsa.seasonal import seasonal_decompose

        # We can choose between additive and multiplicative models
        # additive: when trend seems to grow linearly
        # multiplicative: when trend seems to grow faster than linearly -> exponentially?
        # the result has several outputs, we can explore them with result.TAB
        # - seasonal
        # - trend
        # - resid
        # ...
        # - result.plot() plots them all!
        result = seasonal_decompose(airline['Thousands of Passengers'],model='multiplicative')

        result.plot(); # ; to avoid dual plot, which is a bug in statsmodels

        # We can also plot individdual compoenents: seasonal, trend, etc.
        result.seasonal.plot()
        result.trend.plot()

    ## 4.5 ARIMA Models = Auto-Regressive Integrated Moving Average

    ARIMA models do not work well with historical stock or similar financial data, but they are one of the most known and fudamental models for time series analysis. They do not work well on stock data because stock prices do not barely depend on time, they have some random-like component (brownian motion): maybe a price rises because larger-than-average investor bought/sold, maybe som epiece of news caused some reaction on investors, etc. Eventhough there seems to be causality, the order of causes is too high and the effect are random walks of the price. Therefore, ARIMA models, which expect time-dependent structures, are not good for stock price  modeling.

    However, they work very well with other data, such as sales predictions, etc.

    ARIMA models can get very complex.

    ARIMA models are a generalization of ARMA models
    - ARIMA = Auto-Regressive Integrated Moving Average
    - ARMA = Auto-Regressive Moving Average

    Two types of ARIMA:
    - Non-seasnal ARIMA
    - Seasonal ARIMA

    **Non-seasonal ARIMA: models denoted as ARIMA(p,d,q)**, being all p-d-q positive integers:
    - p, AR = Auto-Regression component: Regression between current observation and observations over a previous period
    - d, I = Integrated component: differencing of observations in order to make series stationary: subtracting the previous observation to the current
    - q, MA = Moving Average component: dependency between an observation and a residual error from a moving average applied to lagged observations

    **Stationary vs Non-Stationary**
    - A stationary series has a constant mean and variance over time
        - note that the the covariance must be also constant: oscillations cannot the compressed in any spot over time
    - A non-stationary series as either or both mean and variance changing over time
        - note that if a series has a non-constant covariance it is considered non-stationary
    - Usually, we do not check stationarity visually, but we perform a test, e.g., the Dickey-Fuller test with `statsmodels`
    - If we hav a non-stationary series, we need to convert it stationary in order to evaluate what type of ARIMA model to use
        - One way of doing that is differencing: to each value at t, we substract the vakue at t-1
        - If that first difference does not make the series stationary, we do more differences until data is stationary; drawback: every differencing removes a row
        - For seasonal data, we can difference by season: for month data that would be a time unit of 12: `.shift(12)`

    **Auto-Correlation Plots** (ACF) for choosing the correct `p,d,q` values:
    - Autocorrelation plot = correlogram
    - Correlation of the series with itself lagged n units:
        - x axis is the k units of lag
        - y axis is the correlation
    - Typically there are two types of autocorrelation plots:
        - Gradual decline
        - Sharp dropoff after (first) lag unit
        
    ARIMA: Chossing the components AR
    - We can take either the AR or MA components, or less frequently, both AR + MA
        - With AR and MA we set the values p (AR) and q (MA)
    - If ACP shows
        - **positive** correlation after the first lag: use **AR**
        - **negative** correlation after the first lag: use **MA**
    - Values for `p,d,q`:
        - p: The number of lag observations included in the model.
        - d: The number of times that the raw observations are differenced, also called the degree of differencing.
        - q: The size of the moving average window, also called the order of moving average.

    **Partial Auto-Correlation Functions** (PACF):
    - A partial correlation is a conditional correlation
    - Correlation between 2 variables under the assumption we know and consider another set of variables (??)
    - If we see
        - a sharpe decline after lag k, use AR-k model
        - a gradual decline, use MA model
    - In general
        - an AR model is best identified with PACF
        - an MA model is best identified with ACF
        
    There is also the seasonal ARIMA, which is equivalent, but we use capital parameters `P,D,Q`.

    **Important links on how to select `p,d,q`**:
    - [https://people.duke.edu/~rnau/arimrule.htm](https://people.duke.edu/~rnau/arimrule.htm)
    - [https://stats.stackexchange.com/questions/44992/what-are-the-values-p-d-q-in-arima](https://stats.stackexchange.com/questions/44992/what-are-the-values-p-d-q-in-arima)
    - [https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/](https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/)

    ### 4.5.1 Load and clean up the data

        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        %matplotlib inline

        import statsmodels.api as sm

        df = pd.read_csv('data/monthly-milk-production-pounds-p.csv')

        # Fix the column names
        df.columns = ['Month', 'Milk in Pounds per Cow']

        # Drop last row, because it's weird
        df.drop(168,axis=0,inplace=True)

        # Convert month column from string to datetime
        df['Month'] = pd.to_datetime(df['Month'])

        # Set the index to be the Month column
        df.set_index('Month',inplace=True)

        # Get descriptive statistics
        df.describe().transpose()

    ### 4.5.2 Visualize the time series data

        # We can see in the plot the trend upwards and the seasonality
        df.plot()

        # We can manually decompose the trend and the seasonality
        # The plot below is typical for seasonal data
        # - a 12 month rolling yields the trend
        # - std. dev. is typically much smaller than the trend and constant (with noise)
        time_series = df['Milk in Pounds per Cow']
        time_series.rolling(12).mean().plot(label='12 Month Rolling/MA Mean')
        time_series.rolling(12).std().plot(label='12 Month Rolling/MA Std. Dev.')
        time_series.plot()
        plt.legend()

        # We can also use tools from statsmodels for series decomposition
        from statsmodels.tsa.seasonal import seasonal_decompose

        # Note that we can pass more params, look with shift+TAB or in docu
        # For example: additive/multiplicative, if no pandas data we must pass period,
        # which is the number of elements in a row after which a cycle re-starts,
        # in our case: period = 12 (months)
        decomp = seasonal_decompose(time_series)

        fig = decomp.plot()
        fig.set_size_inches(16,10)

    ### 4.5.3 Test and fix the stationarity

        Our data is clearly not stationary, because we have a clear trend upwards. However, we check that mathematically with the Augmented Dickey-Fuller test.

        Null hypothesis H0: non-stationary
        Alternative hypothesis HA: stationary
        If p < 0.05 -> we reject H0 -> time series is stationary!

        from statsmodels.tsa.stattools import adfuller

        # Run the Augmented Dickey-Fuller test
        result = adfuller(df['Milk in Pounds per Cow'])

        # The result is tuple with many variables
        # we write a function to unpack it 
        result

        def adf_check(time_series):
            result = adfuller(time_series)
            print('Augmented Dickey-Fuller Test:')
            labels = ['ADF Statistic', 'p', '# of lags', '# of observations']
            for value, label in zip(result, labels):
                print(label+': '+str(value))
            if result[1] <= 0.05:
                print('Reject H0 -> data is stationary!')
            else:
                print('H0 cannot be rejected -> data is probably NOT stationary')

        adf_check(df['Milk in Pounds per Cow'])

        # Since data is probably not stationary, we perform differencing
        df['First Difference'] = df['Milk in Pounds per Cow'] - df['Milk in Pounds per Cow'].shift(1)

        # The differenced series seems quite stationary now
        df['First Difference'].plot()

        # Check again
        # Drop NA values is differencing was done
        adf_check(df['First Difference'].dropna())

        # If one differencing was not enough, we could keep differencing
        # until our transformed time series is stationary
        # For Seasonal ARIMA, we need seasonal differencing
        # which is done by differencing complete seasons
        # = number of elements for a complete cycle
        df['Seasonal Difference'] = df['Milk in Pounds per Cow'] - df['Milk in Pounds per Cow'].shift(12)

        df['Seasonal Difference'].plot()

        adf_check(df['Seasonal Difference'].dropna())

        # When the seasonal difference is not enough
        # the seasonal first difference is often computed:
        # the stationary difference differenced with the season period
        df['Seasonal First Difference'] = df['First Difference'] - df['First Difference'].shift(12)

        df['Seasonal First Difference'].plot()

        adf_check(df['Seasonal First Difference'].dropna())

    ### 4.5.4 Plot the Auto-Correlation and Partial Auto-Correlation charts

        from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

        # Auto-Correlation of first difference
        # we need to pass the total number of lags/correlations
        # which will be: max = number of data points
        fig_first = plot_acf(df['First Difference'].dropna(),lags=150)

        # Auto-Correlation of the seasonal first difference
        # We see a sudden dropoff (quite common)
        fig_first = plot_acf(df['Seasonal First Difference'].dropna(),lags=150)

        # Pandas has also a built-in auto-correlation functionality
        # but it plots a bit differently, a there's no support for partial AC,
        # therefore, better stick to statsmodels
        from pandas.plotting import autocorrelation_plot

        # Correlation values are connected with lines (not really meaningful)
        # First correlation values are sttrangely not visible
        autocorrelation_plot(df['Seasonal First Difference'].dropna())

        # Partial auto-correlation
        # We see a sudden dropoff (quite common)
        # Interpretation of PACF
        # - Sudden dropoff after a k lag -> use AR(p=k)
        # - Gradual decline -> use MA
        result = plot_pacf(df['Seasonal First Difference'].dropna())

    ### 4.5.5 Construct the ARIMA model

        # We're going to use the seasonal ARIMA
        # The ARIMA model would be imported like that
        #from statsmodels.tsa.arima_model import ARIMA
        from statsmodels.tsa.arima.model import ARIMA

        # With ACF and PACF we know the parameters p,q,d or order we need to use
        # We feed the model with the original time series and the p,d,q values
        # To see the full documentation
        #help(ARIMA)

        # We have seasonal data, so we're going to use Seasonal ARIMA
        # SARIMAX = Seasonal ARIMA with eXogenous variables
        # Exogenous variables allow taking into account external variables, 
        # which works very well for series like weather data forecasting
        model = sm.tsa.statespace.SARIMAX(
                            df['Milk in Pounds per Cow'],
                            order=(0,1,0),
                            seasonal_order=(1,1,1,12))

        results = model.fit()

        print(results.summary())

        # Residuals of the predicted data wrt original data
        results.resid.plot()

        # Kernel density estimation: residuals are around 0
        results.resid.plot(kind='kde')

        # Forecasting
        # We need to define start & end points,
        # but end is max the last point in the original series
        # If we want to predict beyonf, we need to extend the df with empty rows
        df['Forecast'] = results.predict(start=150, end=168)

        # Plotting original and forcasted data
        df[['Milk in Pounds per Cow','Forecast']].plot(figsize=(12,8))

        # Forecasting beyond the last point requires extending the df
        # We can do that orderly with the functionality DateOffset of pandas
        from pandas.tseries.offsets import DateOffset

        # We create a list comprehension which starts increasing right after the last point
        # the number of points (months in our case) we want (24 months = 2 years)
        future_dates = [df.index[-1] + DateOffset(months=x) for x in range(1,25)]

        # We create an empty df with future dates and concatenate if to the original
        future_df = pd.DataFrame(index=future_dates,columns=df.columns)
        final_df = pd.concat([df,future_df])

        # Prediction, as before
        final_df['Forecast'] = results.predict(start=168, end=168+24)

        # Plotting original and forcasted data
        final_df[['Milk in Pounds per Cow','Forecast']].plot(figsize=(12,8))

# 5. Python Finance Fundamentals

    This section is focussing on financial concepts, not new python tools
    Very interesting, though - they are practical notebooks on concepts learned from Shiller's course.
    Go to the notebooks to have a look at them.
    Summary of concepts here below:

    05_FinanceFundamentals/

        05_1_PortfolioOptimizationSharpeRatio.ipynb

            Sharpe ratio
            Portfolio allocation with ratios or weights
            Portfolio Optimization with Monte Carlo
                several weight or ratio sets tested to obtain the graph
                volatility (risk) - return - sharpe ratio
            Efficient Frontier with Markowith portfolio optimization

        05_2_FinancialTools.ipynb

            Types of Funds
            Order Books
            Latency Arbitrage or High Frequency Trading (HFT)
            Short Selling
            Stock Splits
            Dividends
            Survivorship bias
            Efficient Market Hypothesis
            Derivatives and Futures
            Some Other Terms: Leveraging, Hedging, Pair-trading

        05_3_CAPM.ipynb
    
            CAPM = Capital Assets Price Model
                Daily returns
                Volatility and risk
                Market and asset returns
                Alpha, Beta
            Hedging

        05_4_ValueCompany.ipynb

            Unlike the other sections, I created this section by browsing the Internet trying to understand how the values of stocks and companies are computed.

            


# 6. Algorithmic Trading with Quantopian

    This section is focussing on algorithmic trading, not really general python tools.
    Quantopian, its libraries (zipline and pyfolio), ans its IDE are used.
    Unfortunatelly, 

    06_AlgorithmicTrading_Quantopian_Notes.md
    

----------------
HD5
----------------


Storing format. Written in C - not python specific.

conda install pytables

HDF5 is similar to data bases, but it's better suited for large measurement data.
For data bases, maybe SQL is better; these are relational data bases.

HDF5 is like a file system inside! I can define different groups (e.g., folders) within it, for example for each different measurement.

----------------
NUMBA
----------------

Just-in-time compiler that compiles code of python.
It works with LLVM: low level virtual machine.
It is commonly as fast as C.
Numba can compile some parts of python, not everything.


----------------
CYTHON
----------------


----------------
UTILS 
----------------

    # argparse

        import argparse

        ap = argparse.ArgumentParser()
        ap.add_argument('-i', '--image', required = True, help = 'Path to image')
        args = vars(ap.parse_args())

        # dictionary containing all args
        args['image']
        ...


    # pyperclip

        pip3 install pyperclip
        conda install pyperclip

        import pyperclip
        # copy text to clipboard
        pyperclip.copy('Hello')
        # paster clipboard content (also from other prgrams)
        pyperclip.paste()


----------------
INSTALLED ENVIRONMENTS
----------------

(cv)

    Computer Vision & Co.


(ds)

    Data Science & Machine Learning

    conda create -n ds python=3.7
    
    conda install jupyter numpy pandas matplotlib scipy sympy cython numba pytables jupyterlab pip -y
    conda install scikit-learn scikit-image -y
    conda install -c pytorch pytorch -y
    conda install statsmodels -y
    conda install seaborn -y
    conda install pandas-datareader -y

    pip3 install tensorflow
    pip install tensorflow==2.6.2
    pip3 install opencv-python
    pip3/conda install ipympl

    # Plotly
    pip3 install plotly
    pip3 install cufflinks
    pip3 install chart-studio
    pip3 install jupyterlab "ipywidgets>=7.5"
    conda install -c conda-forge nodejs  -y
    conda install -c conda-forge/label/gcc7 nodejs  -y
    conda install -c conda-forge/label/cf201901 nodejs  -y 
    conda install -c conda-forge/label/cf202003 nodejs  -y
    jupyter labextension install jupyterlab-plotly
    jupyter labextension install @jupyter-widgets/jupyterlab-manager plotlywidget

    conda install -c anaconda beautifulsoup4  -y
    pip3 install openpyxl
    conda install -c anaconda openpyxl
    conda install -c conda-forge tqdm  -y
    conda install -c anaconda quandl -y

    conda install -c anaconda flask  -y
    conda install -c anaconda flask-wtf  -y
    pip install FLask-SQLAlchemy
    pip install Flask-Migrate

    pip3 install pytrends  -y
    pip3 install pycoingecko

    conda install -c conda-forge octave_kernel  -y
    conda install texinfo -y

    pip3 install gym
    pip install "gym[atari,accept-rom-license]"
    pip install pygame
    pip install pyglet
    pip install keras-rl2
        # Watch out: repository is archived

    # NLP
    conda install keras nltk
    conda install -c conda-forge spacy
    # Download dictionaries/models
    python -m spacy download en # spacy.load('en_core_news_sm')
    python -m spacy download es # spacy.load('es_core_news_sm')
    python -m spacy download de # spacy.load('de_core_news_sm')

    # Feature enginering
    conda install -c conda-forge feature_engine
    conda install -c conda-forge category_encoders

    pip install -U pytest

    # Linting
    pip install pylint
    pip install autopep8

    # MLflow, etc.
    conda install mlflow requests -c conda-forge
    conda install -c conda-forge pandas-profiling
    conda install pyarrow # for reading parquet files
    pip install protobuf==3.20

(3d)

    Robotics & Co.

    conda create -n 3d python=3.8

    conda install pip    
    conda install jupyter numpy pandas matplotlib scipy sympy cython numba pytables jupyterlab
    conda install scikit-learn scikit-image
    conda install -c pytorch pytorch
    conda install statsmodels
    conda install seaborn
    conda install pandas-datareader

    conda install -c anaconda flask
    conda install -c anaconda flask-wtf

    pip3 install opencv-python

    pip3 install open3d

    pip install -U pytest

(tf)

    I needed to create a dedicated environment for tensorflow,
    because Apple M1 systems require it for now...

    conda create -n tf tensorflow
    conda install jupyter -y
    conda install numpy pandas matplotlib scipy sympy cython numba pytables jupyterlab pip -y
    conda install scikit-learn scikit-image -y
    conda install -c pytorch pytorch -y
    conda install statsmodels -y
    conda install seaborn -y
    conda install pandas-datareader -y

    conda install -c anaconda flask -y
    conda install -c anaconda flask-wtf -y

    pip3 install opencv-python
    conda install ipympl -y

    # I needed to downgrade numpy from 1.21 to 1.19.5 because there was an issue
    # https://github.com/tensorflow/models/issues/9706
    pip3 install -U numpy==1.19.5

    pip3 install gym
    pip install "gym[atari,accept-rom-license]"
    pip install pygame
    pip install pyglet

    # NLP
    conda install keras nltk
    conda install -c conda-forge spacy
    # Download dictionaries/models
    python -m spacy download en # spacy.load('en_core_news_sm')

    # Feature enginering
    conda install -c conda-forge feature_engine
    conda install -c conda-forge category_encoders

    pip install -U pytest

(mlops-nd)

(ds)

    Data Science & Machine Learning, focus on MLOps

    conda create -n mlops-nd python=3.8
    
    conda install jupyter numpy pandas matplotlib scipy sympy cython numba pytables jupyterlab pip -y

    conda install scikit-learn scikit-image -y
    conda install -c pytorch pytorch -y
    conda install statsmodels -y
    conda install seaborn -y
    conda install pandas-datareader -y

    # DVC
    pip install -U "dvc[all]"
