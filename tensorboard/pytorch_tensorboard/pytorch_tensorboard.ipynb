{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch TensorBoard Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the following tutorials and sites:\n",
    "\n",
    "- [PyTorch TensorBoard Support](https://pytorch.org/tutorials/beginner/introyt/tensorboardyt_tutorial.html)\n",
    "- [Visualizing Models, Data, and Training with TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)\n",
    "- [Documentation: torch.utils.tensorboard](https://pytorch.org/docs/stable/tensorboard.html?highlight=summarywriter)\n",
    "- [How to use TensorBoard with PyTorch](https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-tensorboard-with-pytorch.md)\n",
    "\n",
    "Here, the [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset is used to train a CNN, while using Tensorboard to perform the following monitoring tasks:\n",
    "\n",
    "- Save sampled experiment images.\n",
    "- Log training metrics:\n",
    "  - Train and validation loss.\n",
    "  - Weight histograms\n",
    "- Visualize model graph.\n",
    "- Visualize embeddings (not working - check [`../embeddings/`](../embeddings/) instead)\n",
    "- Model evaluation: PR curves.\n",
    "- Get a python list of values logged to Tensorboard.\n",
    "\n",
    "The overall idea is to instantiate a `writer = SummaryWriter(...)` with a `logdir` and to add elements to it wherever and whenever we need, as explained in the [documentation site of torch.utils.tensorboard](https://pytorch.org/docs/stable/tensorboard.html?highlight=summarywriter):\n",
    "\n",
    "- `write.add_scalar(...)`\n",
    "- `write.add_scalars(...)`\n",
    "- `write.add_histogram(...)`\n",
    "- `write.add_image(...)`\n",
    "- `write.add_images(...)`\n",
    "- `write.add_figure(...)`\n",
    "- `write.add_video(...)`\n",
    "- `write.add_audio(...)`\n",
    "- `write.add_text(...)`\n",
    "- `write.add_graph(...)`\n",
    "- `write.add_embedding(...)`\n",
    "- `write.add_pr_curve(...)`\n",
    "- `write.add_custom_scalars(...)`\n",
    "- `write.add_mesh(...)`\n",
    "- `write.add_hparams(...)`\n",
    "\n",
    "Then, we do `write.flush()` to write to disk and finally `writer.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `tensorboard` can be launched and interactively consulted as we add data to it!\n",
    "\n",
    "```bash\n",
    "# Don't use blank spaces\n",
    "tensorboard --logdir=./\n",
    "# Open borwser at http://localhost:6006/\n",
    "# Refresh it several times until it works\n",
    "```\n",
    "\n",
    "We need to select the approproate tab for display.\n",
    "\n",
    "Also, note that the dashboard can be open during training, so that the training metrics are monitored in realtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch model and training necessities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Image datasets and image manipulation\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Image display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# In case you are using an environment that has TensorFlow installed,\n",
    "# such as Google Colab, uncomment the following code to avoid\n",
    "# a bug with saving embeddings to your TensorBoard directory\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorboard as tb\n",
    "# tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing Images in TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start by adding sample images from our dataset to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 122])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gather datasets and prepare them for consumption\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Store separate training and validations splits in ./data\n",
    "training_set = torchvision.datasets.FashionMNIST('../datasets',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "validation_set = torchvision.datasets.FashionMNIST('../datasets',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set,\n",
    "                                              batch_size=4,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=2)\n",
    "\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set,\n",
    "                                                batch_size=4,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=2)\n",
    "\n",
    "# Class labels\n",
    "classes = (\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot'\n",
    ")\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# Extract a batch of 4 images\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "print(img_grid.shape)\n",
    "matplotlib_imshow(img_grid, one_channel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default log_dir argument is \"runs\" - but it's good to be specific\n",
    "# torch.utils.tensorboard.SummaryWriter is imported above\n",
    "writer = SummaryWriter('runs/fashion_mnist_experiment_1')\n",
    "\n",
    "# Write image data to TensorBoard log dir\n",
    "writer.add_image('Four Fashion-MNIST Images', img_grid)\n",
    "# When we flush, the data is written to disk\n",
    "writer.flush()\n",
    "\n",
    "# To view, start TensorBoard on the command line with:\n",
    "#   tensorboard --logdir=runs\n",
    "# on a CLI with the propper environment active.\n",
    "# Open a browser tab to http://localhost:6006/\n",
    "# and check the IMAGES tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving weight histograms is a little bit more clumsy in Pytorch, because we need to create the functions that convert weight tensors into vectors. A possible approach is as follows (taken from the article [*How to use TensorBoard with PyTorch*](https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-tensorboard-with-pytorch.md)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_histograms_conv2d(writer, step, weights, layer_number):\n",
    "    weights_shape = weights.shape\n",
    "    num_kernels = weights_shape[0]\n",
    "    for k in range(num_kernels):\n",
    "        flattened_weights = weights[k].flatten()\n",
    "        tag = f\"layer_{layer_number}/kernel_{k}\"\n",
    "        writer.add_histogram(tag, flattened_weights, global_step=step, bins='tensorflow')\n",
    "\n",
    "\n",
    "def weight_histograms_linear(writer, step, weights, layer_number):\n",
    "    flattened_weights = weights.flatten()\n",
    "    tag = f\"layer_{layer_number}\"\n",
    "    writer.add_histogram(tag, flattened_weights, global_step=step, bins='tensorflow')\n",
    "\n",
    "\n",
    "def weight_histograms(writer, step, model):\n",
    "    # Create a list of layers\n",
    "    layers_list = [\n",
    "        model.conv1, model.pool, model.conv2,\n",
    "        model.fc1, model.fc2, model.fc3\n",
    "    ]\n",
    "\n",
    "    # Loop over all layers\n",
    "    for layer_number, layer in enumerate(layers_list):\n",
    "        # Compute weight histograms for appropriate layer\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            weights = layer.weight\n",
    "            weight_histograms_conv2d(writer, step, weights, layer_number)\n",
    "        elif isinstance(layer, nn.Linear):\n",
    "            weights = layer.weight\n",
    "            weight_histograms_linear(writer, step, weights, layer_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Log Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is useful for tracking the progress and efficacy of your training. Below, we’ll run a training loop, track some metrics, and save the data for TensorBoard’s consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000\n",
      "Batch 2000\n",
      "Batch 3000\n",
      "Batch 4000\n",
      "Batch 5000\n",
      "Batch 6000\n",
      "Batch 7000\n",
      "Batch 8000\n",
      "Batch 9000\n",
      "Batch 10000\n",
      "Batch 11000\n",
      "Batch 12000\n",
      "Batch 13000\n",
      "Batch 14000\n",
      "Batch 15000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(training_loader, 0):\n",
    "        # basic training loop\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # Every 1000 mini-batches...\n",
    "            print('Batch {}'.format(i + 1))\n",
    "            # Check against the validation set\n",
    "            running_vloss = 0.0\n",
    "\n",
    "            net.train(False) # Don't need to track gradents for validation\n",
    "            for j, vdata in enumerate(validation_loader, 0):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = net(vinputs)\n",
    "                vloss = criterion(voutputs, vlabels)\n",
    "                running_vloss += vloss.item()\n",
    "            net.train(True) # Turn gradients back on for training\n",
    "\n",
    "            avg_loss = running_loss / 1000\n",
    "            avg_vloss = running_vloss / len(validation_loader)\n",
    "\n",
    "            # Log/Visualize weight histograms\n",
    "            weight_histograms(writer,\n",
    "                              epoch * len(training_loader) + i,\n",
    "                              net)\n",
    "    \n",
    "            # Log the running loss averaged per batch\n",
    "            writer.add_scalars(\n",
    "                'Training vs. Validation Loss',\n",
    "                { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                epoch * len(training_loader) + i\n",
    "            )\n",
    "\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "\n",
    "# When we flush, the data is written to disk\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard can also be used to examine the data flow within your model. To do this, call the add_graph() method with a model and sample input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, grab a single mini-batch of images\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# add_graph() will trace the sample input through your model,\n",
    "# and render it as a graph.\n",
    "writer.add_graph(net, images)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Your Dataset with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 28-by-28 image tiles we’re using can be modeled as 784-dimensional vectors (28 * 28 = 784). It can be instructive to project this to a lower-dimensional representation. The `add_embedding()` method will project a set of data onto the three dimensions with highest variance, and display them as an interactive 3D chart.\n",
    "\n",
    "**However, this did not work in my case**. Instead, have a look at the [`../embeddings/`](../embeddings/) tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines are necessary to avoid an error\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random subset of data and corresponding labels\n",
    "def select_n_random(data, labels, n=100):\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# Extract a random subset of data\n",
    "images, labels = select_n_random(training_set.data, training_set.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[label] for label in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                     metadata=class_labels,\n",
    "                     label_img=images.unsqueeze(1))\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation: PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. gets the probability predictions in a test_size x num_classes Tensor\n",
    "# 2. gets the preds in a test_size Tensor\n",
    "# takes ~10 seconds to run\n",
    "class_probs = []\n",
    "class_label = []\n",
    "with torch.no_grad():\n",
    "    for data in validation_loader:\n",
    "        images, labels = data\n",
    "        output = net(images)\n",
    "        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
    "\n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_label.append(labels)\n",
    "\n",
    "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "test_label = torch.cat(class_label)\n",
    "\n",
    "# helper function\n",
    "def add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "    tensorboard_truth = test_label == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "\n",
    "    writer.add_pr_curve(classes[class_index],\n",
    "                        tensorboard_truth,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "    writer.close()\n",
    "\n",
    "# plot all the pr curves\n",
    "for i in range(len(classes)):\n",
    "    add_pr_curve_tensorboard(i, test_probs, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a Python List of Values Logged to Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably there is a better way than the following, but I managed to get Python objects of the logged events/artifacts as shown below. Note that we need to first look into the folder structure and print the keys of the logs. Then, once the metric is detected, we extract it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event Keys: {'images': [], 'audio': [], 'histograms': [], 'scalars': ['Training vs. Validation Loss'], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n",
      "Scalar Tags: ['Training vs. Validation Loss']\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "# Path to the TensorBoard log directory\n",
    "# NOTE that we check in the folder structure the metric we want...\n",
    "#log_dir = \"./runs/fashion_mnist_experiment_1/\"\n",
    "log_dir = './runs/fashion_mnist_experiment_1/Training vs. Validation Loss_Training/'\n",
    "#log_dir = './runs/fashion_mnist_experiment_1/Training vs. Validation Loss_Validation/'\n",
    "\n",
    "# Load the TensorBoard event files\n",
    "event_acc = event_accumulator.EventAccumulator(log_dir)\n",
    "event_acc.Reload()\n",
    "\n",
    "# Print the list of events\n",
    "print(\"Event Keys:\", event_acc.Tags())\n",
    "\n",
    "# Print the list of scalar tags\n",
    "print(\"Scalar Tags:\", event_acc.scalars.Keys()) # This output should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Values: [1.9090362787246704, 0.884652853012085, 0.7311283349990845, 0.6646563410758972, 0.6157943606376648, 0.5622122287750244, 0.5244928002357483, 0.5216445326805115, 0.5059792399406433, 0.48864346742630005, 0.4763878583908081, 0.46774783730506897, 0.44335687160491943, 0.42448800802230835, 0.42012035846710205]\n"
     ]
    }
   ],
   "source": [
    "# Get the scalar events (train and validation losses)\n",
    "_loss_events = event_acc.Scalars('Training vs. Validation Loss')\n",
    "\n",
    "# Extract the loss values as Python lists\n",
    "_loss_values = [event.value for event in _loss_events]\n",
    "\n",
    "# Print the extracted loss values\n",
    "print(\"Loss Values:\", _loss_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
