{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Lightning Tutorial with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the [starter tutorial from the official documentation](https://lightning.ai/docs/pytorch/stable/starter/introduction.html), with some added features. In the notebook, an Autoencoder is implemented for the MNIST dataset.\n",
    "\n",
    "Pytorch Lightning uses Pytorch under the hood; however, the usual Pytorch boilerplate code is abstracted to a cleaner implementation similar to Keras. In that implementation, the code is structured in two major classes:\n",
    "\n",
    "- `LightningModule`: this class takes the Pytorch model and expects the definition of some functions, such as:\n",
    "  - `training_step()`\n",
    "  - `validation_step()`\n",
    "  - `test_step()`\n",
    "  - `configure_optimizers()`, which returns the optimizer and the scheduler\n",
    "  - `forward()`\n",
    "  - `predict_step()`\n",
    "- `Trainer`: this class takes the `LightningModule` and the datasets in the form of `Dataloaders` and is able to run these methods:\n",
    "  - `fit()`\n",
    "  - `test()`\n",
    "  - `predict()`\n",
    "\n",
    "Many nice operations are automatically performed via arguments/flags in the `Trainer` and minor definitions/calls in `LightningModule`:\n",
    "\n",
    "- Epoch and batch iteration\n",
    "- `optimizer.step()`, `loss.backward()`, `optimizer.zero_grad()` calls\n",
    "- Calling of `model.eval()`, enabling/disabling grads during evaluation\n",
    "- [Checkpoint Saving and Loading](https://lightning.ai/docs/pytorch/stable/common/checkpointing.html)\n",
    "- Tensorboard logging (see [loggers](https://lightning.ai/docs/pytorch/stable/visualize/loggers.html) options)\n",
    "- [Multi-GPU](https://lightning.ai/docs/pytorch/stable/accelerators/gpu.html) support\n",
    "- etc.\n",
    "\n",
    "Alltogether, this is a summary of the contents implemented in the notebook:\n",
    "\n",
    "- The encoder/decoder models are created with Pytorch.\n",
    "- A Lightning model is created with `LightningModule`; it has the most common methods:\n",
    "  - `training_step()` with loss computation and logging\n",
    "  - `validation_step()`\n",
    "  - `test_step()`\n",
    "  - `configure_optimizers()`, which returns the optimizer and the scheduler\n",
    "  - `forward()`, if the model is called\n",
    "  - `predict_step()` for using it in `Trainer().predict()`\n",
    "- A `Trainer` is instantiated and `fit()` with:\n",
    "  - `EarlyStopping` passed as a callbac\n",
    "  - Train and validation data loaders\n",
    "- The model is tested with `Trainer().test()`\n",
    "- The checkpoint is loaded and used:\n",
    "  - The Pytorch model is used independently from Lightning\n",
    "  - The `Trainer().predict()` interface is used to predict a dataset\n",
    "- Training is resumed starting with a desired framework.\n",
    "- Logs are visualized with Tensorboard\n",
    "- Interesting `Trainer` tricks are shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting resources:\n",
    "\n",
    "- [Lightning in 15 Minutes](https://lightning.ai/docs/pytorch/stable/starter/introduction.html)\n",
    "- [How to Organize PyTorch Into Lightning](https://lightning.ai/docs/pytorch/stable/starter/converting.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation of the environment and **Lightning**:\n",
    "\n",
    "```bash\n",
    "# Install/activate a basic environment\n",
    "conda env create -f conda.yaml\n",
    "conda activate ds\n",
    "# Alternatively, if you have an env and wnat to install tensorboard\n",
    "python -m pip install lightning\n",
    "\n",
    "# OPTIONAL: Pytorch on Windows + CUDA 11.7\n",
    "# Update your NVIDIA drivers: https://www.nvidia.com/Download/index.aspx\n",
    "# I have version 12.1, but it works with older versions, e.g. 11.7\n",
    "# Check your CUDA version with: nvidia-smi.exe\n",
    "# In case of any runtime errors, check vrsion compatibility tables:\n",
    "# https://github.com/pytorch/vision#installation\n",
    "python -m pip install -U torch==1.13+cu117 torchvision==0.14+cu117 torchaudio torchtext==0.14 --index-url https://download.pytorch.org/whl/cu117\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(torch.__version__)\n",
    "# '1.13.0+cu117'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 11 12:22:23 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.23                 Driver Version: 536.23       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060      WDDM  | 00000000:22:00.0 Off |                  N/A |\n",
      "|  0%   38C    P8              14W / 170W |      0MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Get info of all GPU devices\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1\n"
     ]
    }
   ],
   "source": [
    "# Set environment variable with possible device ids\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "# Set device: 0 or 1\n",
    "# NOTE: indices are not necessarily the ones shown by nvidia-smi\n",
    "# We need to try them with the cell below\n",
    "torch.cuda.set_device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version? 1.13.0+cu117\n",
      "Torchvision version? 0.14.0+cu117\n",
      "Is cuda available? True\n",
      "Is cuDNN version: 8500\n",
      "cuDNN enabled?  True\n",
      "Device count? 1\n",
      "Current device? 0\n",
      "Device name?  NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "# Check that the selected device is the desired one\n",
    "print(\"Torch version?\", torch.__version__)\n",
    "print(\"Torchvision version?\", torchvision.__version__)\n",
    "print(\"Is cuda available?\", torch.cuda.is_available())\n",
    "print(\"Is cuDNN version:\", torch.backends.cudnn.version())\n",
    "print(\"cuDNN enabled? \", torch.backends.cudnn.enabled)\n",
    "print(\"Device count?\", torch.cuda.device_count())\n",
    "print(\"Current device?\", torch.cuda.current_device())\n",
    "print(\"Device name? \", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Lightning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A LightningModule enables your PyTorch nn.Module to play together in complex ways inside the training_step (there is also an optional validation_step and test_step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# define any number of nn.Modules (or use your current ones)\n",
    "# actually, we can define the model outside and then pass\n",
    "# it to the LightningModule\n",
    "# we can also do it with 1-liners\n",
    "# encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "# decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "# Optional class, not used\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # in this case, the batch is reshaped to (B, 28*28)\n",
    "        return self.decoder(self.encoder(x))    \n",
    "\n",
    "# define the LightningModule\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        # the model can be defined here or outside\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.learning_rate = learning_rate\n",
    "        # OPTIONAL: automatically save all the hyperparameters passed to init,\n",
    "        # but exclude things we don't want to save\n",
    "        # NOTE: hyperparams saved to lightning_logs/version_X/hparams.yaml\n",
    "        self.save_hyperparameters(\"learning_rate\", ignore=[\"encoder\", \"decoder\"])\n",
    "        # then, hyperparams are also available as\n",
    "        # self.hparams.learning_rate\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop\n",
    "        # it is independent of forward()\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z) # x_hat is (B, 28*28)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop (optional)\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        val_loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop (optional)\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        test_loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # intantiate return optimizer and learning rate schedulers\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=70, gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # in this case, the batch is reshaped to (B, 28*28)\n",
    "        return self.decoder(self.encoder(x))\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        # this calls forward\n",
    "        # this will be used in trainer.predict(...)\n",
    "        # we can make this step more sophisticated, if desired\n",
    "        # NOTE: we can also avoid the forward() call\n",
    "        # and implement any desired predict step here!\n",
    "        return self(batch[0]) # a batch is a list with one batch inside, so we take element 0\n",
    "\n",
    "\n",
    "# init the autoencoder\n",
    "encoder = Encoder() # nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "decoder = Decoder() # nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "autoencoder = LitAutoEncoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightning supports ANY iterable (DataLoader, numpy, etc…) for the train/val/test/predict splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data\n",
    "train_set = MNIST(os.path.join(os.getcwd(), \"data\"), download=True, transform=ToTensor())\n",
    "test_set = MNIST(os.path.join(os.getcwd(), \"data\"), download=True, train=False, transform=ToTensor())\n",
    "# use 20% of training data for validation\n",
    "train_set_size = int(len(train_set) * 0.8)\n",
    "valid_set_size = len(train_set) - train_set_size\n",
    "# split the train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, valid_set = utils.data.random_split(train_set, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "# loaders\n",
    "train_loader = utils.data.DataLoader(train_set, batch_size=64)\n",
    "valid_loader = utils.data.DataLoader(valid_set, batch_size=64)\n",
    "test_loader = utils.data.DataLoader(test_set, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lightning [Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html) “mixes” any [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) with any dataset and abstracts away all the engineering complexity needed for scale.\n",
    "\n",
    "The Lightning [Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html) automates [40+ tricks](https://lightning.ai/docs/pytorch/stable/common/trainer.html#trainer-flags) including:\n",
    "\n",
    "- Epoch and batch iteration\n",
    "- `optimizer.step()`, `loss.backward()`, `optimizer.zero_grad()` calls\n",
    "- Calling of `model.eval()`, enabling/disabling grads during evaluation\n",
    "- [Checkpoint Saving and Loading](https://lightning.ai/docs/pytorch/stable/common/checkpointing.html)\n",
    "- Tensorboard logging (see [loggers](https://lightning.ai/docs/pytorch/stable/visualize/loggers.html) options)\n",
    "- [Multi-GPU](https://lightning.ai/docs/pytorch/stable/accelerators/gpu.html) support\n",
    "- [TPU](https://lightning.ai/docs/pytorch/stable/accelerators/tpu.html)\n",
    "- [16-bit precision](https://lightning.ai/docs/pytorch/stable/advanced/speed.html#speed-amp) AMP support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important functionalities to improve our training is [early stopping](https://lightning.ai/docs/pytorch/stable/common/early_stopping.html), which is implemented with callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", # must be saved with self.log() in the LightningModule\n",
    "    min_delta=0.00, # minimum change in the monitored quantity to qualify as an improvement\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode=\"min\" # or max if val_accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 50.4 K\n",
      "1 | decoder | Decoder | 51.2 K\n",
      "------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57193b98b694da4a2ffb62e11c7e0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c48d8c091f2444bbe7045b2004b1569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743709c42b894d55a44a68cc9679bd2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\n",
    "# if GPU is available, we can invoke it with the accelerator flag\n",
    "# the checkpoint and the logs are saved to lightining_logs/version_X\n",
    "trainer = pl.Trainer(\n",
    "    # OPTIONAL arguments\n",
    "    limit_train_batches=100,\n",
    "    limit_val_batches=100,\n",
    "    limit_test_batches=100,\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\", # \"mps\" for Apple Silicon! Metal Performance Shaders\n",
    "    devices=1, # in case we have several GPUs\n",
    "    callbacks=[early_stop_callback]\n",
    "    # if we want to disable saving / checkpointing\n",
    "    # enable_checkpointing=False\n",
    "    # if we want to control the directory where the checkpoint is saved\n",
    "    # default_root_dir=\"some/path/\"\n",
    ")\n",
    "trainer.fit(\n",
    "    model=autoencoder,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=valid_loader\n",
    "    # if we want to resume training where we left\n",
    "    # ckpt_path=\"some/path/to/my_checkpoint.ckpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536cfa9e3be1448faeffe09965acdd76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.05634671822190285    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.05634671822190285   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.05634671822190285}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the model\n",
    "trainer.test(model=autoencoder,\n",
    "             dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Checkpoint and Use the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we’ve trained the model you can export to onnx, torchscript and put it into production or simply load the weights and run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters'])\n",
      "{'learning_rate': 0.001}\n",
      "⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡ \n",
      "Predictions (4 image embeddings):\n",
      " tensor([[ 0.2417,  0.0020,  0.4241],\n",
      "        [ 0.3792, -0.1569,  0.3003],\n",
      "        [ 0.3306, -0.2054,  0.3055],\n",
      "        [ 0.3494, -0.0483,  0.2347]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " ⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint as Lightning object\n",
    "# check logs path for correct version_X and checkpoint\n",
    "checkpoint = \"./lightning_logs/version_19/checkpoints/epoch=0-step=100.ckpt\"\n",
    "lit_autoencoder = LitAutoEncoder.load_from_checkpoint(checkpoint, encoder=Encoder(), decoder=Decoder())\n",
    "\n",
    "# we can also load the checkpoint with torch\n",
    "# many things are saved: 'state_dict', 'optimizer_states', 'hyper_parameters', etc.\n",
    "checkpoint_torch = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
    "print(checkpoint_torch.keys())\n",
    "print(checkpoint_torch[\"hyper_parameters\"])\n",
    "\n",
    "# choose your trained nn.Module\n",
    "# having the model defined outside makes it easier to use it later\n",
    "encoder = lit_autoencoder.encoder\n",
    "encoder.eval()\n",
    "\n",
    "# embed 4 fake images!\n",
    "fake_image_batch = torch.rand(4, 28 * 28, device=lit_autoencoder.device)\n",
    "embeddings = encoder(fake_image_batch)\n",
    "print(\"⚡\" * 20, \"\\nPredictions (4 image embeddings):\\n\", embeddings, \"\\n\", \"⚡\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a89ea08f484c6bb73cc0b840a6efda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Also, it is possible to use the forward() and the predict_step() functions\n",
    "# of the LitAutoencoder from the Trainer\n",
    "# NOTE that predict_step() is called\n",
    "trainer = pl.Trainer()\n",
    "predictions = trainer.predict(model=lit_autoencoder, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of batches\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch 0, all outputs from Autoencoder: (B, 28*28)\n",
    "# NOTE that in this implementation the images were not resized to 28x28\n",
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir ./lightning_logs\n",
    "# http://localhost:6006/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Re-Train or Resume Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at ./lightning_logs/version_8/checkpoints/epoch=0-step=100.ckpt\n",
      "c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:337: UserWarning: The dirpath has changed from 'c:\\\\Users\\\\Msagardi\\\\git_repositories\\\\tool_guides\\\\pytorch_lightning\\\\lightning_logs\\\\version_8\\\\checkpoints' to 'c:\\\\Users\\\\Msagardi\\\\git_repositories\\\\tool_guides\\\\pytorch_lightning\\\\lightning_logs\\\\version_10\\\\checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "  warnings.warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 50.4 K\n",
      "1 | decoder | Sequential | 51.2 K\n",
      "---------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at ./lightning_logs/version_8/checkpoints/epoch=0-step=100.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49f8a39dcdf4b23875984ac8f344924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Msagardi\\AppData\\Local\\anaconda3\\envs\\ds\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "#encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "#decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "autoencoder = LitAutoEncoder(encoder, decoder)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    limit_train_batches=100,\n",
    "    limit_val_batches=100,\n",
    "    limit_test_batches=100,\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\"\n",
    ")\n",
    "\n",
    "# automatically restores model, epoch, step, LR schedulers, etc...\n",
    "trainer.fit(\n",
    "    model=autoencoder,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=valid_loader,\n",
    "    ckpt_path=\"./lightning_logs/version_8/checkpoints/epoch=0-step=100.ckpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Trainer Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
